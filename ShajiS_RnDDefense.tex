\documentclass[aspectratio=169]{beamer}

\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{ragged2e}  % `\justifying` text
\usepackage{booktabs}  % Tables
\usepackage{tabularx}
\usepackage{tikz}      % Diagrams
\usetikzlibrary{calc, shapes, backgrounds}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{dsfont}
\usepackage{url}       % `\url
\usepackage{listings}  % Code listings
\usepackage[T1]{fontenc}
\usepackage[backend=biber, style=ieee, citestyle=numeric-comp, isbn=false, url=false, doi=false, giveninits=true, uniquename=init]{biblatex}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{theme/beamerthemehbrs}
\addbibresource{rnd.bib} % Your bibliography file
\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}


\author[Shinas Shaji]{Shinas Shaji}
\title{Evaluation of Few-Shot Transfer of Vision-Language Foundation Models to Learn Lightweight Models for Robotic Vision Tasks}
\subtitle{R\&D Project Defense}
\institute[HBRS]{Hochschule Bonn-Rhein-Sieg}
\date{\today}
\subject{R\&D Project Defense}

% leave the value of this argument empty if the advisors
% should not be included on the title slide
\def\advisors{Prof. Dr. Sebastian Houben (Hochschule Bonn-Rhein-Sieg, Fraunhofer IAIS), \\
Santosh Thoduka M.Sc. (Fraunhofer IAIS)}

% \thirdpartylogo{path/to/your/image}


\begin{document}
{
\begin{frame}
\titlepage
\end{frame}
}


\section{Introduction}
\begin{frame}{Introduction}
\framesubtitle{Vision-Language Models (VLMs)}
  \begin{itemize}
    \item Neural networks that process both \emph{images} and \emph{text}
    \item Like Large Language Models (LLMs):
    \begin{itemize}
      \item Learn \emph{general visual-textual understanding} from pre-training~\footfullciteieee{Radford2021}
      \item Then aligned to human preferences and \emph{instruction-following}
    \end{itemize}
    \item Shown to be able to \emph{adapt} to new tasks without extensive task-specific training~\footfullciteieee{Liu2023}, hence quite \emph{generalizable}
    \blfootnote{\vspace{0.05em}}
  \end{itemize}
\end{frame}
% SPEAKER NOTES: 
% - VLMs combine computer vision and natural language processing capabilities in a single neural network
% - Pre-training occurs on millions of image-text pairs from the web (e.g., CLIP was trained on 400M image-text pairs)
% - Unlike traditional CV models trained for specific tasks, VLMs learn broader understanding that transfers to many tasks
% - Similar to how ChatGPT can answer questions on many topics, VLMs can handle multiple visual tasks


\begin{frame}{Introduction}
\framesubtitle{Few-Shot Transfer}
  \begin{itemize}
    \item Teaching a \emph{generalizable model} new tasks by showing it a few \emph{examples}
    \item Model `learns' to recognize patterns from these examples
    \item Can then apply this `learning' to new, unseen instances
  \end{itemize}
  \begin{columns}[onlytextwidth]
    \column{.5\textwidth}
    % \centering 
    \small{\emph{Prompt}: A [DOG] has droopy ears and is often fluffy. This is a [DOG]:}
    \begin{figure}
      \includegraphics[width=20mm]{./figures/basset-hound.jpeg}
      \caption{\centering This is a [DOG]. \\ \tiny{Image from Wikipedia, \href{https://commons.wikimedia.org/w/index.php?curid=32270218}{Link}}}
    \end{figure}
    \column{.5\textwidth} 
    \begin{figure}
      \includegraphics[width=20mm]{./figures/golden-retriever-puppy.jpeg}
      \caption{\centering Is this a [DOG]? \\ \tiny{Image from Wikipedia, \href{https://commons.wikimedia.org/w/index.php?curid=18521767}{Link}}}
    \end{figure}
    \vspace{-0.4cm}
    % \centering 
    \small{\emph{Prompt}: Is this a [DOG]?\\
    \emph{Expected Answer}: Yes}
  \end{columns}
\end{frame}
% SPEAKER NOTES:
% - Few-shot transfer is different from traditional machine learning that requires many examples
% - The model doesn't actually learn in the traditional sense; it leverages knowledge from pre-training
% - In this example, we show the model one example of a dog (a basset hound) and ask it to identify another dog
% - This works because the model already knows what dogs look like from pre-training
% - The quotation marks around 'learns' emphasize that it's not learning from scratch. I like to think of it as somehow narrowing the tree of possible responses from a point.


\subsection{Motivation}
\begin{frame}{Motivation}
  \framesubtitle{Few-Shot Transfer}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \centering \emph{Fine-tuning}
      \begin{itemize}
        \item Requires significant computational resources, modifies model parameters
        \item Needs \emph{large amounts} of \emph{labeled data}
        \item Can lead to catastrophic forgetting
        \item Refers to fine-tuning a pre-trained/instruction-tuned model on a specific task
      \end{itemize}
    \column{0.5\textwidth}
    \centering \emph{Few-shot Transfer}
    \begin{itemize}
        \item Uses \emph{`few' examples} or natural language \emph{descriptions}
        \item No model parameters are updated
        \item Potentially more practical for real-world applications~\footfullciteieee{Brown2020}
        \item Can be less effective for complex tasks
    \end{itemize}
  \end{columns}
\end{frame}
% SPEAKER NOTES:
% - Fine-tuning is the traditional approach for adapting pre-trained models to new tasks
% - For VLMs, fine-tuning can require multiple high-end GPUs and days of compute time
% - Catastrophic forgetting means the model may lose previously learned capabilities
% - Few-shot transfer is much more efficient - we can use the model "as is"
% - An important distinction: fine-tuning modifies the model weights, few-shot transfer doesn't
% - The trade-off is that few-shot performance may not match fine-tuned performance for complex tasks


\subsection{Problem Statement}
\begin{frame}{Problem Statement}
\framesubtitle{Dataset Labeling for Computer Vision Tasks}
  \textbf{Challenge:} Creating labeled datasets to train specialized models for computer vision tasks is \emph{time-consuming} and \emph{expensive}~\footfullciteieee{Deng2009}, but VLMs are generalizable
  \vspace{0.5em}

  \textbf{Constraint:} However, VLMs are too \emph{computationally intensive} for direct deployment on resource-constrained environments (e.g., robots)
  \vspace{0.5em}
    
  \textbf{Opportunity:} VLMs could potentially automate label generation (\emph{pseudolabels}) to train \emph{downstream} models
  \vspace{2em}

  \centering \textbf{Research Question:} Can VLMs be transferred to generate \emph{pseudolabels} for computer vision tasks to train \emph{lightweight} downstream models?
  \blfootnote{\vspace{0.05em}}
\end{frame}
% SPEAKER NOTES:
% - The annotation of the ImageNet dataset required crowdsourcing to be feasible
% - Specialized datasets like medical imaging can be even more expensive and require expert annotators
% - VLMs require significant computational resources - multiple high-end GPUs with significant amounts of memory
% - Most robots have limited computational capabilities - often a single low-power GPU or CPU
% - Pseudolabels are automatically generated annotations that can be used in place of human-created labels
% - The key insight: we don't need to deploy the large model - we use it to train a smaller, more efficient model


% \subsection{Proposed Approach}
\begin{frame}{Proposed Approach}
\framesubtitle{Evaluating VLMs for Pseudolabel Generation}
  \textbf{Approach:} Evaluate VLMs on generating \emph{accurate pseudolabels} under various \emph{zero-shot} and \emph{few-shot} transfer conditions
  \vspace{0.3em}
    
  \begin{columns}[T]
    \column{0.55\textwidth}
      \centering \textbf{Key Research Aspects}
      \begin{enumerate}
        \item How does the \emph{number of examples} (few-shot vs. zero-shot) affect pseudolabel quality?
        \item What are the \emph{computational requirements} for practical application?
        \item How effective are the \emph{downstream models} trained on pseudolabels?
      \end{enumerate}

    \column{0.45\textwidth}
    \begin{figure}
      \includegraphics[width=\textwidth]{figures/vlm_transfer_downstream.pdf}
      \caption{\centering Using VLMs to generate pseudolabels for downstream model training}
    \end{figure}
  \end{columns}
\end{frame}
% SPEAKER NOTES:
% - Zero-shot means only describing the task in natural language; few-shot means providing examples
% - We tested multiple prompting strategies to find the most effective approach
% - We examined both common visual tasks (CIFAR-10) and specialized tasks (dermatology) to see if few-shot transfer is generalizable
% - This design helps us understand when VLMs can be reliably transferred for automatic labeling
% - For computational requirements, we measured inference time, memory usage, and scaling properties
% - Downstream models were evaluated against models trained on ground truth labels


\section{Related Work}
\subsection{Vision-Language Models}
\begin{frame}{Related Work}
\framesubtitle{Development and Classes of Vision-Language Models}
  \begin{columns}[T]
    \column{0.5\textwidth}
    \begin{itemize}
      \item \emph{Alignment models}: Generate unified text-image embeddings (CLIP~\footfullciteieee{Radford2021}, FLAVA)
      \item \emph{Generative models}: Geneate text conditoined on multimodal inputs (Flamingo, Frozen~\footfullciteieee{Tsimpoukelli2021}, GPT-4o, Claude 3/3.5/3.7, etc.)
    \end{itemize}
    \column{0.5\textwidth}
      \textbf{Architectural Approaches}
      \begin{itemize}
        \item \emph{Towered}: Separate vision and language models with adapters
        \item \emph{Unified}: Single model processing both modalities "early on"~\footfullciteieee{ChameleonTeam2024}
      \end{itemize}
      \vspace{0.3em}
      \textbf{Key Insight}: Enables framing vision tasks as text generation~\footfullciteieee{Cho2021}, enabling streamlined task transfer
  \end{columns}
\end{frame}
% SPEAKER NOTES:
% - Vision-Language Models (VLMs) have progressed significantly since the introduction of CLIP.
% - Alignment models, such as CLIP, are designed to learn joint representations but do not generate textual outputs.
% - In contrast, generative models are capable of producing text responses based on visual inputs.
% - The Frozen model pioneered the approach of freezing the language model (LLM) while solely training the vision encoder.
% - Contemporary VLMs often incorporate separate pre-trained components for vision and language, connected through adapters or connectors.
% - Additionally, unified architectures that process both modalities simultaneously have emerged, enhancing the integration of vision and language tasks.
% - A pivotal advancement facilitating few-shot transfer is the conceptualization of vision tasks as text generation tasks.
% - This paradigm allows us to articulate tasks using natural language, eliminating the need for specialized architectures.
% - The rapid evolution of this field has led to the emergence of models like GPT-4o, which can comprehend and reason about complex visual scenarios.



\subsection{Transfer Learning \& Adaptation Techniques}
\begin{frame}{Related Work}
\framesubtitle{Transfer Learning \& Adaptation Techniques}
  \begin{columns}[T]
    \column{0.55\textwidth}
      \textbf{Prompting Techniques}: Crafting prompts to improve task performance~\footfullciteieee{Liu2023}
      \begin{itemize}
        \item In-context learning: Providing examples in context~\footfullciteieee{Brown2020}
        \item Chain-of-thought prompting for complex reasoning~\footfullciteieee{Wei2022}
      \end{itemize}
      \vspace{0.5em}
    \column{0.45\textwidth}
      \textbf{Parameter-Efficient Fine-Tuning}: Typically requires more examples than few-shot regime
      \begin{itemize}
        \item Prefix-tuning: Optimizing task-specific prompt vectors~\footfullciteieee{Li2021}
        \item Low-Rank Adaptation (LoRA)~\footfullciteieee{Hu2021}: decomposes weight updates into smaller trainable matrices
      \end{itemize}
  \end{columns}
\end{frame}
% SPEAKER NOTES:
% - Prompting techniques are crucial for adapting models to new tasks without retraining
% - In-context learning allows models to learn from examples without parameter updates
% - Chain-of-thought prompting helps models break down complex tasks into simpler steps
% - Parameter-efficient fine-tuning methods like prefix-tuning are valuable for low-resource settings
% - These techniques are less explored in VLMs compared to NLP models
% - Our research contributes by evaluating these methods in the context of VLMs for dataset annotation


\subsection{Applications and Datasets}
\begin{frame}{Related Work}
\framesubtitle{Applications and Datasets}
  \begin{columns}[T]
    \column{0.55\textwidth}
      \textbf{Applicability of VLMs}
      \begin{itemize}
        \item Large-scale pre-training enables generalization to various tasks
        \item Traditional DNNs trained for specific tasks
      \end{itemize}
      \vspace{0.5em}
      \textbf{LLMs for Data Annotation}
      \begin{itemize}
        \item LLMs have been used to generate multimodal instruction-following data~\footfullciteieee{Liu2023a}
        \item Also seen to outperform crowd-workers in data annotation tasks~\footfullciteieee{Gilardi2023}
      \end{itemize}
    \column{0.45\textwidth}
      \textbf{Key Datasets}:
      Various datasets exist for various vision tasks.
      \begin{itemize}
        \item ImageNet, CIFAR-10~\footfullciteieee{Krizhevsky2009}: Object recognition
        \item Microsoft COCO: Detection, segmentation, captioning
        \item Derm7Pt~\footfullciteieee{Kawahara2019}: Specialized dermatology dataset
        \item MVTec: Anomaly detection
      \end{itemize}
  \end{columns}
\end{frame}
% SPEAKER NOTES:
% - VLMs are used in a wide range of vision tasks, leveraging their ability to generalize from large-scale pre-training
% - Traditional DNNs are typically trained on specific datasets for specific tasks, limiting their generalization
% - LLMs have been used to generate multimodal instruction-following data to train VLMs, enhancing dataset creation
% - LLMs have been seen to outperform human annotators, providing more consistent and scalable data annotation
% - Key datasets like ImageNet and CIFAR-10 are benchmarks for evaluating model performance on object recognition
% - Specialized datasets like Derm7Pt are crucial for testing models on domain-specific tasks
% - Our research leverages these datasets to evaluate the effectiveness of VLMs in generating pseudolabels for downstream tasks


\subsection{Research Gaps and Limitations}
\begin{frame}{Research Gaps and Limitations}
  \begin{itemize}
    \item Few-shot transfer in VLMs less explored than in NLP
    \item Limited research on VLMs for dataset annotation
    \item Few studies on downstream model performance with VLM-generated labels
    \item Our work addresses these gaps
  \end{itemize}
\end{frame}
% SPEAKER NOTES:


\section{Methodology}
\subsection{Experimental Setup}
\begin{frame}{Experimental Setup}
  % Content will be added later
\end{frame}

\subsection{Datasets}
\begin{frame}{Datasets}
  % Content will be added later
\end{frame}

\subsection{Models and Prompting Strategies}
\begin{frame}{Models and Prompting Strategies}
  % Content will be added later
\end{frame}

\section{Evaluation}
\subsection{CIFAR-10 Experiments}
\begin{frame}{CIFAR-10 Experiments}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \textbf{Experimental Setup}
      \begin{itemize}
        \item Four VLMs evaluated:
        \begin{itemize}
          \item Pixtral 12B
          \item InternVL2
          \item MiniCPM v2.6
          \item Phi-3.5 Vision Instruct
        \end{itemize}
        \item Various prompting strategies:
        \begin{itemize}
          \item Zero-shot (0-shot, 0.5-shot)
          \item Few-shot (1-16 examples)
          \item Reasoning-based approaches
        \end{itemize}
      \end{itemize}
    \column{0.5\textwidth}
      \textbf{Key Findings}
      \begin{itemize}
        \item Zero-shot performance:
        \begin{itemize}
          \item Consistent across prompting strategies
          \item Reasoning requirements decreased accuracy by 10-15\%
        \end{itemize}
        \item Few-shot insights:
        \begin{itemize}
          \item Performance peaks at 4-8 shots
          \item Super-linear increase in resource usage
          \item Prompting improves low-shot scenarios
        \end{itemize}
      \end{itemize}
  \end{columns}
\end{frame}
% SPEAKER NOTES:
% - Evaluated four state-of-the-art VLMs on CIFAR-10 image classification
% - Zero-shot experiments showed consistent performance across different prompting strategies
% - Adding reasoning requirements actually decreased performance by 10-15% due to format constraints
% - Few-shot performance varied by model: Pixtral and InternVL2 showed improvements up to 8 shots
% - MiniCPM and Phi-3.5 peaked earlier at 4-6 shots
% - Resource usage increased super-linearly with number of shots
% - Memory usage patterns varied: Pixtral showed super-linear increase, MiniCPM remained constant
% - Processing time increased dramatically: 2 shots ~10x, 4 shots ~20x, 8 shots 30-50x longer than zero-shot
% - Downstream models trained on pseudolabels often outperformed the VLMs themselves

\subsection{Downstream Model Analysis}
\begin{frame}{Downstream Model Analysis}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \textbf{Performance Characteristics}
      \begin{itemize}
        \item Downstream models outperformed VLMs
        \begin{itemize}
          \item Pre-trained ResNet-18 architecture
          \item Focus on pattern recognition
        \end{itemize}
        \item Label quality vs. dataset size:
        \begin{itemize}
          \item Quality more crucial than quantity
          \item 90\% VLM accuracy threshold
        \end{itemize}
      \end{itemize}
    \column{0.5\textwidth}
      \textbf{Key Insights}
      \begin{itemize}
        \item Weak dependence on shot count
        \item Label smoothing improved performance by ~1\%
        \item Practical implications:
        \begin{itemize}
          \item Zero/low-shot approaches sufficient
          \item Simple prompts more effective
          \item High resilience to label noise
        \end{itemize}
      \end{itemize}
  \end{columns}
\end{frame}
% SPEAKER NOTES:
% - Downstream models consistently outperformed VLMs due to ImageNet pre-training and focus on pattern recognition
% - Performance showed weak dependence on number of shots used in VLM predictions
% - Models maintained good performance even with lower-quality labels from 1-shot experiments
% - Label quality proved more important than dataset size in training
% - Models trained on limited but accurate ground truth labels performed better until VLM accuracy reached ~90%
% - Practical finding: zero-shot or low-shot (1-2 examples) with prompting is most efficient implementation strategy
% - Higher shot counts showed diminishing returns despite increased computational costs
% - Downstream models showed remarkable resilience to label quality, suggesting practical utility even with imperfect data


\subsection{Specialized Domain (Derm7Pt) Experiments}
\begin{frame}{Specialized Domain Experiments}
  % Content will be added later
\end{frame}

\subsection{Computational Resources Analysis}
\begin{frame}{Computational Resources Analysis}
  % Content will be added later
\end{frame}

\section{Conclusions}
% \subsection{Key Findings}
\begin{frame}{Key Findings}
  % Content will be added later
\end{frame}

% \subsection{Limitations and Future Work}
\begin{frame}{Limitations and Future Work}
    \begin{columns}[T]
        \column{0.5\textwidth}
            \textbf{Technical Limitations}
            \begin{itemize}
                \item Context length and memory constraints
                \begin{itemize}
                    \item Limited number of examples
                    \item High-resolution image limitations
                    \item Restricted use of larger models
                \end{itemize}
                \item Limited / degraded performance scaling with number of examples
                \item Limited generalization to specialized domains
                \item Our findings may not generalize to all vision tasks
            \end{itemize}
        \column{0.5\textwidth}
            \textbf{Future Research Directions}
            \begin{itemize}
                \item Generalizable domain-specific pre-training strategies
                \item More efficient VLM architectures for multi-image processing
                \item Improving prompting strategies and visual grounding~\footfullciteieee{Yang2023}
                \item Extending investigation to other vision tasks:
                \begin{itemize}
                    \item Image captioning
                    \item Segmentation
                \end{itemize}
            \end{itemize}
    \end{columns}
\end{frame}
% SPEAKER NOTES:
% - Context length and memory limitations restricted few-shot experiments to limited examples and constrained larger models
% - Observed trends already show limited/degraded performance scaling with higher number of shots
% - Poor performance on specialized domains suggests models rely more on pre-trained priors than learning new capabilities
% - Promising research directions include investigating domain-specific pre-training strategies
% - More efficient VLM architectures needed for scalable high-resolution multi-image processing
% - Beyond classification, future work could explore VLM application for image captioning and segmentation
% - Improved prompting strategies needed to enhance visual grounding and reasoning capabilities

% \section{Q\&A}
\begin{frame}{Thank You!}
  \centering
  \vfill
  {\LARGE Questions?}
  \vfill
  \raggedright\textbf{Contact:}
  \begin{itemize}
    \item shinas.shaji@smail.inf.h-brs.de
    \item shinas.shaji@iais.fraunhofer.de
  \end{itemize}
  \vfill
\end{frame}

\end{document}
