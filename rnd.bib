@Article{Yuan2021,
  author        = {Yuan, Lu and Chen, Dongdong and Chen, Yi-Ling and Codella, Noel and Dai, Xiyang and Gao, Jianfeng and Hu, Houdong and Huang, Xuedong and Li, Boxin and Li, Chunyuan and Liu, Ce and Liu, Mengchen and Liu, Zicheng and Lu, Yumao and Shi, Yu and Wang, Lijuan and Wang, Jianfeng and Xiao, Bin and Xiao, Zhen and Yang, Jianwei and Zeng, Michael and Zhou, Luowei and Zhang, Pengchuan},
  journal       = {arXiv preprint},
  title         = {{Florence: A New Foundation Model for Computer Vision}},
  year          = {2021},
  month         = nov,
  abstract      = {Automated visual understanding of our diverse and open world demands computer vision models to generalize well with minimal customization for specific tasks, similar to human vision. Computer vision foundation models, which are trained on diverse, large-scale dataset and can be adapted to a wide range of downstream tasks, are critical for this mission to solve real-world computer vision applications. While existing vision foundation models such as CLIP, ALIGN, and Wu Dao 2.0 focus mainly on mapping images and textual representations to a cross-modal shared representation, we introduce a new computer vision foundation model, Florence, to expand the representations from coarse (scene) to fine (object), from static (images) to dynamic (videos), and from RGB to multiple modalities (caption, depth). By incorporating universal visual-language representations from Web-scale image-text data, our Florence model can be easily adapted for various computer vision tasks, such as classification, retrieval, object detection, VQA, image caption, video retrieval and action recognition. Moreover, Florence demonstrates outstanding performance in many types of transfer learning: fully sampled fine-tuning, linear probing, few-shot transfer and zero-shot transfer for novel images and objects. All of these properties are critical for our vision foundation model to serve general purpose vision tasks. Florence achieves new state-of-the-art results in majority of 44 representative benchmarks, e.g., ImageNet-1K zero-shot classification with top-1 accuracy of 83.74 and the top-5 accuracy of 97.18, 62.4 mAP on COCO fine tuning, 80.36 on VQA, and 87.8 on Kinetics-600.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2111.11432},
  eprint        = {2111.11432},
  file          = {:Yuan2021 - Florence_ a New Foundation Model for Computer Vision.pdf:PDF:http\://arxiv.org/pdf/2111.11432v1},
  keywords      = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences},
  primaryclass  = {cs.CV},
  publisher     = {arXiv},
}

@Article{Zeng2022,
  author        = {Zeng, Andy and Attarian, Maria and Ichter, Brian and Choromanski, Krzysztof and Wong, Adrian and Welker, Stefan and Tombari, Federico and Purohit, Aveek and Ryoo, Michael and Sindhwani, Vikas and Lee, Johnny and Vanhoucke, Vincent and Florence, Pete},
  title         = {Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language},
  year          = {2022},
  month         = apr,
  abstract      = {Large pretrained (e.g., "foundation") models exhibit distinct capabilities depending on the domain of data they are trained on. While these domains are generic, they may only barely overlap. For example, visual-language models (VLMs) are trained on Internet-scale image captions, but large language models (LMs) are further trained on Internet-scale text with no images (e.g., spreadsheets, SAT questions, code). As a result, these models store different forms of commonsense knowledge across different domains. In this work, we show that this diversity is symbiotic, and can be leveraged through Socratic Models (SMs): a modular framework in which multiple pretrained models may be composed zero-shot i.e., via multimodal-informed prompting, to exchange information with each other and capture new multimodal capabilities, without requiring finetuning. With minimal engineering, SMs are not only competitive with state-of-the-art zero-shot image captioning and video-to-text retrieval, but also enable new applications such as (i) answering free-form questions about egocentric video, (ii) engaging in multimodal assistive dialogue with people (e.g., for cooking recipes) by interfacing with external APIs and databases (e.g., web search), and (iii) robot perception and planning.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2204.00598},
  eprint        = {2204.00598},
  file          = {:Zeng2022 - Socratic Models_ Composing Zero Shot Multimodal Reasoning with Language.pdf:PDF:http\://arxiv.org/pdf/2204.00598v2},
  groups        = {Few-Shot Transfer},
  keywords      = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences},
  primaryclass  = {cs.CV},
  publisher     = {arXiv},
}

@Article{Bao2021,
  author        = {Bao, Hangbo and Dong, Li and Piao, Songhao and Wei, Furu},
  title         = {{BEiT: BERT Pre-Training of Image Transformers}},
  year          = {2021},
  month         = jun,
  abstract      = {We introduce a self-supervised vision representation model BEiT, which stands for Bidirectional Encoder representation from Image Transformers. Following BERT developed in the natural language processing area, we propose a masked image modeling task to pretrain vision Transformers. Specifically, each image has two views in our pre-training, i.e, image patches (such as 16x16 pixels), and visual tokens (i.e., discrete tokens). We first "tokenize" the original image into visual tokens. Then we randomly mask some image patches and fed them into the backbone Transformer. The pre-training objective is to recover the original visual tokens based on the corrupted image patches. After pre-training BEiT, we directly fine-tune the model parameters on downstream tasks by appending task layers upon the pretrained encoder. Experimental results on image classification and semantic segmentation show that our model achieves competitive results with previous pre-training methods. For example, base-size BEiT achieves 83.2% top-1 accuracy on ImageNet-1K, significantly outperforming from-scratch DeiT training (81.8%) with the same setup. Moreover, large-size BEiT obtains 86.3% only using ImageNet-1K, even outperforming ViT-L with supervised pre-training on ImageNet-22K (85.2%). The code and pretrained models are available at https://aka.ms/beit.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2106.08254},
  eprint        = {2106.08254},
  file          = {:Bao2021 - BEiT_ BERT Pre Training of Image Transformers.pdf:PDF:http\://arxiv.org/pdf/2106.08254v2},
  groups        = {Models},
  keywords      = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences},
  primaryclass  = {cs.CV},
  publisher     = {arXiv},
}

@Article{Wang2022,
  author        = {Wang, Wenhui and Bao, Hangbo and Dong, Li and Bjorck, Johan and Peng, Zhiliang and Liu, Qiang and Aggarwal, Kriti and Mohammed, Owais Khan and Singhal, Saksham and Som, Subhojit and Wei, Furu},
  title         = {Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks},
  year          = {2022},
  month         = aug,
  abstract      = {A big convergence of language, vision, and multimodal pretraining is emerging. In this work, we introduce a general-purpose multimodal foundation model BEiT-3, which achieves state-of-the-art transfer performance on both vision and vision-language tasks. Specifically, we advance the big convergence from three aspects: backbone architecture, pretraining task, and model scaling up. We introduce Multiway Transformers for general-purpose modeling, where the modular architecture enables both deep fusion and modality-specific encoding. Based on the shared backbone, we perform masked "language" modeling on images (Imglish), texts (English), and image-text pairs ("parallel sentences") in a unified manner. Experimental results show that BEiT-3 obtains state-of-the-art performance on object detection (COCO), semantic segmentation (ADE20K), image classification (ImageNet), visual reasoning (NLVR2), visual question answering (VQAv2), image captioning (COCO), and cross-modal retrieval (Flickr30K, COCO).},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2208.10442},
  eprint        = {2208.10442},
  file          = {:Wang2022 - Image As a Foreign Language_ BEiT Pretraining for All Vision and Vision Language Tasks.pdf:PDF:http\://arxiv.org/pdf/2208.10442v2},
  keywords      = {Computer Vision and Pattern Recognition (cs.CV), Computation and Language (cs.CL), FOS: Computer and information sciences},
  primaryclass  = {cs.CV},
  publisher     = {arXiv},
}

@Article{Xiao2023,
  author        = {Xiao, Xuan and Liu, Jiahang and Wang, Zhipeng and Zhou, Yanmin and Qi, Yong and Cheng, Qian and He, Bin and Jiang, Shuo},
  title         = {Robot Learning in the Era of Foundation Models: A Survey},
  year          = {2023},
  month         = nov,
  abstract      = {The proliferation of Large Language Models (LLMs) has s fueled a shift in robot learning from automation towards general embodied Artificial Intelligence (AI). Adopting foundation models together with traditional learning methods to robot learning has increasingly gained recent interest research community and showed potential for real-life application. However, there are few literatures comprehensively reviewing the relatively new technologies combined with robotics. The purpose of this review is to systematically assess the state-of-the-art foundation model techniques in the robot learning and to identify future potential areas. Specifically, we first summarized the technical evolution of robot learning and identified the necessary preliminary preparations for foundation models including the simulators, datasets, foundation model framework. In addition, we focused on the following four mainstream areas of robot learning including manipulation, navigation, planning, and reasoning and demonstrated how the foundation model techniques can be adopted in the above scenarios. Furthermore, critical issues which are neglected in the current literatures including robot hardware and software decoupling, dynamic data, generalization performance with the presence of human, etc. were discussed. This review highlights the state-of-the-art progress of foundation models in robot learning and future research should focus on multimodal interaction especially dynamics data, exclusive foundation models for robots, and AI alignment, etc.},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International},
  doi           = {10.48550/ARXIV.2311.14379},
  eprint        = {2311.14379},
  file          = {:Xiao2023 - Robot Learning in the Era of Foundation Models_ a Survey.pdf:PDF:http\://arxiv.org/pdf/2311.14379v1},
  keywords      = {Robotics (cs.RO), FOS: Computer and information sciences},
  primaryclass  = {cs.RO},
  publisher     = {arXiv},
}

@InProceedings{Hu2022,
  author    = {Hu, Shell Xu and Li, Da and St\"uhmer, Jan and Kim, Minyoung and Hospedales, Timothy M.},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Pushing the Limits of Simple Pipelines for Few-Shot Learning: External Data and Fine-Tuning Make a Difference},
  year      = {2022},
  month     = {June},
  pages     = {9068-9077},
  comment   = {138},
  doi       = {10.1109/cvpr52688.2022.00886},
  file      = {:Hu2022 - Pushing the Limits of Simple Pipelines for Few Shot Learning_ External Data and Fine Tuning Make a Difference.pdf:PDF},
  groups    = {To Check, Few-Shot Transfer},
}

@InProceedings{Zhang2023,
  author    = {Zhang, Renrui and Hu, Xiangfei and Li, Bohao and Huang, Siyuan and Deng, Hanqiu and Qiao, Yu and Gao, Peng and Li, Hongsheng},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Prompt, Generate, Then Cache: Cascade of Foundation Models Makes Strong Few-Shot Learners},
  year      = {2023},
  month     = {June},
  pages     = {15211-15222},
  comment   = {80},
  doi       = {10.1109/cvpr52729.2023.01460},
  file      = {:Zhang2023 - Prompt, Generate, Then Cache_ Cascade of Foundation Models Makes Strong Few Shot Learners.pdf:PDF},
  groups    = {Few-Shot Transfer},
}

@InProceedings{Caron2021,
  author    = {Caron, Mathilde and Touvron, Hugo and Misra, Ishan and J\'egou, Herv\'e and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  title     = {Emerging Properties in Self-Supervised Vision Transformers},
  year      = {2021},
  month     = {October},
  pages     = {9650-9660},
  doi       = {10.1109/iccv48922.2021.00951},
  file      = {:Caron2021 - Emerging Properties in Self Supervised Vision Transformers.pdf:PDF},
  groups    = {To Check},
}

@InProceedings{Pratt2023,
  author    = {Pratt, Sarah and Covert, Ian and Liu, Rosanne and Farhadi, Ali},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  title     = {What Does a Platypus Look Like? Generating Customized Prompts for Zero-Shot Image Classification},
  year      = {2023},
  month     = {October},
  pages     = {15691-15701},
  doi       = {10.1109/iccv51070.2023.01438},
  file      = {:Pratt2023 - What Does a Platypus Look Like_ Generating Customized Prompts for Zero Shot Image Classification.pdf:PDF},
  groups    = {To Check, Few-Shot Transfer},
}

@InProceedings{Brown2020,
  author    = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Language {M}odels are {F}ew-{S}hot {L}earners},
  year      = {2020},
  editor    = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
  pages     = {1877--1901},
  publisher = {Curran Associates, Inc.},
  volume    = {33},
  abstract  = {We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks. We also identify some datasets where GPT-3’s few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora.},
  file      = {:Brown2020 - Language Models Are Few Shot Learners.pdf:PDF},
  groups    = {Reading Reports, To Check, Models, Few-Shot Transfer},
  ranking   = {rank5},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
}

@InProceedings{Radford2021,
  author    = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  title     = {{Learning Transferable Visual Models From Natural Language Supervision}},
  year      = {2021},
  editor    = {Meila, Marina and Zhang, Tong},
  month     = jul,
  pages     = {8748--8763},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {139},
  abstract  = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on.},
  file      = {:Radford2021 - Learning Transferable Visual Models from Natural Language Supervision.pdf:PDF},
  groups    = {To Check},
  pdf       = {http://proceedings.mlr.press/v139/radford21a/radford21a.pdf},
  ranking   = {rank5},
  url       = {https://proceedings.mlr.press/v139/radford21a.html},
}

@InProceedings{Tsimpoukelli2021,
  author    = {Tsimpoukelli, Maria and Menick, Jacob L and Cabi, Serkan and Eslami, S. M. Ali and Vinyals, Oriol and Hill, Felix},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {{Multimodal Few-Shot Learning with Frozen Language Models}},
  year      = {2021},
  editor    = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
  pages     = {200--212},
  publisher = {Curran Associates, Inc.},
  volume    = {34},
  abstract  = {When trained at sufficient scale, auto-regressive language models exhibit the notable ability to learn a new language task after being prompted with just a few examples. Here, we present a simple, yet effective, approach for transferring this few-shot learning ability to a multimodal setting (vision and language). Using aligned image and caption data, we train a vision encoder to represent each image as a sequence of continuous embeddings, such that a pre-trained, frozen language model prompted with this prefix generates the appropriate caption. The resulting system is a multimodal few-shot learner, with the surprising ability to learn a variety of new tasks when conditioned on examples, represented as a sequence of multiple interleaved image and text embeddings. We demonstrate that it can rapidly learn words for new objects and novel visual categories, do visual question-answering with only a handful of examples, and make use of outside knowledge, by measuring a single model on a variety of established and new benchmarks.},
  file      = {:Tsimpoukelli2021 - Multimodal Few Shot Learning with Frozen Language Models.pdf:PDF},
  groups    = {Reading Reports, Few-Shot Transfer},
  ranking   = {rank4},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2021/file/01b7575c38dac42f3cfb7d500438b875-Paper.pdf},
}

@Article{Radford2019,
  author  = {Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  journal = {arXiv preprint},
  title   = {{Language Models are Unsupervised Multitask Learners}},
  year    = {2019},
  file    = {:Radford2019 - Language Models Are Unsupervised Multitask Learners.pdf:PDF;:NeurIPS-2020-language-models-are-few-shot-learners-Supplemental.pdf:PDF},
  groups  = {Models},
}

@Article{Kaplan2020,
  author  = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal = {arXiv preprint arXiv:2001.08361},
  title   = {Scaling laws for neural language models},
  year    = {2020},
  file    = {:Kaplan2020 - Scaling Laws for Neural Language Models.pdf:PDF},
}

@Article{Lu2021,
  author        = {Lu, Kevin and Grover, Aditya and Abbeel, Pieter and Mordatch, Igor},
  title         = {Pretrained Transformers as Universal Computation Engines},
  year          = {2021},
  month         = mar,
  abstract      = {We investigate the capability of a transformer pretrained on natural language to generalize to other modalities with minimal finetuning -- in particular, without finetuning of the self-attention and feedforward layers of the residual blocks. We consider such a model, which we call a Frozen Pretrained Transformer (FPT), and study finetuning it on a variety of sequence classification tasks spanning numerical computation, vision, and protein fold prediction. In contrast to prior works which investigate finetuning on the same modality as the pretraining dataset, we show that pretraining on natural language can improve performance and compute efficiency on non-language downstream tasks. Additionally, we perform an analysis of the architecture, comparing the performance of a random initialized transformer to a random LSTM. Combining the two insights, we find language-pretrained transformers can obtain strong performance on a variety of non-language tasks.},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution 4.0 International},
  doi           = {10.48550/ARXIV.2103.05247},
  eprint        = {2103.05247},
  file          = {:Lu2021 - Pretrained Transformers As Universal Computation Engines.pdf:PDF:http\://arxiv.org/pdf/2103.05247v2},
  keywords      = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), FOS: Computer and information sciences},
  primaryclass  = {cs.LG},
  publisher     = {arXiv},
}

@Article{Kiela2019,
  author        = {Kiela, Douwe and Bhooshan, Suvrat and Firooz, Hamed and Perez, Ethan and Testuggine, Davide},
  title         = {Supervised Multimodal Bitransformers for Classifying Images and Text},
  year          = {2019},
  month         = sep,
  abstract      = {Self-supervised bidirectional transformer models such as BERT have led to dramatic improvements in a wide variety of textual classification tasks. The modern digital world is increasingly multimodal, however, and textual information is often accompanied by other modalities such as images. We introduce a supervised multimodal bitransformer model that fuses information from text and image encoders, and obtain state-of-the-art performance on various multimodal classification benchmark tasks, outperforming strong baselines, including on hard test sets specifically designed to measure multimodal performance.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1909.02950},
  eprint        = {1909.02950},
  file          = {:Kiela2019 - Supervised Multimodal Bitransformers for Classifying Images and Text.pdf:PDF:http\://arxiv.org/pdf/1909.02950v2},
  keywords      = {Computation and Language (cs.CL), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  publisher     = {arXiv},
}

@Article{Zhai2021,
  author        = {Zhai, Xiaohua and Kolesnikov, Alexander and Houlsby, Neil and Beyer, Lucas},
  title         = {Scaling Vision Transformers},
  year          = {2021},
  month         = jun,
  abstract      = {Attention-based neural networks such as the Vision Transformer (ViT) have recently attained state-of-the-art results on many computer vision benchmarks. Scale is a primary ingredient in attaining excellent results, therefore, understanding a model's scaling properties is a key to designing future generations effectively. While the laws for scaling Transformer language models have been studied, it is unknown how Vision Transformers scale. To address this, we scale ViT models and data, both up and down, and characterize the relationships between error rate, data, and compute. Along the way, we refine the architecture and training of ViT, reducing memory consumption and increasing accuracy of the resulting models. As a result, we successfully train a ViT model with two billion parameters, which attains a new state-of-the-art on ImageNet of 90.45% top-1 accuracy. The model also performs well for few-shot transfer, for example, reaching 84.86% top-1 accuracy on ImageNet with only 10 examples per class.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2106.04560},
  eprint        = {2106.04560},
  file          = {:Zhai2021 - Scaling Vision Transformers.pdf:PDF:http\://arxiv.org/pdf/2106.04560v2},
  keywords      = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences},
  primaryclass  = {cs.CV},
  publisher     = {arXiv},
}

@Article{Liu2023,
  author     = {Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
  journal    = {ACM Comput. Surv.},
  title      = {{Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing}},
  year       = {2023},
  issn       = {0360-0300},
  month      = {jan},
  number     = {9},
  volume     = {55},
  abstract   = {This article surveys and organizes research works in a new paradigm in natural language processing, which we dub “prompt-based learning.” Unlike traditional supervised learning, which trains a model to take in an input x and predict an output y as P(y|x), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input x is modified using a template into a textual string prompt x′ that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string x̂, from which the final output y can be derived. This framework is powerful and attractive for a number of reasons: It allows the language model to be pre-trained on massive amounts of raw text, and by defining a new prompting function the model is able to perform few-shot or even zero-shot learning, adapting to new scenarios with few or no labeled data. In this article, we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g.,&nbsp;the choice of pre-trained language models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts but also release other resources, e.g., a website  including constantly updated survey and paperlist.},
  address    = {New York, NY, USA},
  articleno  = {195},
  doi        = {10.1145/3560815},
  file       = {:Liu2023 - Pre Train, Prompt, and Predict_ a Systematic Survey of Prompting Methods in Natural Language Processing.pdf:PDF},
  groups     = {Essays, To Check, Few-Shot Transfer},
  issue_date = {September 2023},
  keywords   = {Pre-trained language models, prompting},
  numpages   = {35},
  priority   = {prio1},
  publisher  = {Association for Computing Machinery},
}

@InProceedings{Huang2022,
  author    = {Huang, Wenlong and Abbeel, Pieter and Pathak, Deepak and Mordatch, Igor},
  booktitle = {Proceedings of the 39th International Conference on Machine Learning},
  title     = {Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents},
  year      = {2022},
  editor    = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  month     = {17--23 Jul},
  pages     = {9118--9147},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {162},
  abstract  = {Can world knowledge learned by large language models (LLMs) be used to act in interactive environments? In this paper, we investigate the possibility of grounding high-level tasks, expressed in natural language (e.g. “make breakfast”), to a chosen set of actionable steps (e.g. “open fridge”). While prior work focused on learning from explicit step-by-step examples of how to act, we surprisingly find that if pre-trained LMs are large enough and prompted appropriately, they can effectively decompose high-level tasks into mid-level plans without any further training. However, the plans produced naively by LLMs often cannot map precisely to admissible actions. We propose a procedure that conditions on existing demonstrations and semantically translates the plans to admissible actions. Our evaluation in the recent VirtualHome environment shows that the resulting method substantially improves executability over the LLM baseline. The conducted human evaluation reveals a trade-off between executability and correctness but shows a promising sign towards extracting actionable knowledge from language models.},
  file      = {:Huang2022 - Language Models As Zero Shot Planners_ Extracting Actionable Knowledge for Embodied Agents.pdf:PDF},
  groups    = {Few-Shot Transfer},
  pdf       = {https://proceedings.mlr.press/v162/huang22a/huang22a.pdf},
  url       = {https://proceedings.mlr.press/v162/huang22a.html},
}

@Article{Alzubaidi2023,
  author    = {Alzubaidi, Laith and Bai, Jinshuai and Al-Sabaawi, Aiman and Santamar{\'\i}a, Jose and Albahri, Ahmed Shihab and Al-dabbagh, Bashar Sami Nayyef and Fadhel, Mohammed A and Manoufali, Mohamed and Zhang, Jinglan and Al-Timemy, Ali H and others},
  journal   = {Journal of Big Data},
  title     = {A survey on deep learning tools dealing with data scarcity: definitions, challenges, solutions, tips, and applications},
  year      = {2023},
  number    = {1},
  pages     = {46},
  volume    = {10},
  file      = {:Alzubaidi2023 - A Survey on Deep Learning Tools Dealing with Data Scarcity_ Definitions, Challenges, Solutions, Tips, and Applications.pdf:PDF},
  groups    = {To Check, Few-Shot Transfer},
  publisher = {Springer},
}

@Article{AllenZhu2020,
  author        = {Allen-Zhu, Zeyuan and Li, Yuanzhi},
  title         = {Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning},
  year          = {2020},
  month         = dec,
  abstract      = {We formally study how ensemble of deep learning models can improve test accuracy, and how the superior performance of ensemble can be distilled into a single model using knowledge distillation. We consider the challenging case where the ensemble is simply an average of the outputs of a few independently trained neural networks with the SAME architecture, trained using the SAME algorithm on the SAME data set, and they only differ by the random seeds used in the initialization. We show that ensemble/knowledge distillation in Deep Learning works very differently from traditional learning theory (such as boosting or NTKs, neural tangent kernels). To properly understand them, we develop a theory showing that when data has a structure we refer to as ``multi-view'', then ensemble of independently trained neural networks can provably improve test accuracy, and such superior test accuracy can also be provably distilled into a single model by training a single model to match the output of the ensemble instead of the true label. Our result sheds light on how ensemble works in deep learning in a way that is completely different from traditional theorems, and how the ``dark knowledge'' is hidden in the outputs of the ensemble and can be used in distillation. In the end, we prove that self-distillation can also be viewed as implicitly combining ensemble and knowledge distillation to improve test accuracy.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2012.09816},
  eprint        = {2012.09816},
  file          = {:AllenZhu2020 - Towards Understanding Ensemble, Knowledge Distillation and Self Distillation in Deep Learning.pdf:PDF:http\://arxiv.org/pdf/2012.09816v3},
  groups        = {To Check},
  keywords      = {Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), Optimization and Control (math.OC), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Mathematics},
  primaryclass  = {cs.LG},
  publisher     = {arXiv},
}

@InProceedings{Schick2021a,
  author    = {Schick, Timo and Sch{\"u}tze, Hinrich},
  booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
  title     = {Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language Inference},
  year      = {2021},
  address   = {Online},
  editor    = {Merlo, Paola and Tiedemann, Jorg and Tsarfaty, Reut},
  month     = apr,
  pages     = {255--269},
  publisher = {Association for Computational Linguistics},
  abstract  = {Some NLP tasks can be solved in a fully unsupervised fashion by providing a pretrained language model with {``}task descriptions{''} in natural language (e.g., Radford et al., 2019). While this approach underperforms its supervised counterpart, we show in this work that the two ideas can be combined: We introduce Pattern-Exploiting Training (PET), a semi-supervised training procedure that reformulates input examples as cloze-style phrases to help language models understand a given task. These phrases are then used to assign soft labels to a large set of unlabeled examples. Finally, standard supervised training is performed on the resulting training set. For several tasks and languages, PET outperforms supervised training and strong semi-supervised approaches in low-resource settings by a large margin.},
  doi       = {10.18653/v1/2021.eacl-main.20},
  file      = {:Schick2021a - Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference.pdf:PDF},
  groups    = {To Check, Few-Shot Transfer},
  url       = {https://aclanthology.org/2021.eacl-main.20},
}

@InProceedings{Wei2021,
  author    = {Wei, Colin and Xie, Sang Michael and Ma, Tengyu},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Why Do Pretrained Language Models Help in Downstream Tasks? An Analysis of Head and Prompt Tuning},
  year      = {2021},
  editor    = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
  pages     = {16158--16170},
  publisher = {Curran Associates, Inc.},
  volume    = {34},
  file      = {:Wei2021 - Why Do Pretrained Language Models Help in Downstream Tasks_ an Analysis of Head and Prompt Tuning.pdf:PDF},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2021/file/86b3e165b8154656a71ffe8a327ded7d-Paper.pdf},
}

@Article{Scao2021,
  author        = {Scao, Teven Le and Rush, Alexander M.},
  title         = {How Many Data Points is a Prompt Worth?},
  year          = {2021},
  month         = mar,
  abstract      = {When fine-tuning pretrained models for classification, researchers either use a generic model head or a task-specific prompt for prediction. Proponents of prompting have argued that prompts provide a method for injecting task-specific guidance, which is beneficial in low-data regimes. We aim to quantify this benefit through rigorous testing of prompts in a fair setting: comparing prompted and head-based fine-tuning in equal conditions across many tasks and data sizes. By controlling for many sources of advantage, we find that prompting does indeed provide a benefit, and that this benefit can be quantified per task. Results show that prompting is often worth 100s of data points on average across classification tasks.},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution 4.0 International},
  doi           = {10.48550/ARXIV.2103.08493},
  eprint        = {2103.08493},
  file          = {:Scao2021 - How Many Data Points Is a Prompt Worth_.pdf:PDF:http\://arxiv.org/pdf/2103.08493v2},
  keywords      = {Machine Learning (cs.LG), FOS: Computer and information sciences},
  primaryclass  = {cs.LG},
  publisher     = {arXiv},
}

@Article{Kumar2021,
  author        = {Kumar, Sawan and Talukdar, Partha},
  title         = {{Reordering Examples Helps during Priming-based Few-Shot Learning}},
  year          = {2021},
  month         = jun,
  abstract      = {The ability to learn from limited data, or few-shot learning, is a desirable and often critical requirement for NLP systems. While many existing methods do poorly at learning from a handful of examples, large pretrained language models have recently been shown to be efficient few-shot learners. One approach to few-shot learning, which does not require finetuning of model parameters, is to augment the language model's input with priming text which is typically constructed using task specific descriptions and examples. In this work, we further explore priming-based few-shot learning, with focus on using examples as prompts. We show that presenting examples in the right order is key for generalization. We introduce PERO (Prompting with Examples in the Right Order), where we formulate few-shot learning as search over the set of permutations of the training examples. We show that PERO can learn to generalize efficiently using as few as 10 examples, in contrast to existing approaches. While the newline token is a natural choice for separating the examples in the prompt, we show that learning a new separator token can potentially provide further gains in performance. We demonstrate the effectiveness of the proposed method on the tasks of sentiment classification, natural language inference and fact retrieval. Finally, we analyze the learned prompts to reveal novel insights, including the idea that two training examples in the right order alone can provide competitive performance for sentiment classification and natural language inference.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2106.01751},
  eprint        = {2106.01751},
  file          = {:Kumar2021 - Reordering Examples Helps during Priming Based Few Shot Learning.pdf:PDF:http\://arxiv.org/pdf/2106.01751v1},
  groups        = {Few-Shot Transfer},
  keywords      = {Computation and Language (cs.CL), FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  publisher     = {arXiv},
}

@InProceedings{Reynolds2021,
  author    = {Reynolds, Laria and McDonell, Kyle},
  booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
  title     = {Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm},
  year      = {2021},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {CHI EA '21},
  abstract  = {Prevailing methods for mapping large generative language models to supervised tasks may fail to sufficiently probe models’ novel capabilities. Using GPT-3 as a case study, we show that 0-shot prompts can significantly outperform few-shot prompts. We suggest that the function of few-shot examples in these cases is better described as locating an already learned task rather than meta-learning. This analysis motivates rethinking the role of prompts in controlling and evaluating powerful language models. We discuss methods of prompt programming, emphasizing the usefulness of considering prompts through the lens of natural language. We explore techniques for exploiting the capacity of narratives and cultural anchors to encode nuanced intentions and techniques for encouraging deconstruction of a problem into components before producing a verdict. Informed by this more encompassing theory of prompt programming, we also introduce the idea of a metaprompt that seeds the model to generate its own natural language prompts for a range of tasks. Finally, we discuss how these more general methods of interacting with language models can be incorporated into existing and future benchmarks and practical applications.},
  articleno = {314},
  doi       = {10.1145/3411763.3451760},
  file      = {:Reynolds2021 - Prompt Programming for Large Language Models_ beyond the Few Shot Paradigm.pdf:PDF},
  groups    = {Few-Shot Transfer},
  isbn      = {9781450380959},
  keywords  = {transformers, serial reasoning, semiotics, prompt programming, metaprompts, language models, few-shot learning, GPT-3},
  location  = {<conf-loc>, <city>Yokohama</city>, <country>Japan</country>, </conf-loc>},
  numpages  = {7},
}

@Article{Wallace2019,
  author        = {Wallace, Eric and Wang, Yizhong and Li, Sujian and Singh, Sameer and Gardner, Matt},
  title         = {Do NLP Models Know Numbers? Probing Numeracy in Embeddings},
  year          = {2019},
  month         = sep,
  abstract      = {The ability to understand and work with numbers (numeracy) is critical for many complex reasoning tasks. Currently, most NLP models treat numbers in text in the same way as other tokens---they embed them as distributed vectors. Is this enough to capture numeracy? We begin by investigating the numerical reasoning capabilities of a state-of-the-art question answering model on the DROP dataset. We find this model excels on questions that require numerical reasoning, i.e., it already captures numeracy. To understand how this capability emerges, we probe token embedding methods (e.g., BERT, GloVe) on synthetic list maximum, number decoding, and addition tasks. A surprising degree of numeracy is naturally present in standard embeddings. For example, GloVe and word2vec accurately encode magnitude for numbers up to 1,000. Furthermore, character-level embeddings are even more precise---ELMo captures numeracy the best for all pre-trained methods---but BERT, which uses sub-word units, is less exact.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1909.07940},
  eprint        = {1909.07940},
  file          = {:Wallace2019 - Do NLP Models Know Numbers_ Probing Numeracy in Embeddings.pdf:PDF:http\://arxiv.org/pdf/1909.07940v2},
  keywords      = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  publisher     = {arXiv},
}

@Article{Gao2020,
  author        = {Gao, Tianyu and Fisch, Adam and Chen, Danqi},
  title         = {Making Pre-trained Language Models Better Few-shot Learners},
  year          = {2020},
  month         = dec,
  abstract      = {The recent GPT-3 model (Brown et al., 2020) achieves remarkable few-shot performance solely by leveraging a natural-language prompt and a few task demonstrations as input context. Inspired by their findings, we study few-shot learning in a more practical scenario, where we use smaller language models for which fine-tuning is computationally efficient. We present LM-BFF--better few-shot fine-tuning of language models--a suite of simple and complementary techniques for fine-tuning language models on a small number of annotated examples. Our approach includes (1) prompt-based fine-tuning together with a novel pipeline for automating prompt generation; and (2) a refined strategy for dynamically and selectively incorporating demonstrations into each context. Finally, we present a systematic evaluation for analyzing few-shot performance on a range of NLP tasks, including classification and regression. Our experiments demonstrate that our methods combine to dramatically outperform standard fine-tuning procedures in this low resource setting, achieving up to 30% absolute improvement, and 11% on average across all tasks. Our approach makes minimal assumptions on task resources and domain expertise, and hence constitutes a strong task-agnostic method for few-shot learning.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2012.15723},
  eprint        = {2012.15723},
  file          = {:Gao2020 - Making Pre Trained Language Models Better Few Shot Learners.pdf:PDF:http\://arxiv.org/pdf/2012.15723v2},
  groups        = {Few-Shot Transfer},
  keywords      = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  publisher     = {arXiv},
}

@Article{Petroni2019,
  author        = {Petroni, Fabio and Rocktäschel, Tim and Lewis, Patrick and Bakhtin, Anton and Wu, Yuxiang and Miller, Alexander H. and Riedel, Sebastian},
  title         = {Language Models as Knowledge Bases?},
  year          = {2019},
  month         = sep,
  abstract      = {Recent progress in pretraining language models on large textual corpora led to a surge of improvements for downstream NLP tasks. Whilst learning linguistic knowledge, these models may also be storing relational knowledge present in the training data, and may be able to answer queries structured as "fill-in-the-blank" cloze statements. Language models have many advantages over structured knowledge bases: they require no schema engineering, allow practitioners to query about an open class of relations, are easy to extend to more data, and require no human supervision to train. We present an in-depth analysis of the relational knowledge already present (without fine-tuning) in a wide range of state-of-the-art pretrained language models. We find that (i) without fine-tuning, BERT contains relational knowledge competitive with traditional NLP methods that have some access to oracle knowledge, (ii) BERT also does remarkably well on open-domain question answering against a supervised baseline, and (iii) certain types of factual knowledge are learned much more readily than others by standard language model pretraining approaches. The surprisingly strong ability of these models to recall factual knowledge without any fine-tuning demonstrates their potential as unsupervised open-domain QA systems. The code to reproduce our analysis is available at https://github.com/facebookresearch/LAMA.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1909.01066},
  eprint        = {1909.01066},
  file          = {:Petroni2019 - Language Models As Knowledge Bases_.pdf:PDF:http\://arxiv.org/pdf/1909.01066v2},
  keywords      = {Computation and Language (cs.CL), FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  publisher     = {arXiv},
}

@InProceedings{Yuan2021a,
  author    = {Yuan, Weizhe and Neubig, Graham and Liu, Pengfei},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {BARTScore: Evaluating Generated Text as Text Generation},
  year      = {2021},
  editor    = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
  pages     = {27263--27277},
  publisher = {Curran Associates, Inc.},
  volume    = {34},
  file      = {:Yuan2021a - BARTScore_ Evaluating Generated Text As Text Generation.pdf:PDF},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2021/file/e4d2b6e6fdeca3e60e0f1a62fee3d9dd-Paper.pdf},
}

@InProceedings{Ilievski2017,
  author    = {Ilievski, Ilija and Feng, Jiashi},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Multimodal Learning and Reasoning for Visual Question Answering},
  year      = {2017},
  editor    = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  publisher = {Curran Associates, Inc.},
  volume    = {30},
  abstract  = {Reasoning about entities and their relationships from multimodal data is a key goal of Artificial General Intelligence. The visual question answering (VQA) problem is an excellent way to test such reasoning capabilities of an AI model and its multimodal representation learning. However, the current VQA models are over-simplified deep neural networks, comprised of a long short-term memory (LSTM) unit for question comprehension and a convolutional neural network (CNN) for learning single image representation. We argue that the single visual representation contains a limited and general information about the image contents and thus limits the model reasoning capabilities. In this work we introduce a modular neural network model that learns a multimodal and multifaceted representation of the image and the question. The proposed model learns to use the multimodal representation to reason about the image entities and achieves a new state-of-the-art performance on both VQA benchmark datasets, VQA v1.0 and v2.0, by a wide margin.},
  file      = {:Ilievski2017 - Multimodal Learning and Reasoning for Visual Question Answering.pdf:PDF},
  groups    = {VIsual Question Answering},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2017/file/f61d6947467ccd3aa5af24db320235dd-Paper.pdf},
}

@Article{Schick2021,
  author        = {Timo Schick and Hinrich Schütze},
  title         = {It's Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners},
  year          = {2021},
  month         = sep,
  abstract      = {When scaled to hundreds of billions of parameters, pretrained language models such as GPT-3 (Brown et al., 2020) achieve remarkable few-shot performance. However, enormous amounts of compute are required for training and applying such big models, resulting in a large carbon footprint and making it difficult for researchers and practitioners to use them. We show that performance similar to GPT-3 can be obtained with language models that are much "greener" in that their parameter count is several orders of magnitude smaller. This is achieved by converting textual inputs into cloze questions that contain a task description, combined with gradient-based optimization; exploiting unlabeled data gives further improvements. We identify key factors required for successful natural language understanding with small language models.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/arxiv.2009.07118},
  eprint        = {2009.07118},
  file          = {:Schick2021 - It's Not Just Size That Matters_ Small Language Models Are Also Few Shot Learners.pdf:PDF},
  keywords      = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  publisher     = {arXiv},
}

@InProceedings{Ding2022,
  author    = {Ding, Yang and Yu, Jing and Liu, Bang and Hu, Yue and Cui, Mingxin and Wu, Qi},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {MuKEA: Multimodal Knowledge Extraction and Accumulation for Knowledge-Based Visual Question Answering},
  year      = {2022},
  month     = {June},
  pages     = {5089-5098},
  file      = {:Ding2022 - MuKEA_ Multimodal Knowledge Extraction and Accumulation for Knowledge Based Visual Question Answering.pdf:PDF},
  groups    = {VIsual Question Answering},
}

@Article{Kafle2017,
  author   = {Kushal Kafle and Christopher Kanan},
  journal  = {Computer Vision and Image Understanding},
  title    = {Visual question answering: Datasets, algorithms, and future challenges},
  year     = {2017},
  issn     = {1077-3142},
  note     = {Language in Vision},
  pages    = {3-20},
  volume   = {163},
  abstract = {Visual Question Answering (VQA) is a recent problem in computer vision and natural language processing that has garnered a large amount of interest from the deep learning, computer vision, and natural language processing communities. In VQA, an algorithm needs to answer text-based questions about images. Since the release of the first VQA dataset in 2014, additional datasets have been released and many algorithms have been proposed. In this review, we critically examine the current state of VQA in terms of problem formulation, existing datasets, evaluation metrics, and algorithms. In particular, we discuss the limitations of current datasets with regard to their ability to properly train and assess VQA algorithms. We then exhaustively review existing algorithms for VQA. Finally, we discuss possible future directions for VQA and image understanding research.},
  doi      = {10.1016/j.cviu.2017.06.005},
  file     = {:Kafle2017 - Visual Question Answering_ Datasets, Algorithms, and Future Challenges.pdf:PDF},
  groups   = {VIsual Question Answering},
  keywords = {Image understanding, Natural language processing, Vision and language},
  url      = {https://www.sciencedirect.com/science/article/pii/S1077314217301170},
}

@Article{Wu2017,
  author   = {Qi Wu and Damien Teney and Peng Wang and Chunhua Shen and Anthony Dick and Anton {van den Hengel}},
  journal  = {Computer Vision and Image Understanding},
  title    = {Visual question answering: A survey of methods and datasets},
  year     = {2017},
  issn     = {1077-3142},
  note     = {Language in Vision},
  pages    = {21-40},
  volume   = {163},
  abstract = {Visual Question Answering (VQA) is a challenging task that has received increasing attention from both the computer vision and the natural language processing communities. Given an image and a question in natural language, it requires reasoning over visual elements of the image and general knowledge to infer the correct answer. In the first part of this survey, we examine the state of the art by comparing modern approaches to the problem. We classify methods by their mechanism to connect the visual and textual modalities. In particular, we examine the common approach of combining convolutional and recurrent neural networks to map images and questions to a common feature space. We also discuss memory-augmented and modular architectures that interface with structured knowledge bases. In the second part of this survey, we review the datasets available for training and evaluating VQA systems. The various datatsets contain questions at different levels of complexity, which require different capabilities and types of reasoning. We examine in depth the question/answer pairs from the Visual Genome project, and evaluate the relevance of the structured annotations of images with scene graphs for VQA. Finally, we discuss promising future directions for the field, in particular the connection to structured knowledge bases and the use of natural language processing models.},
  doi      = {10.1016/j.cviu.2017.05.001},
  file     = {:Wu2017 - Visual Question Answering_ a Survey of Methods and Datasets.pdf:PDF},
  groups   = {VIsual Question Answering},
  keywords = {Visual question answering, Natural language processing, Knowledge bases, Recurrent neural networks},
  url      = {https://www.sciencedirect.com/science/article/pii/S1077314217300772},
}

@InProceedings{Antol2015,
  author    = {Antol, Stanislaw and Agrawal, Aishwarya and Lu, Jiasen and Mitchell, Margaret and Batra, Dhruv and Zitnick, C. Lawrence and Parikh, Devi},
  booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  title     = {VQA: Visual Question Answering},
  year      = {2015},
  month     = {December},
  file      = {:Antol2015 - VQA_ Visual Question Answering.pdf:PDF},
  groups    = {VIsual Question Answering},
}

@InProceedings{Teney2018,
  author    = {Teney, Damien and Anderson, Peter and He, Xiaodong and van den Hengel, Anton},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Tips and Tricks for Visual Question Answering: Learnings From the 2017 Challenge},
  year      = {2018},
  month     = {June},
  file      = {:Teney2018 - Tips and Tricks for Visual Question Answering_ Learnings from the 2017 Challenge.pdf:PDF;:0021-supp.pdf:PDF},
  groups    = {VIsual Question Answering},
}

@InProceedings{Karpathy_2014_CVPR,
  author    = {Karpathy, Andrej and Toderici, George and Shetty, Sanketh and Leung, Thomas and Sukthankar, Rahul and Fei-Fei, Li},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Large-scale Video Classification with Convolutional Neural Networks},
  year      = {2014},
  month     = {June},
  file      = {:Karpathy_2014_CVPR - Large Scale Video Classification with Convolutional Neural Networks.pdf:PDF},
}

@InProceedings{Shao2023,
  author    = {Shao, Zhenwei and Yu, Zhou and Wang, Meng and Yu, Jun},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Prompting Large Language Models With Answer Heuristics for Knowledge-Based Visual Question Answering},
  year      = {2023},
  month     = {June},
  pages     = {14974-14983},
  file      = {:Shao2023 - Prompting Large Language Models with Answer Heuristics for Knowledge Based Visual Question Answering.pdf:PDF;:Shao_Prompting_Large_Language_CVPR_2023_supplemental.pdf:PDF},
  groups    = {VIsual Question Answering},
}

@InProceedings{Liu2019,
  author    = {Liu, Shikun and Davison, Andrew and Johns, Edward},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Self-Supervised Generalisation with Meta Auxiliary Learning},
  year      = {2019},
  editor    = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  publisher = {Curran Associates, Inc.},
  volume    = {32},
  abstract  = {Learning with auxiliary tasks can improve the ability of a primary task to generalise. However, this comes at the cost of manually labelling auxiliary data. We propose a new method which automatically learns appropriate labels for an auxiliary task, such that any supervised learning task can be improved without requiring access to any further data. The approach is to train two neural networks: a label-generation network to predict the auxiliary labels, and a multi-task network to train the primary task alongside the auxiliary task. The loss for the label-generation network incorporates the loss of the multi-task network, and so this interaction between the two networks can be seen as a form of meta learning with a double gradient. We show that our proposed method, Meta AuXiliary Learning (MAXL), outperforms single-task learning on 7 image datasets, without requiring any additional data. We also show that MAXL outperforms several other baselines for generating auxiliary labels, and is even competitive when compared with human-defined auxiliary labels. The self-supervised nature of our method leads to a promising new direction towards automated generalisation. Source code can be found at https://github.com/lorenmt/maxl.},
  file      = {:Liu2019 - Self Supervised Generalisation with Meta Auxiliary Learning.pdf:PDF;:MAXL_NIPS_supp.pdf:PDF},
  groups    = {Auxiliary Training Objectives, Reading Reports},
  priority  = {prio1},
  ranking   = {rank4},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2019/file/92262bf907af914b95a0fc33c3f33bf6-Paper.pdf},
}

@InProceedings{Li2023,
  author    = {Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  title     = {{BLIP}-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models},
  year      = {2023},
  editor    = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  month     = {23--29 Jul},
  pages     = {19730--19742},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {202},
  abstract  = {The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model’s emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.},
  file      = {:Li2023 - BLIP 2_ Bootstrapping Language Image Pre Training with Frozen Image Encoders and Large Language Models.pdf:PDF},
  groups    = {VIsual Question Answering, Models, Reading Reports},
  pdf       = {https://proceedings.mlr.press/v202/li23q/li23q.pdf},
  priority  = {prio2},
  url       = {https://proceedings.mlr.press/v202/li23q.html},
}

@InProceedings{Hu2023,
  author    = {Hu, Yushi and Liu, Benlin and Kasai, Jungo and Wang, Yizhong and Ostendorf, Mari and Krishna, Ranjay and Smith, Noah A.},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  title     = {TIFA: Accurate and Interpretable Text-to-Image Faithfulness Evaluation with Question Answering},
  year      = {2023},
  month     = {October},
  pages     = {20406-20417},
  file      = {:Hu2023 - TIFA_ Accurate and Interpretable Text to Image Faithfulness Evaluation with Question Answering.pdf:PDF},
  groups    = {VIsual Question Answering},
}

@InProceedings{Parelli2023,
  author    = {Parelli, Maria and Delitzas, Alexandros and Hars, Nikolas and Vlassis, Georgios and Anagnostidis, Sotirios and Bachmann, Gregor and Hofmann, Thomas},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
  title     = {CLIP-Guided Vision-Language Pre-Training for Question Answering in 3D Scenes},
  year      = {2023},
  month     = {June},
  pages     = {5607-5612},
  file      = {:Parelli2023 - CLIP Guided Vision Language Pre Training for Question Answering in 3D Scenes.pdf:PDF},
  groups    = {VIsual Question Answering},
}

@InProceedings{Guo2023,
  author    = {Guo, Jiaxian and Li, Junnan and Li, Dongxu and Tiong, Anthony Meng Huat and Li, Boyang and Tao, Dacheng and Hoi, Steven},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {From Images to Textual Prompts: Zero-Shot Visual Question Answering With Frozen Large Language Models},
  year      = {2023},
  month     = {June},
  pages     = {10867-10877},
  file      = {:Guo2023 - From Images to Textual Prompts_ Zero Shot Visual Question Answering with Frozen Large Language Models.pdf:PDF},
  groups    = {VIsual Question Answering},
}

@InProceedings{Wu2018,
  author    = {Wu, Chenfei and Liu, Jinlai and Wang, Xiaojie and Dong, Xuan},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Chain of Reasoning for Visual Question Answering},
  year      = {2018},
  editor    = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
  publisher = {Curran Associates, Inc.},
  volume    = {31},
  file      = {:Wu2018 - Chain of Reasoning for Visual Question Answering.pdf:PDF},
  groups    = {VIsual Question Answering},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2018/file/31fefc0e570cb3860f2a6d4b38c6490d-Paper.pdf},
}

@Article{He2021,
  author   = {Feijuan He and Yaxian Wang and Xianglin Miao and Xia Sun},
  journal  = {Image and Vision Computing},
  title    = {Interpretable visual reasoning: A survey},
  year     = {2021},
  issn     = {0262-8856},
  pages    = {104194},
  volume   = {112},
  abstract = {Visual reasoning refers to the process of solving questions about visual information. At present, most visual reasoning models are mainly based on deep learning and end-to-end architecture. Although these models have achieved good performance, they are usually black boxes for users, and it is difficult to understand the basic rationales of the reasoning process. In recent years, the academic community has realized the importance of interpretability in visual reasoning and has developed a series of Interpretable Visual Reasoning (IVR) models. In this paper, we review these models. First, we have established a taxonomy based on four explanation forms of vision, text, graph and symbol used in current visual reasoning. Secondly, we explore the typical IVR models of each category and analyze their pros and cons. Thirdly, we elaborate on the current mainstream datasets about visual reasoning and VQA, and analyze how these datasets promote IVR research from different perspectives. Finally, we summarize the challenges for IVR and point out potential research directions.},
  doi      = {10.1016/j.imavis.2021.104194},
  file     = {:He2021 - Interpretable Visual Reasoning_ a Survey.pdf:PDF},
  keywords = {Visual question answering, Visual reasoning, Interpretability, Datasets, Survey},
  url      = {https://www.sciencedirect.com/science/article/pii/S0262885621000998},
}

@InProceedings{Lee2020,
  author    = {Lee, Hankook and Hwang, Sung Ju and Shin, Jinwoo},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  title     = {Self-supervised Label Augmentation via Input Transformations},
  year      = {2020},
  editor    = {III, Hal Daumé and Singh, Aarti},
  month     = {13--18 Jul},
  pages     = {5714--5724},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {119},
  abstract  = {Self-supervised learning, which learns by constructing artificial labels given only the input signals, has recently gained considerable attention for learning representations with unlabeled datasets, i.e., learning without any human-annotated supervision. In this paper, we show that such a technique can be used to significantly improve the model accuracy even under fully-labeled datasets. Our scheme trains the model to learn both original and self-supervised tasks, but is different from conventional multi-task learning frameworks that optimize the summation of their corresponding losses. Our main idea is to learn a single unified task with respect to the joint distribution of the original and self-supervised labels, i.e., we augment original labels via self-supervision. This simple, yet effective approach allows to train models easier by relaxing a certain invariant constraint during learning the original and self-supervised tasks simultaneously. It also enables an aggregated inference which combines the predictions from different augmentations to improve the prediction accuracy. Furthermore, we propose a novel knowledge transfer technique, which we refer to as self-distillation, that has the effect of the aggregated inference in a single (faster) inference. We demonstrate the large accuracy improvement and wide applicability of our framework on various fully-supervised settings, e.g., the few-shot and imbalanced classification scenarios.},
  file      = {:Lee2020 - Self Supervised Label Augmentation Via Input Transformations.pdf:PDF},
  groups    = {Auxiliary Training Objectives, Few-Shot Transfer},
  pdf       = {http://proceedings.mlr.press/v119/lee20c/lee20c.pdf},
  url       = {https://proceedings.mlr.press/v119/lee20c.html},
}

@InProceedings{Xu2021,
  author    = {Xu, Lian and Ouyang, Wanli and Bennamoun, Mohammed and Boussaid, Farid and Sohel, Ferdous and Xu, Dan},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  title     = {Leveraging Auxiliary Tasks With Affinity Learning for Weakly Supervised Semantic Segmentation},
  year      = {2021},
  month     = {October},
  pages     = {6984-6993},
  file      = {:Xu2021 - Leveraging Auxiliary Tasks with Affinity Learning for Weakly Supervised Semantic Segmentation.pdf:PDF},
  groups    = {Auxiliary Training Objectives},
}

@InProceedings{Zhu2020,
  author    = {Zhu, Fengda and Zhu, Yi and Chang, Xiaojun and Liang, Xiaodan},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Vision-Language Navigation With Self-Supervised Auxiliary Reasoning Tasks},
  year      = {2020},
  month     = {June},
  file      = {:Zhu2020 - Vision Language Navigation with Self Supervised Auxiliary Reasoning Tasks.pdf:PDF},
  groups    = {Auxiliary Training Objectives},
}

@Article{Liu2022,
  author       = {Liu, Xianpeng and Xue, Nan and Wu, Tianfu},
  journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
  title        = {Learning Auxiliary Monocular Contexts Helps Monocular 3D Object Detection},
  year         = {2022},
  month        = {Jun.},
  number       = {2},
  pages        = {1810-1818},
  volume       = {36},
  abstractnote = {Monocular 3D object detection aims to localize 3D bounding boxes in an input single 2D image. It is a highly challenging problem and remains open, especially when no extra information (e.g., depth, lidar and/or multi-frames) can be leveraged in training and/or inference. This paper proposes a simple yet effective formulation for monocular 3D object detection without exploiting any extra information. It presents the MonoCon method which learns Monocular Contexts, as auxiliary tasks in training, to help monocular 3D object detection. The key idea is that with the annotated 3D bounding boxes of objects in an image, there is a rich set of well-posed projected 2D supervision signals available in training, such as the projected corner keypoints and their associated offset vectors with respect to the center of 2D bounding box, which should be exploited as auxiliary tasks in training. The proposed MonoCon is motivated by the Cramer–Wold theorem in measure theory at a high level. In implementation, it utilizes a very simple end-to-end design to justify the effectiveness of learning auxiliary monocular contexts, which consists of three components: a Deep Neural Network (DNN) based feature backbone, a number of regression head branches for learning the essential parameters used in the 3D bounding box prediction, and a number of regression head branches for learning auxiliary contexts. After training, the auxiliary context regression branches are discarded for better inference efficiency. In experiments, the proposed MonoCon is tested in the KITTI benchmark (car, pedestrian and cyclist). It outperforms all prior arts in the leaderboard on the car category and obtains comparable performance on pedestrian and cyclist in terms of accuracy. Thanks to the simple design, the proposed MonoCon method obtains the fastest inference speed with 38.7 fps in comparisons. Our code is released at https://git.io/MonoCon.},
  doi          = {10.1609/aaai.v36i2.20074},
  file         = {:Liu2022 - Learning Auxiliary Monocular Contexts Helps Monocular 3D Object Detection.pdf:PDF},
  groups       = {Auxiliary Training Objectives},
  url          = {https://ojs.aaai.org/index.php/AAAI/article/view/20074},
}

@Article{Tian2024,
  author   = {Songsong Tian and Lusi Li and Weijun Li and Hang Ran and Xin Ning and Prayag Tiwari},
  journal  = {Neural Networks},
  title    = {A survey on few-shot class-incremental learning},
  year     = {2024},
  issn     = {0893-6080},
  pages    = {307-324},
  volume   = {169},
  abstract = {Large deep learning models are impressive, but they struggle when real-time data is not available. Few-shot class-incremental learning (FSCIL) poses a significant challenge for deep neural networks to learn new tasks from just a few labeled samples without forgetting the previously learned ones. This setup can easily leads to catastrophic forgetting and overfitting problems, severely affecting model performance. Studying FSCIL helps overcome deep learning model limitations on data volume and acquisition time, while improving practicality and adaptability of machine learning models. This paper provides a comprehensive survey on FSCIL. Unlike previous surveys, we aim to synthesize few-shot learning and incremental learning, focusing on introducing FSCIL from two perspectives, while reviewing over 30 theoretical research studies and more than 20 applied research studies. From the theoretical perspective, we provide a novel categorization approach that divides the field into five subcategories, including traditional machine learning methods, meta learning-based methods, feature and feature space-based methods, replay-based methods, and dynamic network structure-based methods. We also evaluate the performance of recent theoretical research on benchmark datasets of FSCIL. From the application perspective, FSCIL has achieved impressive achievements in various fields of computer vision such as image classification, object detection, and image segmentation, as well as in natural language processing and graph. We summarize the important applications. Finally, we point out potential future research directions, including applications, problem setups, and theory development. Overall, this paper offers a comprehensive analysis of the latest advances in FSCIL from a methodological, performance, and application perspective.},
  doi      = {10.1016/j.neunet.2023.10.039},
  file     = {:Tian2024 - A Survey on Few Shot Class Incremental Learning.pdf:PDF},
  groups   = {Few-Shot Transfer},
  keywords = {Few-shot learning, Class-incremental learning, Catastrophic forgetting, Overfitting, Performance evaluation},
  url      = {https://www.sciencedirect.com/science/article/pii/S0893608023006019},
}

@InProceedings{Agrawal2018,
  author    = {Agrawal, Aishwarya and Batra, Dhruv and Parikh, Devi and Kembhavi, Aniruddha},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Don't Just Assume; Look and Answer: Overcoming Priors for Visual Question Answering},
  year      = {2018},
  month     = {June},
  file      = {:Agrawal2018 - Don't Just Assume\; Look and Answer_ Overcoming Priors for Visual Question Answering.pdf:PDF},
  groups    = {VIsual Question Answering},
}

@InProceedings{Doersch2017,
  author    = {Doersch, Carl and Zisserman, Andrew},
  booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  title     = {Multi-Task Self-Supervised Visual Learning},
  year      = {2017},
  month     = {Oct},
  file      = {:Doersch2017 - Multi Task Self Supervised Visual Learning.pdf:PDF},
}

@InProceedings{Deng2009,
  author    = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Kai Li and Li Fei-Fei},
  booktitle = {2009 IEEE Conference on Computer Vision and Pattern Recognition},
  title     = {{ImageNet: A Large-Scale Hierarchical Image Database}},
  year      = {2009},
  pages     = {248-255},
  doi       = {10.1109/CVPR.2009.5206848},
  file      = {:Deng2009 - ImageNet_ a Large Scale Hierarchical Image Database.pdf:PDF},
  groups    = {Datasets},
  keywords  = {Large-scale systems,Image databases,Explosions,Internet,Robustness,Information retrieval,Image retrieval,Multimedia databases,Ontologies,Spine},
}

@Article{Qi2022,
  author   = {Qi, Guo-Jun and Luo, Jiebo},
  journal  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title    = {Small Data Challenges in Big Data Era: A Survey of Recent Progress on Unsupervised and Semi-Supervised Methods},
  year     = {2022},
  number   = {4},
  pages    = {2168-2187},
  volume   = {44},
  doi      = {10.1109/TPAMI.2020.3031898},
  file     = {:Qi2022 - Small Data Challenges in Big Data Era_ a Survey of Recent Progress on Unsupervised and Semi Supervised Methods.pdf:PDF},
  keywords = {Data models,Task analysis,Training,Adaptation models,Gallium nitride,Big Data,Supervised learning,Unsupervised methods,semi-supervised methods,domain adaptation,transformation equivariance and invariance,disentangled representations,generative models,auto-encoders,generative adversarial networks,auto-regressive models,flow-based generative models,transformers,self-supervised methods,teach-student models,instance discrimination and equivariance},
}

@InProceedings{Singh2021,
  author        = {Singh, Amanpreet and Hu, Ronghang and Goswami, Vedanuj and Couairon, Guillaume and Galuba, Wojciech and Rohrbach, Marcus and Kiela, Douwe},
  booktitle     = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title         = {{FLAVA: A Foundational Language And Vision Alignment Model}},
  year          = {2021},
  month         = dec,
  publisher     = {arXiv},
  abstract      = {State-of-the-art vision and vision-and-language models rely on large-scale visio-linguistic pretraining for obtaining good performance on a variety of downstream tasks. Generally, such models are often either cross-modal (contrastive) or multi-modal (with earlier fusion) but not both; and they often only target specific modalities or tasks. A promising direction would be to use a single holistic universal model, as a "foundation", that targets all modalities at once -- a true vision and language foundation model should be good at vision tasks, language tasks, and cross- and multi-modal vision and language tasks. We introduce FLAVA as such a model and demonstrate impressive performance on a wide range of 35 tasks spanning these target modalities.},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution 4.0 International},
  doi           = {10.48550/ARXIV.2112.04482},
  eprint        = {2112.04482},
  file          = {:Singh2021 - FLAVA_ a Foundational Language and Vision Alignment Model.pdf:PDF:http\://arxiv.org/pdf/2112.04482v3},
  groups        = {Models, Essays},
  keywords      = {Computer Vision and Pattern Recognition (cs.CV), Computation and Language (cs.CL), FOS: Computer and information sciences},
  primaryclass  = {cs.CV},
}

@Misc{Devlin2019,
  author        = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  title         = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  year          = {2019},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.1810.04805},
  eprint        = {1810.04805},
  file          = {:Devlin2019 - BERT_ Pre Training of Deep Bidirectional Transformers for Language Understanding.pdf:PDF},
  groups        = {Models},
  primaryclass  = {cs.CL},
}

@Misc{Wang2019,
  author        = {Alex Wang and Amanpreet Singh and Julian Michael and Felix Hill and Omer Levy and Samuel R. Bowman},
  title         = {GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
  year          = {2019},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.1804.07461},
  eprint        = {1804.07461},
  file          = {:Wang2019 - GLUE_ a Multi Task Benchmark and Analysis Platform for Natural Language Understanding.pdf:PDF},
  groups        = {Datasets},
  primaryclass  = {cs.CL},
}

@Article{Zhou2022,
  author    = {Zhou, Kaiyang and Yang, Jingkang and Loy, Chen Change and Liu, Ziwei},
  journal   = {International Journal of Computer Vision},
  title     = {{Learning to Prompt for Vision-Language Models}},
  year      = {2022},
  number    = {9},
  pages     = {2337--2348},
  volume    = {130},
  doi       = {10.1007/s11263-022-01653-1},
  file      = {:Zhou2022 - Learning to Prompt for Vision Language Models.pdf:PDF},
  groups    = {Few-Shot Transfer},
  priority  = {prio1},
  publisher = {Springer},
}

@Article{Abdin2024,
  author        = {Abdin, Marah and Jacobs, Sam Ade and Awan, Ammar Ahmad and Aneja, Jyoti and Awadallah, Ahmed and Awadalla, Hany and Bach, Nguyen and Bahree, Amit and Bakhtiari, Arash and Bao, Jianmin and Behl, Harkirat and Benhaim, Alon and Bilenko, Misha and Bjorck, Johan and Bubeck, Sébastien and Cai, Qin and Cai, Martin and Mendes, Caio César Teodoro and Chen, Weizhu and Chaudhary, Vishrav and Chen, Dong and Chen, Dongdong and Chen, Yen-Chun and Chen, Yi-Ling and Chopra, Parul and Dai, Xiyang and Del Giorno, Allie and de Rosa, Gustavo and Dixon, Matthew and Eldan, Ronen and Fragoso, Victor and Iter, Dan and Gao, Mei and Gao, Min and Gao, Jianfeng and Garg, Amit and Goswami, Abhishek and Gunasekar, Suriya and Haider, Emman and Hao, Junheng and Hewett, Russell J. and Huynh, Jamie and Javaheripi, Mojan and Jin, Xin and Kauffmann, Piero and Karampatziakis, Nikos and Kim, Dongwoo and Khademi, Mahoud and Kurilenko, Lev and Lee, James R. and Lee, Yin Tat and Li, Yuanzhi and Li, Yunsheng and Liang, Chen and Liden, Lars and Liu, Ce and Liu, Mengchen and Liu, Weishung and Lin, Eric and Lin, Zeqi and Luo, Chong and Madan, Piyush and Mazzola, Matt and Mitra, Arindam and Modi, Hardik and Nguyen, Anh and Norick, Brandon and Patra, Barun and Perez-Becker, Daniel and Portet, Thomas and Pryzant, Reid and Qin, Heyang and Radmilac, Marko and Rosset, Corby and Roy, Sambudha and Ruwase, Olatunji and Saarikivi, Olli and Saied, Amin and Salim, Adil and Santacroce, Michael and Shah, Shital and Shang, Ning and Sharma, Hiteshi and Shukla, Swadheen and Song, Xia and Tanaka, Masahiro and Tupini, Andrea and Wang, Xin and Wang, Lijuan and Wang, Chunyu and Wang, Yu and Ward, Rachel and Wang, Guanhua and Witte, Philipp and Wu, Haiping and Wyatt, Michael and Xiao, Bin and Xu, Can and Xu, Jiahang and Xu, Weijian and Yadav, Sonali and Yang, Fan and Yang, Jianwei and Yang, Ziyi and Yang, Yifan and Yu, Donghan and Yuan, Lu and Zhang, Chengruidong and Zhang, Cyril and Zhang, Jianwen and Zhang, Li Lyna and Zhang, Yi and Zhang, Yue and Zhang, Yunan and Zhou, Xiren},
  title         = {{Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone}},
  year          = {2024},
  month         = apr,
  abstract      = {We introduce phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT-3.5 (e.g., phi-3-mini achieves 69% on MMLU and 8.38 on MT-bench), despite being small enough to be deployed on a phone. The innovation lies entirely in our dataset for training, a scaled-up version of the one used for phi-2, composed of heavily filtered publicly available web data and synthetic data. The model is also further aligned for robustness, safety, and chat format. We also provide some initial parameter-scaling results with a 7B and 14B models trained for 4.8T tokens, called phi-3-small and phi-3-medium, both significantly more capable than phi-3-mini (e.g., respectively 75% and 78% on MMLU, and 8.7 and 8.9 on MT-bench). Moreover, we also introduce phi-3-vision, a 4.2 billion parameter model based on phi-3-mini with strong reasoning capabilities for image and text prompts.},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution 4.0 International},
  doi           = {10.48550/ARXIV.2404.14219},
  eprint        = {2404.14219},
  file          = {:Abdin2024 - Phi 3 Technical Report_ a Highly Capable Language Model Locally on Your Phone.pdf:PDF:http\://arxiv.org/pdf/2404.14219v3},
  groups        = {Models},
  keywords      = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  publisher     = {arXiv},
}

@InProceedings{Zhang2015,
  author       = {Zhang, Qilin and Hua, Gang and Liu, Wei and Liu, Zicheng and Zhang, Zhengyou},
  booktitle    = {Computer Vision--ACCV 2014: 12th Asian Conference on Computer Vision, Singapore, Singapore, November 1-5, 2014, Revised Selected Papers, Part I 12},
  title        = {Can visual recognition benefit from auxiliary information in training?},
  year         = {2015},
  organization = {Springer},
  pages        = {65--80},
  file         = {:Zhang2015 - Can Visual Recognition Benefit from Auxiliary Information in Training_.pdf:PDF},
}

@Article{Liebel2018,
  author        = {Lukas Liebel and Marco Körner},
  journal       = {arXiv preprint},
  title         = {{Auxiliary Tasks in Multi-task Learning}},
  year          = {2018},
  abstract      = {Multi-task convolutional neural networks (CNNs) have shown impressive results for certain combinations of tasks, such as single-image depth estimation (SIDE) and semantic segmentation. This is achieved by pushing the network towards learning a robust representation that generalizes well to different atomic tasks. We extend this concept by adding auxiliary tasks, which are of minor relevance for the application, to the set of learned tasks. As a kind of additional regularization, they are expected to boost the performance of the ultimately desired main tasks. To study the proposed approach, we picked vision-based road scene understanding (RSU) as an exemplary application. Since multi-task learning requires specialized datasets, particularly when using extensive sets of tasks, we provide a multi-modal dataset for multi-task RSU, called synMT. More than $ 2.5 \cdot 10^5 $ synthetic images, annotated with 21 different labels, were acquired from the video game Grand Theft Auto V (GTA V). Our proposed deep multi-task CNN architecture was trained on various combination of tasks using synMT. The experiments confirmed that auxiliary tasks can indeed boost network performance, both in terms of final results and training time.},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.1805.06334},
  eprint        = {1805.06334},
  file          = {:Liebel2018 - Auxiliary Tasks in Multi Task Learning.pdf:PDF},
  groups        = {Reading Reports},
  primaryclass  = {cs.CV},
}

@Article{Zhang2021,
  author   = {Junjie Zhang and Lingqiao Liu and Peng Wang and Jian Zhang},
  journal  = {Neurocomputing},
  title    = {Exploring the auxiliary learning for long-tailed visual recognition},
  year     = {2021},
  issn     = {0925-2312},
  pages    = {303-314},
  volume   = {449},
  abstract = {Real-world visual data often exhibits a long-tailed distribution, where some “head” classes have a large number of samples, yet only a few samples are available for “tail” classes. The fundamental problem of learning with the imbalanced data is that insufficient training samples easily lead to the over-fitting of feature extractor and classifier for tail classes, which can be boiled down into a dilemma: on the one hand, we prefer to increase the exposure of tail class samples to avoid the excessive dominance of head classes in the classifier training. On the other hand, oversampling tail classes makes the network prone to over-fitting, since head class samples are often consequently under-represented. To resolve this dilemma, in this paper, we propose an effective auxiliary learning approach. The key idea is to split a network into a classifier part and a feature extractor part, and then employ different training strategies for each part in an auxiliary learning manner. Specifically, to promote the awareness of tail-classes, a class-balanced sampling scheme is utilised for training both the classifier and the feature extractor as the primary task. For the feature extractor, we also introduce an auxiliary training task, which is to train a classifier under the regular random sampling scheme. In this way, the feature extractor is jointly trained from both sampling strategies and thus can take advantage of all training data and avoid the over-fitting issue. Apart from this basic auxiliary task, we further explore the benefits of different types of auxiliary tasks for improving the generality of learned features, including self-supervised learning and class-wise re-weighting. Without using any bells and whistles, our model compares favourably over state-of-the-art solutions.},
  doi      = {10.1016/j.neucom.2021.03.096},
  file     = {:Zhang2021 - Exploring the Auxiliary Learning for Long Tailed Visual Recognition.pdf:PDF},
  keywords = {Long-tail, Class balance, Auxiliary learning},
  url      = {https://www.sciencedirect.com/science/article/pii/S0925231221004823},
}

@InProceedings{Jiang2021,
  author    = {Jiang, Zi-Hang and Hou, Qibin and Yuan, Li and Zhou, Daquan and Shi, Yujun and Jin, Xiaojie and Wang, Anran and Feng, Jiashi},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {All Tokens Matter: Token Labeling for Training Better Vision Transformers},
  year      = {2021},
  editor    = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
  pages     = {18590--18602},
  publisher = {Curran Associates, Inc.},
  volume    = {34},
  file      = {:Jiang2021 - All Tokens Matter_ Token Labeling for Training Better Vision Transformers.pdf:PDF;:NeurIPS-2021-all-tokens-matter-token-labeling-for-training-better-vision-transformers-Supplemental.pdf:PDF},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2021/file/9a49a25d845a483fae4be7e341368e36-Paper.pdf},
}

@Article{Zhang2024,
  author   = {Zhang, Jingyi and Huang, Jiaxing and Jin, Sheng and Lu, Shijian},
  journal  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title    = {Vision-Language Models for Vision Tasks: A Survey},
  year     = {2024},
  pages    = {1-20},
  abstract = {Most visual recognition studies rely heavily on crowd-labelled data in deep neural networks (DNNs) training, and they usually train a DNN for each single visual recognition task, leading to a laborious and time-consuming visual recognition paradigm. To address the two challenges, Vision-Language Models (VLMs) have been intensively investigated recently, which learns rich vision-language correlation from web-scale image-text pairs that are almost infinitely available on the Internet and enables zero-shot predictions on various visual recognition tasks with a single VLM. This paper provides a systematic review of visual language models for various visual recognition tasks, including: (1) the background that introduces the development of visual recognition paradigms; (2) the foundations of VLM that summarize the widely-adopted network architectures, pre-training objectives, and downstream tasks; (3) the widely-adopted datasets in VLM pre-training and evaluations; (4) the review and categorization of existing VLM pre-training methods, VLM transfer learning methods, and VLM knowledge distillation methods; (5) the benchmarking, analysis and discussion of the reviewed methods; (6) several research challenges and potential research directions that could be pursued in the future VLM studies for visual recognition. A project associated with this survey has been created at https://github.com/jingyi0000/VLM_survey.},
  comment  = {This work details many few-shot learning methods applied to CLIP-like models such as CoOp and MaPLe. 

Consider avoiding the field of CLIP-like models and few-shot learning? Multimodal-Large-Language models and VQA, with an extension of some of the approaches in the paper to these seems better.},
  doi      = {10.1109/TPAMI.2024.3369699},
  file     = {:Zhang2024 - Vision Language Models for Vision Tasks_ a Survey.pdf:PDF},
  groups   = {Essays},
  keywords = {Task analysis;Visualization;Training;Deep learning;Surveys;Data models;Predictive models;Visual recognition;vision-language model;pre-training;transfer learning;knowledge distillation;image classification;object detection;semantic segmentation;deep neural network;deep learning;big model;big data},
  priority = {prio1},
}

@InProceedings{Ouyang2022,
  author    = {Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul F and Leike, Jan and Lowe, Ryan},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Training language models to follow instructions with human feedback},
  year      = {2022},
  editor    = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
  pages     = {27730--27744},
  publisher = {Curran Associates, Inc.},
  volume    = {35},
  file      = {:Ouyang2022 - Training Language Models to Follow Instructions with Human Feedback.pdf:PDF;:NeurIPS-2022-training-language-models-to-follow-instructions-with-human-feedback-Supplemental-Conference.pdf:PDF},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf},
}

@Article{Chen2017,
  author        = {Chen, Liang-Chieh and Papandreou, George and Schroff, Florian and Adam, Hartwig},
  title         = {Rethinking Atrous Convolution for Semantic Image Segmentation},
  year          = {2017},
  month         = jun,
  abstract      = {In this work, we revisit atrous convolution, a powerful tool to explicitly adjust filter's field-of-view as well as control the resolution of feature responses computed by Deep Convolutional Neural Networks, in the application of semantic image segmentation. To handle the problem of segmenting objects at multiple scales, we design modules which employ atrous convolution in cascade or in parallel to capture multi-scale context by adopting multiple atrous rates. Furthermore, we propose to augment our previously proposed Atrous Spatial Pyramid Pooling module, which probes convolutional features at multiple scales, with image-level features encoding global context and further boost performance. We also elaborate on implementation details and share our experience on training our system. The proposed `DeepLabv3' system significantly improves over our previous DeepLab versions without DenseCRF post-processing and attains comparable performance with other state-of-art models on the PASCAL VOC 2012 semantic image segmentation benchmark.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1706.05587},
  eprint        = {1706.05587},
  file          = {:Chen2017 - Rethinking Atrous Convolution for Semantic Image Segmentation.pdf:PDF:http\://arxiv.org/pdf/1706.05587v3},
  keywords      = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
  primaryclass  = {cs.CV},
  publisher     = {arXiv},
}

@InProceedings{Paredes2012,
  author    = {Paredes, Bernardino Romera and Argyriou, Andreas and Berthouze, Nadia and Pontil, Massimiliano},
  booktitle = {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},
  title     = {Exploiting Unrelated Tasks in Multi-Task Learning},
  year      = {2012},
  address   = {La Palma, Canary Islands},
  editor    = {Lawrence, Neil D. and Girolami, Mark},
  month     = {21--23 Apr},
  pages     = {951--959},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {22},
  abstract  = {We study the problem of learning a group of principal tasks using a group of auxiliary tasks, unrelated to the principal ones. In many applications, joint learning of unrelated tasks which use the same input data can be beneficial. The reason is that prior knowledge about which tasks are unrelated can lead to sparser and more informative representations for each task, essentially screening out idiosyncrasies of the data distribution. We propose a novel method which builds on a prior multitask methodology by favoring a shared low dimensional  representation within each group of tasks. In addition, we impose a penalty on tasks from different groups which encourages the two representations to be orthogonal. We further discuss a condition which ensures convexity of the optimization problem and argue that it can be solved by alternating minimization. We present experiments on synthetic and real data, which indicate that incorporating unrelated tasks can improve significantly over standard multi-task learning methods.},
  file      = {:Paredes2012 - Exploiting Unrelated Tasks in Multi Task Learning.pdf:PDF},
  pdf       = {http://proceedings.mlr.press/v22/romera12/romera12.pdf},
  url       = {https://proceedings.mlr.press/v22/romera12.html},
}

@Article{Ruder2017,
  author        = {Ruder, Sebastian},
  title         = {An Overview of Multi-Task Learning in Deep Neural Networks},
  year          = {2017},
  month         = jun,
  abstract      = {Multi-task learning (MTL) has led to successes in many applications of machine learning, from natural language processing and speech recognition to computer vision and drug discovery. This article aims to give a general overview of MTL, particularly in deep neural networks. It introduces the two most common methods for MTL in Deep Learning, gives an overview of the literature, and discusses recent advances. In particular, it seeks to help ML practitioners apply MTL by shedding light on how MTL works and providing guidelines for choosing appropriate auxiliary tasks.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1706.05098},
  eprint        = {1706.05098},
  file          = {:Ruder2017 - An Overview of Multi Task Learning in Deep Neural Networks.pdf:PDF:http\://arxiv.org/pdf/1706.05098v1},
  keywords      = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Machine Learning (stat.ML), FOS: Computer and information sciences},
  primaryclass  = {cs.LG},
  publisher     = {arXiv},
}

@Article{Everingham2015,
  author    = {Everingham, Mark and Eslami, SM Ali and Van Gool, Luc and Williams, Christopher KI and Winn, John and Zisserman, Andrew},
  journal   = {International journal of computer vision},
  title     = {The pascal visual object classes challenge: A retrospective},
  year      = {2015},
  pages     = {98--136},
  volume    = {111},
  file      = {:Everingham2015 - The Pascal Visual Object Classes Challenge_ a Retrospective.pdf:PDF},
  publisher = {Springer},
}

@Article{Fields2023,
  author        = {Fields, Clayton and Kennington, Casey},
  journal       = {arXiv preprint},
  title         = {Vision {L}anguage {T}ransformers: {A} {S}urvey},
  year          = {2023},
  month         = jul,
  abstract      = {Vision language tasks, such as answering questions about or generating captions that describe an image, are difficult tasks for computers to perform. A relatively recent body of research has adapted the pretrained transformer architecture introduced in \citet{vaswani2017attention} to vision language modeling. Transformer models have greatly improved performance and versatility over previous vision language models. They do so by pretraining models on a large generic datasets and transferring their learning to new tasks with minor changes in architecture and parameter values. This type of transfer learning has become the standard modeling practice in both natural language processing and computer vision. Vision language transformers offer the promise of producing similar advancements in tasks which require both vision and language. In this paper, we provide a broad synthesis of the currently available research on vision language transformer models and offer some analysis of their strengths, limitations and some open questions that remain.},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution 4.0 International},
  doi           = {10.48550/ARXIV.2307.03254},
  eprint        = {2307.03254},
  file          = {:Fields2023 - Vision Language Transformers_ a Survey.pdf:PDF:http\://arxiv.org/pdf/2307.03254v1},
  keywords      = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences},
  primaryclass  = {cs.CV},
  publisher     = {arXiv},
}

@Article{Bordes2024,
  author        = {Bordes, Florian and Pang, Richard Yuanzhe and Ajay, Anurag and Li, Alexander C. and Bardes, Adrien and Petryk, Suzanne and Mañas, Oscar and Lin, Zhiqiu and Mahmoud, Anas and Jayaraman, Bargav and Ibrahim, Mark and Hall, Melissa and Xiong, Yunyang and Lebensold, Jonathan and Ross, Candace and Jayakumar, Srihari and Guo, Chuan and Bouchacourt, Diane and Al-Tahan, Haider and Padthe, Karthik and Sharma, Vasu and Xu, Hu and Tan, Xiaoqing Ellen and Richards, Megan and Lavoie, Samuel and Astolfi, Pietro and Hemmat, Reyhane Askari and Chen, Jun and Tirumala, Kushal and Assouel, Rim and Moayeri, Mazda and Talattof, Arjang and Chaudhuri, Kamalika and Liu, Zechun and Chen, Xilun and Garrido, Quentin and Ullrich, Karen and Agrawal, Aishwarya and Saenko, Kate and Celikyilmaz, Asli and Chandra, Vikas},
  journal       = {arXiv preprint},
  title         = {An {I}ntroduction to {V}ision-{L}anguage {M}odeling},
  year          = {2024},
  month         = may,
  abstract      = {Following the recent popularity of Large Language Models (LLMs), several attempts have been made to extend them to the visual domain. From having a visual assistant that could guide us through unfamiliar environments to generative models that produce images using only a high-level text description, the vision-language model (VLM) applications will significantly impact our relationship with technology. However, there are many challenges that need to be addressed to improve the reliability of those models. While language is discrete, vision evolves in a much higher dimensional space in which concepts cannot always be easily discretized. To better understand the mechanics behind mapping vision to language, we present this introduction to VLMs which we hope will help anyone who would like to enter the field. First, we introduce what VLMs are, how they work, and how to train them. Then, we present and discuss approaches to evaluate VLMs. Although this work primarily focuses on mapping images to language, we also discuss extending VLMs to videos.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2405.17247},
  eprint        = {2405.17247},
  file          = {:Bordes2024 - An Introduction to Vision Language Modeling.pdf:PDF:http\://arxiv.org/pdf/2405.17247v1},
  groups        = {Essays},
  keywords      = {Machine Learning (cs.LG), FOS: Computer and information sciences},
  primaryclass  = {cs.LG},
  priority      = {prio1},
  publisher     = {arXiv},
}

@Article{Zeng2023,
  author        = {Zeng, Yuchen and Lee, Kangwook},
  title         = {The Expressive Power of Low-Rank Adaptation},
  year          = {2023},
  month         = oct,
  abstract      = {Low-Rank Adaptation (LoRA), a parameter-efficient fine-tuning method that leverages low-rank adaptation of weight matrices, has emerged as a prevalent technique for fine-tuning pre-trained models such as large language models and diffusion models. Despite its huge success in practice, the theoretical underpinnings of LoRA have largely remained unexplored. This paper takes the first step to bridge this gap by theoretically analyzing the expressive power of LoRA. We prove that, for fully connected neural networks, LoRA can adapt any model $f$ to accurately represent any smaller target model $\overline{f}$ if LoRA-rank $\geq(\text{width of }f) \times \frac{\text{depth of }\overline{f}}{\text{depth of }f}$. We also quantify the approximation error when LoRA-rank is lower than the threshold. For Transformer networks, we show any model can be adapted to a target model of the same size with rank-$(\frac{\text{embedding size}}{2})$ LoRA adapters.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2310.17513},
  eprint        = {2310.17513},
  file          = {:Zeng2023 - The Expressive Power of Low Rank Adaptation.pdf:PDF:http\://arxiv.org/pdf/2310.17513v3},
  keywords      = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computation and Language (cs.CL), Machine Learning (stat.ML), FOS: Computer and information sciences},
  primaryclass  = {cs.LG},
  publisher     = {arXiv},
}

@Article{Hu2021,
  author        = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  title         = {LoRA: Low-Rank Adaptation of Large Language Models},
  year          = {2021},
  month         = jun,
  abstract      = {An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2106.09685},
  eprint        = {2106.09685},
  file          = {:Hu2021 - LoRA_ Low Rank Adaptation of Large Language Models.pdf:PDF:http\://arxiv.org/pdf/2106.09685v2},
  keywords      = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  priority      = {prio2},
  publisher     = {arXiv},
}

@Article{Bergmann2021,
  author    = {Bergmann, Paul and Batzner, Kilian and Fauser, Michael and Sattlegger, David and Steger, Carsten},
  journal   = {International Journal of Computer Vision},
  title     = {The {MVT}ec {A}nomaly {D}etection {D}ataset: {A} {C}omprehensive {R}eal-{W}orld {D}ataset for {U}nsupervised {A}nomaly {D}etection},
  year      = {2021},
  number    = {4},
  pages     = {1038--1059},
  volume    = {129},
  abstract  = {The detection of anomalous structures in natural image data is of utmost importance for numerous tasks in the field of computer vision. The development of methods for unsupervised anomaly detection requires data on which to train and evaluate new approaches and ideas. We introduce the MVTec anomaly detection dataset containing 5354 high-resolution color images of different object and texture categories. It contains normal, i.e., defect-free images intended for training and images with anomalies intended for testing. The anomalies manifest themselves in the form of over 70 different types of defects such as scratches, dents, contaminations, and various structural changes. In addition, we provide pixel-precise ground truth annotations for all anomalies. We conduct a thorough evaluation of current state-of-the-art unsupervised anomaly detection methods based on deep architectures such as convolutional autoencoders, generative adversarial networks, and feature descriptors using pretrained convolutional neural networks, as well as classical computer vision methods. We highlight the advantages and disadvantages of multiple performance metrics as well as threshold estimation techniques. This benchmark indicates that methods that leverage descriptors of pretrained networks outperform all other approaches and deep-learning-based generative models show considerable room for improvement.},
  doi       = {10.1007/s11263-020-01400-4},
  file      = {:Bergmann2021 - The MVTec Anomaly Detection Dataset_ a Comprehensive Real World Dataset for Unsupervised Anomaly Detection.pdf:PDF},
  groups    = {Datasets, Reading Reports},
  publisher = {Springer},
}

@Article{Lecun1998,
  author   = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  journal  = {Proceedings of the IEEE},
  title    = {Gradient-based learning applied to document recognition},
  year     = {1998},
  number   = {11},
  pages    = {2278-2324},
  volume   = {86},
  doi      = {10.1109/5.726791},
  file     = {:Lecun1998 - Gradient Based Learning Applied to Document Recognition.pdf:PDF},
  groups   = {Datasets},
  keywords = {Neural networks;Pattern recognition;Machine learning;Optical character recognition software;Character recognition;Feature extraction;Multi-layer neural network;Optical computing;Hidden Markov models;Principal component analysis},
}

@MastersThesis{Krizhevsky2009,
  author    = {Krizhevsky, Alex and Hinton, Geoffrey and others},
  school    = {Department of Computer Science, University of Toronto},
  title     = {Learning multiple layers of features from tiny images},
  year      = {2009},
  file      = {:Krizhevsky2009 - Learning Multiple Layers of Features from Tiny Images.pdf:PDF},
  groups    = {Datasets},
  publisher = {Toronto, ON, Canada},
}

@Misc{Dao2022,
  author        = {Tri Dao and Daniel Y. Fu and Stefano Ermon and Atri Rudra and Christopher Ré},
  title         = {FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness},
  year          = {2022},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2205.14135},
  eprint        = {2205.14135},
  file          = {:Dao2022 - FlashAttention_ Fast and Memory Efficient Exact Attention with IO Awareness.pdf:PDF},
  primaryclass  = {cs.LG},
}

@Article{Zhang2023a,
  author        = {Zhang, Renrui and Jiang, Zhengkai and Guo, Ziyu and Yan, Shilin and Pan, Junting and Ma, Xianzheng and Dong, Hao and Gao, Peng and Li, Hongsheng},
  title         = {{Personalize Segment Anything Model with One Shot}},
  year          = {2023},
  month         = may,
  abstract      = {Driven by large-data pre-training, Segment Anything Model (SAM) has been demonstrated as a powerful and promptable framework, revolutionizing the segmentation models. Despite the generality, customizing SAM for specific visual concepts without man-powered prompting is under explored, e.g., automatically segmenting your pet dog in different images. In this paper, we propose a training-free Personalization approach for SAM, termed as PerSAM. Given only a single image with a reference mask, PerSAM first localizes the target concept by a location prior, and segments it within other images or videos via three techniques: target-guided attention, target-semantic prompting, and cascaded post-refinement. In this way, we effectively adapt SAM for private use without any training. To further alleviate the mask ambiguity, we present an efficient one-shot fine-tuning variant, PerSAM-F. Freezing the entire SAM, we introduce two learnable weights for multi-scale masks, only training 2 parameters within 10 seconds for improved performance. To demonstrate our efficacy, we construct a new segmentation dataset, PerSeg, for personalized evaluation, and test our methods on video object segmentation with competitive performance. Besides, our approach can also enhance DreamBooth to personalize Stable Diffusion for text-to-image generation, which discards the background disturbance for better target appearance learning. Code is released at https://github.com/ZrrSkywalker/Personalize-SAM},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2305.03048},
  eprint        = {2305.03048},
  file          = {:Zhang2023a - Personalize Segment Anything Model with One Shot.pdf:PDF:http\://arxiv.org/pdf/2305.03048v2},
  groups        = {Models},
  keywords      = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Computation and Language (cs.CL), Machine Learning (cs.LG), Multimedia (cs.MM), FOS: Computer and information sciences},
  primaryclass  = {cs.CV},
  priority      = {prio2},
  publisher     = {arXiv},
}

@InProceedings{Chen2023,
  author        = {Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Zhong, Muyan and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and Li, Bin and Luo, Ping and Lu, Tong and Qiao, Yu and Dai, Jifeng},
  booktitle     = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title         = {{InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks}},
  year          = {2023},
  month         = dec,
  pages         = {24185-24198},
  publisher     = {arXiv},
  abstract      = {The exponential growth of large language models (LLMs) has opened up numerous possibilities for multimodal AGI systems. However, the progress in vision and vision-language foundation models, which are also critical elements of multi-modal AGI, has not kept pace with LLMs. In this work, we design a large-scale vision-language foundation model (InternVL), which scales up the vision foundation model to 6 billion parameters and progressively aligns it with the LLM, using web-scale image-text data from various sources. This model can be broadly applied to and achieve state-of-the-art performance on 32 generic visual-linguistic benchmarks including visual perception tasks such as image-level or pixel-level recognition, vision-language tasks such as zero-shot image/video classification, zero-shot image/video-text retrieval, and link with LLMs to create multi-modal dialogue systems. It has powerful visual capabilities and can be a good alternative to the ViT-22B. We hope that our research could contribute to the development of multi-modal large models. Code and models are available at https://github.com/OpenGVLab/InternVL.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2312.14238},
  eprint        = {2312.14238},
  file          = {:Chen2023 - InternVL_ Scaling up Vision Foundation Models and Aligning for Generic Visual Linguistic Tasks.pdf:PDF:http\://arxiv.org/pdf/2312.14238v3},
  groups        = {Models},
  keywords      = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
  primaryclass  = {cs.CV},
  priority      = {prio1},
}

@Article{Gu2021,
  author        = {Gu, Yuxian and Han, Xu and Liu, Zhiyuan and Huang, Minlie},
  title         = {PPT: Pre-trained Prompt Tuning for Few-shot Learning},
  year          = {2021},
  month         = sep,
  abstract      = {Prompts for pre-trained language models (PLMs) have shown remarkable performance by bridging the gap between pre-training tasks and various downstream tasks. Among these methods, prompt tuning, which freezes PLMs and only tunes soft prompts, provides an efficient and effective solution for adapting large-scale PLMs to downstream tasks. However, prompt tuning is yet to be fully explored. In our pilot experiments, we find that prompt tuning performs comparably with conventional full-model fine-tuning when downstream data are sufficient, whereas it performs much worse under few-shot learning settings, which may hinder the application of prompt tuning in practice. We attribute this low performance to the manner of initializing soft prompts. Therefore, in this work, we propose to pre-train prompts by adding soft prompts into the pre-training stage to obtain a better initialization. We name this Pre-trained Prompt Tuning framework "PPT". To ensure the generalization of PPT, we formulate similar classification tasks into a unified task form and pre-train soft prompts for this unified task. Extensive experiments show that tuning pre-trained prompts for downstream tasks can reach or even outperform full-model fine-tuning under both full-data and few-shot settings. Our approach is effective and efficient for using large-scale PLMs in practice.},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution 4.0 International},
  doi           = {10.48550/ARXIV.2109.04332},
  eprint        = {2109.04332},
  file          = {:Gu2021 - PPT_ Pre Trained Prompt Tuning for Few Shot Learning.pdf:PDF:http\://arxiv.org/pdf/2109.04332v3},
  groups        = {Few-Shot Transfer},
  keywords      = {Computation and Language (cs.CL), FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  publisher     = {arXiv},
}

@Article{Song2023,
  author     = {Song, Yisheng and Wang, Ting and Cai, Puyu and Mondal, Subrota K. and Sahoo, Jyoti Prakash},
  journal    = {ACM Comput. Surv.},
  title      = {A Comprehensive Survey of Few-shot Learning: Evolution, Applications, Challenges, and Opportunities},
  year       = {2023},
  issn       = {0360-0300},
  month      = {jul},
  number     = {13s},
  volume     = {55},
  abstract   = {Few-shot learning (FSL) has emerged as an effective learning method and shows great potential. Despite the recent creative works in tackling FSL tasks, learning valid information rapidly from just a few or even zero samples remains a serious challenge. In this context, we extensively investigated 200+ FSL papers published in top journals and conferences in the past three years, aiming to present a timely and comprehensive overview of the most recent advances in FSL with a fresh perspective and to provide an impartial comparison of the strengths and weaknesses of existing work. To avoid conceptual confusion, we first elaborate and contrast a set of relevant concepts including few-shot learning, transfer learning, and meta-learning. Then, we inventively extract prior knowledge related to few-shot learning in the form of a pyramid, which summarizes and classifies previous work in detail from the perspective of challenges. Furthermore, to enrich this survey, we present in-depth analysis and insightful discussions of recent advances in each subsection. What is more, taking computer vision as an example, we highlight the important application of FSL, covering various research hotspots. Finally, we conclude the survey with unique insights into technology trends and potential future research opportunities to guide FSL follow-up research.},
  address    = {New York, NY, USA},
  articleno  = {271},
  doi        = {10.1145/3582688},
  file       = {:Song2023 - A Comprehensive Survey of Few Shot Learning_ Evolution, Applications, Challenges, and Opportunities.pdf:PDF},
  issue_date = {December 2023},
  keywords   = {Few-shot learning, one-shot learning, zero-shot learning, low-shot learning, meta-learning, prior knowledge},
  numpages   = {40},
  priority   = {prio2},
  publisher  = {Association for Computing Machinery},
}

@InProceedings{Zhou2022a,
  author    = {Zhou, Kaiyang and Yang, Jingkang and Loy, {Chen Change} and Liu, Ziwei},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {{Conditional Prompt Learning for Vision-Language Models}},
  year      = {2022},
  month     = {June},
  pages     = {16816-16825},
  file      = {:Zhou2022a - Conditional Prompt Learning for Vision Language Models.pdf:PDF},
  groups    = {Few-Shot Transfer},
}

@InProceedings{Khattak2023,
  author    = {Khattak, Muhammad Uzair and Rasheed, Hanoona and Maaz, Muhammad and Khan, Salman and Khan, Fahad Shahbaz},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {MaPLe: Multi-Modal Prompt Learning},
  year      = {2023},
  month     = {June},
  pages     = {19113-19122},
  abstract  = {Pre-trained vision-language (V-L) models such as CLIP have shown excellent generalization ability to downstream tasks. However, they are sensitive to the choice of input text prompts and require careful selection of prompt templates to perform well. Inspired by the Natural Language Processing (NLP) literature, recent CLIP adaptation approaches learn prompts as the textual inputs to fine-tune CLIP for down-stream tasks. We note that using prompting to adapt representations in a single branch of CLIP (language or vision) is sub-optimal since it does not allow the flexibility to dynamically adjust both representation spaces on a downstream task. In this work, we propose Multi-modal Prompt Learning (MaPLe) for both vision and language branches to improve alignment between the vision and language representations. Our design promotes strong coupling between the vision-language prompts to ensure mutual synergy and discourages learning independent uni-modal solutions. Further, we learn separate prompts across different early stages to progressively model the stage-wise feature relationships to allow rich context learning. We evaluate the effectiveness of our approach on three representative tasks of generalization to novel classes, new target datasets and unseen domain shifts. Compared with the state-of-the-art method Co-CoOp, MaPLe exhibits favorable performance and achieves an absolute gain of 3.45\% on novel classes and 2.72\% on overall harmonic-mean, averaged over 11 diverse image recognition datasets. Our code and pre-trained models are available at \href{https://github.com/muzairkhattak/multimodal-prompt-learning}{https://github.com/muzairkhattak/multimodal-prompt-learning}.},
  file      = {:Khattak2023 - MaPLe_ Multi Modal Prompt Learning.pdf:PDF;:Khattak_MaPLe_Multi-Modal_Prompt_CVPR_2023_supplemental.pdf:PDF},
  groups    = {Reading Reports, Few-Shot Transfer},
  priority  = {prio1},
}

@Article{Zheng2024,
  author        = {Kaizhi Zheng and Xuehai He and Xin Eric Wang},
  journal       = {arXiv preprint},
  title         = {{MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens}},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2310.02239},
  eprint        = {2310.02239},
  file          = {:Zheng2024 - MiniGPT 5_ Interleaved Vision and Language Generation Via Generative Vokens.pdf:PDF},
  groups        = {Models},
  primaryclass  = {cs.CV},
}

@Article{Dao2023,
  author   = {Dao, Son D. and Shi, Hengcan and Phung, Dinh and Cai, Jianfei},
  journal  = {IEEE Transactions on Multimedia},
  title    = {Class Enhancement Losses with Pseudo Labels for Open-Vocabulary Semantic Segmentation},
  year     = {2023},
  pages    = {1-12},
  doi      = {10.1109/TMM.2023.3330102},
  file     = {:Dao2023 - Class Enhancement Losses with Pseudo Labels for Open Vocabulary Semantic Segmentation.pdf:PDF},
  keywords = {Proposals;Training;Semantic segmentation;Annotations;Semantics;Predictive models;Visualization;Open-vocabulary semantic segmentation;Zero-shot semantic segmentation},
}

@Misc{Kim2024,
  author        = {Junsu Kim and Yunhoe Ku and Jihyeon Kim and Junuk Cha and Seungryul Baek},
  title         = {VLM-PL: Advanced Pseudo Labeling Approach for Class Incremental Object Detection via Vision-Language Model},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2403.05346},
  eprint        = {2403.05346},
  file          = {:Kim2024 - VLM PL_ Advanced Pseudo Labeling Approach for Class Incremental Object Detection Via Vision Language Model.pdf:PDF},
  primaryclass  = {cs.CV},
}

@Article{Mikolov2013,
  author        = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  title         = {Efficient Estimation of Word Representations in Vector Space},
  year          = {2013},
  month         = jan,
  abstract      = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1301.3781},
  eprint        = {1301.3781},
  file          = {:Mikolov2013 - Efficient Estimation of Word Representations in Vector Space.pdf:PDF:http\://arxiv.org/pdf/1301.3781v3},
  keywords      = {Computation and Language (cs.CL), FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  publisher     = {arXiv},
}

@Article{Halloran2024,
  author        = {Halloran, John T. and Gulati, Manbir and Roysdon, Paul F.},
  title         = {Mamba State-Space Models Can Be Strong Downstream Learners},
  year          = {2024},
  month         = may,
  abstract      = {Mamba state-space models (SSMs) have recently outperformed state-of-the-art (SOTA) Transformer large language models (LLMs) in various tasks and been widely adapted. However, Mamba's downstream learning capabilities remain either unexplored$\unicode{x2013}$e.g., mixed-precision (MPFT) and parameter-efficient fine-tuning (PEFT)--or under-evaluated$\unicode{x2013}$e.g., in-context learning (ICL). For the latter, recent works reported Mamba's ICL rivals SOTA Transformer LLMs using non-standard benchmarks. In contrast, we show that on standard benchmarks, pretrained Mamba models achieve only 38% of the ICL performance improvements (over zero-shot) of comparable Transformers. Enabling MPFT and PEFT in Mamba architectures is challenging due to recurrent dynamics and highly customized CUDA kernels, respectively. However, we prove that Mamba's recurrent dynamics are robust to small input changes using dynamical systems theory. Empirically, we show that performance changes in Mamba's inference and fine-tuning due to mixed-precision align with Transformer LLMs. Furthermore, we show that targeting key memory buffers in Mamba's customized CUDA kernels for low-rank adaptation regularizes SSM parameters, thus achieving parameter efficiency while retaining speedups. We show that combining MPFT and PEFT enables up to 2.15 times more tokens-per-second and 65.5% reduced per-token-memory compared to full Mamba fine-tuning, while achieving up to 81.5% of the ICL performance improvements (over zero-shot) of comparably fine-tuned Transformers.},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution 4.0 International},
  doi           = {10.48550/ARXIV.2406.00209},
  eprint        = {2406.00209},
  file          = {:Halloran2024 - Mamba State Space Models Can Be Strong Downstream Learners.pdf:PDF:http\://arxiv.org/pdf/2406.00209v1},
  keywords      = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), FOS: Computer and information sciences},
  primaryclass  = {cs.LG},
  publisher     = {arXiv},
}

@Article{Gu2023,
  author        = {Gu, Albert and Dao, Tri},
  title         = {Mamba: Linear-Time Sequence Modeling with Selective State Spaces},
  year          = {2023},
  month         = dec,
  abstract      = {Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution 4.0 International},
  doi           = {10.48550/ARXIV.2312.00752},
  eprint        = {2312.00752},
  file          = {:Gu2023 - Mamba_ Linear Time Sequence Modeling with Selective State Spaces.pdf:PDF:http\://arxiv.org/pdf/2312.00752v2},
  keywords      = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), FOS: Computer and information sciences},
  primaryclass  = {cs.LG},
  publisher     = {arXiv},
}

@Article{Child2019,
  author        = {Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
  title         = {Generating Long Sequences with Sparse Transformers},
  year          = {2019},
  month         = apr,
  abstract      = {Transformers are powerful sequence models, but require time and memory that grows quadratically with the sequence length. In this paper we introduce sparse factorizations of the attention matrix which reduce this to $O(n \sqrt{n})$. We also introduce a) a variation on architecture and initialization to train deeper networks, b) the recomputation of attention matrices to save memory, and c) fast attention kernels for training. We call networks with these changes Sparse Transformers, and show they can model sequences tens of thousands of timesteps long using hundreds of layers. We use the same architecture to model images, audio, and text from raw bytes, setting a new state of the art for density modeling of Enwik8, CIFAR-10, and ImageNet-64. We generate unconditional samples that demonstrate global coherence and great diversity, and show it is possible in principle to use self-attention to model sequences of length one million or more.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1904.10509},
  eprint        = {1904.10509},
  file          = {:Child2019 - Generating Long Sequences with Sparse Transformers.pdf:PDF:http\://arxiv.org/pdf/1904.10509v1},
  keywords      = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences},
  primaryclass  = {cs.LG},
  publisher     = {arXiv},
}

@Article{Hosseini2024,
  author        = {Hosseini, Mehran and Hosseini, Peyman},
  title         = {You Need to Pay Better Attention: Rethinking the Mathematics of Attention Mechanism},
  year          = {2024},
  month         = mar,
  abstract      = {Scaled Dot Product Attention (SDPA) is the backbone of many modern deep-learning models. It is so versatile that it has been used in natural language, vision, and multi-modal domains with very little change compared to its original formulation. This paper discusses why the current formulation is inefficient by delving into the mathematical details of the attention mechanism. We propose three improvements to mitigate these inefficiencies, thereby, introducing three enhanced attention mechanisms: Optimised, Efficient, and Super Attention. Optimised and Efficient Attention have one and two matrix multiplications fewer per head, respectively, and 25% and 50% fewer parameters, respectively, than standard SDPA, but perform similarly to standard SDPA in both vision and natural language tasks. They can be used in all applications where SDPA is used while offering smaller model sizes and faster training and inference without noticeable loss in performance. Super Attention introduces a new linear transformation on the values, transforming them from the left. It outperforms standard SPDA on vision and natural language tasks by up to 17% while having one fewer matrix multiplication per head and 25% fewer parameters than standard SDPA. Consequently, it is also faster than standard SDPA. Super Attention is ideal in applications where the attention layer's context length is fixed, such as Vision Transformers. In addition to providing mathematical reasoning, we evaluate the presented attention mechanisms on several datasets including MNIST, CIFAR100, ImageNet, IMDB Movie Reviews, and Amazon Reviews datasets, as well as combined Europarl and Anki English-Spanish datasets for neural machine translation.},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution 4.0 International},
  doi           = {10.48550/ARXIV.2403.01643},
  eprint        = {2403.01643},
  file          = {:Hosseini2024 - You Need to Pay Better Attention_ Rethinking the Mathematics of Attention Mechanism.pdf:PDF:http\://arxiv.org/pdf/2403.01643v2},
  keywords      = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computation and Language (cs.CL), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, I.2.6; I.2.7; I.2.10; I.4.0; I.5.0; I.7.0, 68T07 (Primary) 68T45, 68T50, 68T10, 15A03, 15A04 (Secondary)},
  primaryclass  = {cs.LG},
  publisher     = {arXiv},
}

@Article{Wu2023,
  author        = {Wu, Letian and Zhang, Wenyao and Jiang, Tengping and Yang, Wankou and Jin, Xin and Zeng, Wenjun},
  title         = {[CLS] Token is All You Need for Zero-Shot Semantic Segmentation},
  year          = {2023},
  month         = apr,
  abstract      = {In this paper, we propose an embarrassingly simple yet highly effective zero-shot semantic segmentation (ZS3) method, based on the pre-trained vision-language model CLIP. First, our study provides a couple of key discoveries: (i) the global tokens (a.k.a [CLS] tokens in Transformer) of the text branch in CLIP provide a powerful representation of semantic information and (ii) these text-side [CLS] tokens can be regarded as category priors to guide CLIP visual encoder pay more attention on the corresponding region of interest. Based on that, we build upon the CLIP model as a backbone which we extend with a One-Way [CLS] token navigation from text to the visual branch that enables zero-shot dense prediction, dubbed \textbf{ClsCLIP}. Specifically, we use the [CLS] token output from the text branch, as an auxiliary semantic prompt, to replace the [CLS] token in shallow layers of the ViT-based visual encoder. This one-way navigation embeds such global category prior earlier and thus promotes semantic segmentation. Furthermore, to better segment tiny objects in ZS3, we further enhance ClsCLIP with a local zoom-in strategy, which employs a region proposal pre-processing and we get ClsCLIP+. Extensive experiments demonstrate that our proposed ZS3 method achieves a SOTA performance, and it is even comparable with those few-shot semantic segmentation methods.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2304.06212},
  eprint        = {2304.06212},
  file          = {:Wu2023 - [CLS] Token Is All You Need for Zero Shot Semantic Segmentation.pdf:PDF:http\://arxiv.org/pdf/2304.06212v1},
  keywords      = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
  primaryclass  = {cs.CV},
  publisher     = {arXiv},
}

@Article{Munkhdalai2024,
  author        = {Munkhdalai, Tsendsuren and Faruqui, Manaal and Gopal, Siddharth},
  title         = {Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention},
  year          = {2024},
  month         = apr,
  abstract      = {This work introduces an efficient method to scale Transformer-based Large Language Models (LLMs) to infinitely long inputs with bounded memory and computation. A key component in our proposed approach is a new attention technique dubbed Infini-attention. The Infini-attention incorporates a compressive memory into the vanilla attention mechanism and builds in both masked local attention and long-term linear attention mechanisms in a single Transformer block. We demonstrate the effectiveness of our approach on long-context language modeling benchmarks, 1M sequence length passkey context block retrieval and 500K length book summarization tasks with 1B and 8B LLMs. Our approach introduces minimal bounded memory parameters and enables fast streaming inference for LLMs.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2404.07143},
  eprint        = {2404.07143},
  file          = {:Munkhdalai2024 - Leave No Context Behind_ Efficient Infinite Context Transformers with Infini Attention.pdf:PDF:http\://arxiv.org/pdf/2404.07143v1},
  keywords      = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  publisher     = {arXiv},
}

@Article{Li2023a,
  author   = {Li, Wenbin and Wang, Ziyi and Yang, Xuesong and Dong, Chuanqi and Tian, Pinzhuo and Qin, Tiexin and Huo, Jing and Shi, Yinghuan and Wang, Lei and Gao, Yang and Luo, Jiebo},
  journal  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title    = {LibFewShot: A Comprehensive Library for Few-Shot Learning},
  year     = {2023},
  number   = {12},
  pages    = {14938-14955},
  volume   = {45},
  doi      = {10.1109/TPAMI.2023.3312125},
  file     = {:Li2023a - LibFewShot_ a Comprehensive Library for Few Shot Learning.pdf:PDF},
  keywords = {Task analysis;Training;Metalearning;Libraries;Software;Deep learning;Benchmark testing;Fair comparison;few-shot learning;image classification;unified framework},
}

@InProceedings{Lin2023,
  author    = {Lin, Zhiqiu and Yu, Samuel and Kuang, Zhiyi and Pathak, Deepak and Ramanan, Deva},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {{Multimodality Helps Unimodality: Cross-Modal Few-Shot Learning With Multimodal Models}},
  year      = {2023},
  month     = {June},
  pages     = {19325-19337},
  abstract  = {The ability to quickly learn a new task with minimal instruction – known as few-shot learning – is a central aspect of intelligent agents. Classical few-shot benchmarks make use of few-shot samples from a single modality, but such samples may not be sufficient to characterize an entire concept class. In contrast, humans use cross-modal information to learn new concepts efficiently. In this work, we demonstrate that one can indeed build a better visual dog classifier by reading about dogs and listening to them bark. To do so, we exploit the fact that recent multimodal foundation models such as CLIP are inherently cross-modal, mapping different modalities to the same representation space. Specifically, we propose a simple cross-modal adaptation approach that learns from few-shot examples spanning different modalities. By repurposing class names as additional one-shot training samples, we achieve SOTA results with an embarrassingly simple linear classifier for vision-language adaptation. Furthermore, we show that our approach can benefit existing methods such as prefix tuning, adapters, and classifier ensembling. Finally, to explore other modalities beyond vision and language, we construct the first (to our knowledge) audiovisual few-shot benchmark and use cross-modal training to improve the performance of both image and audio classification. Project site at link.},
  doi       = {10.1109/cvpr52729.2023.01852},
  file      = {:Lin2023 - Multimodality Helps Unimodality_ Cross Modal Few Shot Learning with Multimodal Models.pdf:PDF},
  groups    = {Reading Reports},
  priority  = {prio2},
}

@InProceedings{Dai2023,
  author    = {Dai, Wenliang and Li, Junnan and LI, DONGXU and Tiong, Anthony and Zhao, Junqi and Wang, Weisheng and Li, Boyang and Fung, Pascale N and Hoi, Steven},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {{InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning}},
  year      = {2023},
  editor    = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
  pages     = {49250--49267},
  publisher = {Curran Associates, Inc.},
  volume    = {36},
  file      = {:Dai2023 - InstructBLIP_ Towards General Purpose Vision Language Models with Instruction Tuning.pdf:PDF;:NeurIPS-2023-instructblip-towards-general-purpose-vision-language-models-with-instruction-tuning-Supplemental-Conference.pdf:PDF},
  groups    = {Models},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2023/file/9a6a435e75419a836fe47ab6793623e6-Paper-Conference.pdf},
}

@Misc{Chen2024,
  author        = {Yangyi Chen and Karan Sikka and Michael Cogswell and Heng Ji and Ajay Divakaran},
  title         = {Measuring and Improving Chain-of-Thought Reasoning in Vision-Language Models},
  year          = {2024},
  archiveprefix = {arXiv},
  comment       = {Shows the generation of a dataset of Chain-of-Thought Reasoning using VLMs and LLMs. Specifically intended for reasoning and training VLMs to reason. But is there too much of an overlap here?},
  doi           = {10.48550/arxiv.2309.04461},
  eprint        = {2309.04461},
  file          = {:Chen2024 - Measuring and Improving Chain of Thought Reasoning in Vision Language Models.pdf:PDF},
  primaryclass  = {id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'},
}

@InProceedings{Ma2023,
  author    = {Ma, Zixian and Hong, Jerry and Gul, Mustafa Omer and Gandhi, Mona and Gao, Irena and Krishna, Ranjay},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {CREPE: Can Vision-Language Foundation Models Reason Compositionally?},
  year      = {2023},
  month     = {June},
  pages     = {10910-10921},
  file      = {:Ma2023 - CREPE_ Can Vision Language Foundation Models Reason Compositionally_.pdf:PDF;:Ma_CREPE_Can_Vision-Language_CVPR_2023_supplemental.pdf:PDF},
}

@Misc{Jin2023,
  author        = {Chuhao Jin and Wenhui Tan and Jiange Yang and Bei Liu and Ruihua Song and Limin Wang and Jianlong Fu},
  title         = {AlphaBlock: Embodied Finetuning for Vision-Language Reasoning in Robot Manipulation},
  year          = {2023},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2305.18898},
  eprint        = {2305.18898},
  file          = {:Jin2023 - AlphaBlock_ Embodied Finetuning for Vision Language Reasoning in Robot Manipulation.pdf:PDF},
  primaryclass  = {id='cs.RO' full_name='Robotics' is_active=True alt_name=None in_archive='cs' is_general=False description='Roughly includes material in ACM Subject Class I.2.9.'},
}

@InProceedings{Gupta2023,
  author    = {Gupta, Tanmay and Kembhavi, Aniruddha},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Visual Programming: Compositional Visual Reasoning Without Training},
  year      = {2023},
  month     = {June},
  pages     = {14953-14962},
  file      = {:Gupta2023 - Visual Programming_ Compositional Visual Reasoning without Training.pdf:PDF;:appendix.pdf:PDF;:qualitative_results.pdf:PDF},
}

@Misc{Ge2023,
  author        = {Jiaxin Ge and Hongyin Luo and Siyuan Qian and Yulu Gan and Jie Fu and Shanghang Zhang},
  title         = {Chain of Thought Prompt Tuning in Vision Language Models},
  year          = {2023},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2304.07919},
  eprint        = {2304.07919},
  file          = {:Ge2023 - Chain of Thought Prompt Tuning in Vision Language Models.pdf:PDF},
  primaryclass  = {id='cs.CV' full_name='Computer Vision and Pattern Recognition' is_active=True alt_name=None in_archive='cs' is_general=False description='Covers image processing, computer vision, pattern recognition, and scene understanding. Roughly includes material in ACM Subject Classes I.2.10, I.4, and I.5.'},
  priority      = {prio3},
}

@Misc{Aryabumi2024,
  author        = {Viraat Aryabumi and John Dang and Dwarak Talupuru and Saurabh Dash and David Cairuz and Hangyu Lin and Bharat Venkitesh and Madeline Smith and Kelly Marchisio and Sebastian Ruder and Acyr Locatelli and Julia Kreutzer and Nick Frosst and Phil Blunsom and Marzieh Fadaee and Ahmet Üstün and Sara Hooker},
  title         = {Aya 23: Open Weight Releases to Further Multilingual Progress},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2405.15032},
  eprint        = {2405.15032},
  file          = {:Aryabumi2024 - Aya 23_ Open Weight Releases to Further Multilingual Progress.pdf:PDF:http\://arxiv.org/pdf/2405.15032v2},
  groups        = {Models},
  primaryclass  = {cs.CL},
}

@Misc{Xiao2024,
  author        = {Guangxuan Xiao and Yuandong Tian and Beidi Chen and Song Han and Mike Lewis},
  title         = {Efficient Streaming Language Models with Attention Sinks},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2309.17453},
  eprint        = {2309.17453},
  file          = {:Xiao2024 - Efficient Streaming Language Models with Attention Sinks.pdf:PDF:https\://arxiv.org/pdf/2309.17453.pdf},
  primaryclass  = {id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'},
}

@InProceedings{Guzhov2022,
  author    = {Guzhov, Andrey and Raue, Federico and Hees, Jörn and Dengel, Andreas},
  booktitle = {ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  title     = {Audioclip: Extending Clip to Image, Text and Audio},
  year      = {2022},
  pages     = {976-980},
  doi       = {10.1109/ICASSP43922.2022.9747631},
  file      = {:Guzhov2022 - Audioclip_ Extending Clip to Image, Text and Audio.pdf:PDF},
  groups    = {Models},
  keywords  = {Training;Visualization;Codes;Fuses;Conferences;Signal processing;Market research;Audio;multimodal;zero-shot;classification},
}

@Article{Beltagy2020,
  author        = {Beltagy, Iz and Peters, Matthew E. and Cohan, Arman},
  title         = {Longformer: The Long-Document Transformer},
  year          = {2020},
  month         = apr,
  abstract      = {Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a Longformer variant for supporting long document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv summarization dataset.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2004.05150},
  eprint        = {2004.05150},
  file          = {:Beltagy2020 - Longformer_ the Long Document Transformer.pdf:PDF:http\://arxiv.org/pdf/2004.05150v2},
  keywords      = {Computation and Language (cs.CL), FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  publisher     = {arXiv},
}

@Article{Jiang2024,
  author        = {Jiang, Albert Q. and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Hanna, Emma Bou and Bressand, Florian and Lengyel, Gianna and Bour, Guillaume and Lample, Guillaume and Lavaud, Lélio Renard and Saulnier, Lucile and Lachaux, Marie-Anne and Stock, Pierre and Subramanian, Sandeep and Yang, Sophia and Antoniak, Szymon and Scao, Teven Le and Gervet, Théophile and Lavril, Thibaut and Wang, Thomas and Lacroix, Timothée and Sayed, William El},
  title         = {Mixtral of Experts},
  year          = {2024},
  month         = jan,
  abstract      = {We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine-tuned to follow instructions, Mixtral 8x7B - Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B - chat model on human benchmarks. Both the base and instruct models are released under the Apache 2.0 license.},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution 4.0 International},
  doi           = {10.48550/ARXIV.2401.04088},
  eprint        = {2401.04088},
  file          = {:Jiang2024 - Mixtral of Experts.pdf:PDF:http\://arxiv.org/pdf/2401.04088v1},
  groups        = {Models},
  keywords      = {Machine Learning (cs.LG), Computation and Language (cs.CL), FOS: Computer and information sciences},
  primaryclass  = {cs.LG},
  publisher     = {arXiv},
}

@InProceedings{SilvaRodriguez2024,
  author    = {Silva-Rodr{\'\i}guez, Julio and Hajimiri, Sina and Ben Ayed, Ismail and Dolz, Jose},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {A Closer Look at the Few-Shot Adaptation of Large Vision-Language Models},
  year      = {2024},
  month     = {June},
  pages     = {23681-23690},
  file      = {:SilvaRodriguez2024 - A Closer Look at the Few Shot Adaptation of Large Vision Language Models.pdf:PDF;:Silva-Rodriguez_A_Closer_Look_CVPR_2024_supplemental.pdf:PDF},
}

@InProceedings{Liu2024,
  author    = {Liu, Shihong and Yu, Samuel and Lin, Zhiqiu and Pathak, Deepak and Ramanan, Deva},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Language Models as Black-Box Optimizers for Vision-Language Models},
  year      = {2024},
  month     = {June},
  pages     = {12687-12697},
  file      = {:Liu2024 - Language Models As Black Box Optimizers for Vision Language Models.pdf:PDF;:Liu_Language_Models_as_CVPR_2024_supplemental.pdf:PDF},
}

@InProceedings{Du2022,
  author    = {Du, Yu and Wei, Fangyun and Zhang, Zihe and Shi, Miaojing and Gao, Yue and Li, Guoqi},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Learning To Prompt for Open-Vocabulary Object Detection With Vision-Language Model},
  year      = {2022},
  month     = {June},
  pages     = {14084-14093},
  file      = {:Du2022 - Learning to Prompt for Open Vocabulary Object Detection with Vision Language Model.pdf:PDF;:Du_Learning_To_Prompt_CVPR_2022_supplemental.pdf:PDF},
  groups    = {Few-Shot Transfer},
}

@InProceedings{Vaswani2017,
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Attention Is All You Need},
  year      = {2017},
  editor    = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  publisher = {Curran Associates, Inc.},
  volume    = {30},
  file      = {:Vaswani2017 - Attention Is All You Need.pdf:PDF},
  groups    = {Models},
  priority  = {prio2},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
}

@Article{Zhang2020,
  author        = {Zhang, Yuhao and Jiang, Hang and Miura, Yasuhide and Manning, Christopher D. and Langlotz, Curtis P.},
  title         = {Contrastive Learning of Medical Visual Representations from Paired Images and Text},
  year          = {2020},
  month         = oct,
  abstract      = {Learning visual representations of medical images (e.g., X-rays) is core to medical image understanding but its progress has been held back by the scarcity of human annotations. Existing work commonly relies on fine-tuning weights transferred from ImageNet pretraining, which is suboptimal due to drastically different image characteristics, or rule-based label extraction from the textual report data paired with medical images, which is inaccurate and hard to generalize. Meanwhile, several recent studies show exciting results from unsupervised contrastive learning from natural images, but we find these methods help little on medical images because of their high inter-class similarity. We propose ConVIRT, an alternative unsupervised strategy to learn medical visual representations by exploiting naturally occurring paired descriptive text. Our new method of pretraining medical image encoders with the paired text data via a bidirectional contrastive objective between the two modalities is domain-agnostic, and requires no additional expert input. We test ConVIRT by transferring our pretrained weights to 4 medical image classification tasks and 2 zero-shot retrieval tasks, and show that it leads to image representations that considerably outperform strong baselines in most settings. Notably, in all 4 classification tasks, our method requires only 10\% as much labeled training data as an ImageNet initialized counterpart to achieve better or comparable performance, demonstrating superior data efficiency.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2010.00747},
  eprint        = {2010.00747},
  file          = {:Zhang2020 - Contrastive Learning of Medical Visual Representations from Paired Images and Text.pdf:PDF:http\://arxiv.org/pdf/2010.00747v2},
  keywords      = {Computer Vision and Pattern Recognition (cs.CV), Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences},
  primaryclass  = {cs.CV},
  publisher     = {arXiv},
}

@Article{Radford2018,
  author    = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  title     = {Improving language understanding by generative pre-training},
  year      = {2018},
  file      = {:Radford2018 - Improving Language Understanding by Generative Pre Training.pdf:PDF},
  publisher = {OpenAI},
}

@Article{Chen2023a,
  author        = {Chen, Jun and Zhu, Deyao and Shen, Xiaoqian and Li, Xiang and Liu, Zechun and Zhang, Pengchuan and Krishnamoorthi, Raghuraman and Chandra, Vikas and Xiong, Yunyang and Elhoseiny, Mohamed},
  title         = {MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning},
  year          = {2023},
  month         = oct,
  abstract      = {Large language models have shown their remarkable capabilities as a general interface for various language-related applications. Motivated by this, we target to build a unified interface for completing many vision-language tasks including image description, visual question answering, and visual grounding, among others. The challenge is to use a single model for performing diverse vision-language tasks effectively with simple multi-modal instructions. Towards this objective, we introduce MiniGPT-v2, a model that can be treated as a unified interface for better handling various vision-language tasks. We propose using unique identifiers for different tasks when training the model. These identifiers enable our model to better distinguish each task instruction effortlessly and also improve the model learning efficiency for each task. After the three-stage training, the experimental results show that MiniGPT-v2 achieves strong performance on many visual question-answering and visual grounding benchmarks compared to other vision-language generalist models. Our model and codes are available at https://minigpt-v2.github.io/},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution 4.0 International},
  doi           = {10.48550/ARXIV.2310.09478},
  eprint        = {2310.09478},
  file          = {:Chen2023a - MiniGPT V2_ Large Language Model As a Unified Interface for Vision Language Multi Task Learning.pdf:PDF:http\://arxiv.org/pdf/2310.09478v3},
  groups        = {Models},
  keywords      = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
  primaryclass  = {cs.CV},
  publisher     = {arXiv},
}

@Article{Madasu2023,
  author        = {Madasu, Avinash and Lal, Vasudev},
  title         = {Is Multimodal Vision Supervision Beneficial to Language?},
  year          = {2023},
  month         = feb,
  abstract      = {Vision (image and video) - Language (VL) pre-training is the recent popular paradigm that achieved state-of-the-art results on multi-modal tasks like image-retrieval, video-retrieval, visual question answering etc. These models are trained in an unsupervised way and greatly benefit from the complementary modality supervision. In this paper, we explore if the language representations trained using vision supervision perform better than vanilla language representations on Natural Language Understanding and commonsense reasoning benchmarks. We experiment with a diverse set of image-text models such as ALBEF, BLIP, METER and video-text models like ALPRO, Frozen-in-Time (FiT), VIOLET. We compare the performance of language representations of stand-alone text encoders of these models to the language representations of text encoders learnt through vision supervision. Our experiments suggest that vanilla language representations show superior performance on most of the tasks. These results shed light on the current drawbacks of the vision-language models.},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution 4.0 International},
  doi           = {10.48550/ARXIV.2302.05016},
  eprint        = {2302.05016},
  file          = {:Madasu2023 - Is Multimodal Vision Supervision Beneficial to Language_.pdf:PDF:http\://arxiv.org/pdf/2302.05016v2},
  keywords      = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences},
  primaryclass  = {cs.CV},
  publisher     = {arXiv},
}

@Article{Chen2023b,
  author        = {Chen, Xiaofei and He, Yuting and Xue, Cheng and Ge, Rongjun and Li, Shuo and Yang, Guanyu},
  title         = {Knowledge Boosting: Rethinking Medical Contrastive Vision-Language Pre-Training},
  year          = {2023},
  month         = jul,
  abstract      = {The foundation models based on pre-training technology have significantly advanced artificial intelligence from theoretical to practical applications. These models have facilitated the feasibility of computer-aided diagnosis for widespread use. Medical contrastive vision-language pre-training, which does not require human annotations, is an effective approach for guiding representation learning using description information in diagnostic reports. However, the effectiveness of pre-training is limited by the large-scale semantic overlap and shifting problems in medical field. To address these issues, we propose the Knowledge-Boosting Contrastive Vision-Language Pre-training framework (KoBo), which integrates clinical knowledge into the learning of vision-language semantic consistency. The framework uses an unbiased, open-set sample-wise knowledge representation to measure negative sample noise and supplement the correspondence between vision-language mutual information and clinical knowledge. Extensive experiments validate the effect of our framework on eight tasks including classification, segmentation, retrieval, and semantic relatedness, achieving comparable or better performance with the zero-shot or few-shot settings. Our code is open on https://github.com/ChenXiaoFei-CS/KoBo.},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},
  doi           = {10.48550/ARXIV.2307.07246},
  eprint        = {2307.07246},
  file          = {:Chen2023b - Knowledge Boosting_ Rethinking Medical Contrastive Vision Language Pre Training.pdf:PDF:http\://arxiv.org/pdf/2307.07246v2},
  keywords      = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences},
  primaryclass  = {cs.CV},
  publisher     = {arXiv},
}

@Article{Zheng2023,
  author        = {Zheng, Zhaoheng and Wei, Jingmin and Hu, Xuefeng and Zhu, Haidong and Nevatia, Ram},
  title         = {Large Language Models are Good Prompt Learners for Low-Shot Image Classification},
  year          = {2023},
  month         = dec,
  abstract      = {Low-shot image classification, where training images are limited or inaccessible, has benefited from recent progress on pre-trained vision-language (VL) models with strong generalizability, e.g. CLIP. Prompt learning methods built with VL models generate text features from the class names that only have confined class-specific information. Large Language Models (LLMs), with their vast encyclopedic knowledge, emerge as the complement. Thus, in this paper, we discuss the integration of LLMs to enhance pre-trained VL models, specifically on low-shot classification. However, the domain gap between language and vision blocks the direct application of LLMs. Thus, we propose LLaMP, Large Language Models as Prompt learners, that produces adaptive prompts for the CLIP text encoder, establishing it as the connecting bridge. Experiments show that, compared with other state-of-the-art prompt learning methods, LLaMP yields better performance on both zero-shot generalization and few-shot image classification, over a spectrum of 11 datasets. Code will be made available at: https://github.com/zhaohengz/LLaMP.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2312.04076},
  eprint        = {2312.04076},
  file          = {:Zheng2023 - Large Language Models Are Good Prompt Learners for Low Shot Image Classification.pdf:PDF:http\://arxiv.org/pdf/2312.04076v2},
  keywords      = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
  primaryclass  = {cs.CV},
  publisher     = {arXiv},
}

@Misc{Gu2022,
  author        = {Xiuye Gu and Tsung-Yi Lin and Weicheng Kuo and Yin Cui},
  title         = {Open-vocabulary Object Detection via Vision and Language Knowledge Distillation},
  year          = {2022},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2104.13921},
  eprint        = {2104.13921},
  file          = {:Gu2022 - Open Vocabulary Object Detection Via Vision and Language Knowledge Distillation.pdf:PDF:http\://arxiv.org/pdf/2104.13921v3},
  primaryclass  = {id='cs.CV' full_name='Computer Vision and Pattern Recognition' is_active=True alt_name=None in_archive='cs' is_general=False description='Covers image processing, computer vision, pattern recognition, and scene understanding. Roughly includes material in ACM Subject Classes I.2.10, I.4, and I.5.'},
}

@InProceedings{Krizhevsky2012,
  author    = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {ImageNet Classification with Deep Convolutional Neural Networks},
  year      = {2012},
  editor    = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},
  publisher = {Curran Associates, Inc.},
  volume    = {25},
  file      = {:Krizhevsky2012 - ImageNet Classification with Deep Convolutional Neural Networks.pdf:PDF},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
}

@Article{Peng2023,
  author        = {Peng, Zhiliang and Wang, Wenhui and Dong, Li and Hao, Yaru and Huang, Shaohan and Ma, Shuming and Wei, Furu},
  title         = {Kosmos-2: Grounding Multimodal Large Language Models to the World},
  year          = {2023},
  month         = jun,
  abstract      = {We introduce Kosmos-2, a Multimodal Large Language Model (MLLM), enabling new capabilities of perceiving object descriptions (e.g., bounding boxes) and grounding text to the visual world. Specifically, we represent refer expressions as links in Markdown, i.e., ``[text span](bounding boxes)'', where object descriptions are sequences of location tokens. Together with multimodal corpora, we construct large-scale data of grounded image-text pairs (called GrIT) to train the model. In addition to the existing capabilities of MLLMs (e.g., perceiving general modalities, following instructions, and performing in-context learning), Kosmos-2 integrates the grounding capability into downstream applications. We evaluate Kosmos-2 on a wide range of tasks, including (i) multimodal grounding, such as referring expression comprehension, and phrase grounding, (ii) multimodal referring, such as referring expression generation, (iii) perception-language tasks, and (iv) language understanding and generation. This work lays out the foundation for the development of Embodiment AI and sheds light on the big convergence of language, multimodal perception, action, and world modeling, which is a key step toward artificial general intelligence. Code and pretrained models are available at https://aka.ms/kosmos-2.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2306.14824},
  eprint        = {2306.14824},
  file          = {:Peng2023 - Kosmos 2_ Grounding Multimodal Large Language Models to the World.pdf:PDF:http\://arxiv.org/pdf/2306.14824v3},
  groups        = {Models},
  keywords      = {Computation and Language (cs.CL), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  publisher     = {arXiv},
}

@Article{Touvron2023,
  author        = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timothée and Rozière, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
  title         = {LLaMA: Open and Efficient Foundation Language Models},
  year          = {2023},
  month         = feb,
  abstract      = {We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution 4.0 International},
  doi           = {10.48550/ARXIV.2302.13971},
  eprint        = {2302.13971},
  file          = {:Touvron2023 - LLaMA_ Open and Efficient Foundation Language Models.pdf:PDF:http\://arxiv.org/pdf/2302.13971v1},
  groups        = {Models},
  keywords      = {Computation and Language (cs.CL), FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  publisher     = {arXiv},
}

@InProceedings{Chen2022,
  author    = {Chen, Jun and Guo, Han and Yi, Kai and Li, Boyang and Elhoseiny, Mohamed},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {VisualGPT: Data-Efficient Adaptation of Pretrained Language Models for Image Captioning},
  year      = {2022},
  month     = {June},
  pages     = {18030-18040},
  file      = {:Chen2022 - VisualGPT_ Data Efficient Adaptation of Pretrained Language Models for Image Captioning.pdf:PDF;:Chen_VisualGPT_Data-Efficient_Adaptation_CVPR_2022_supplemental.pdf:PDF},
  groups    = {Models},
}

@Misc{Driess2023,
  author        = {Danny Driess and Fei Xia and Mehdi S. M. Sajjadi and Corey Lynch and Aakanksha Chowdhery and Brian Ichter and Ayzaan Wahid and Jonathan Tompson and Quan Vuong and Tianhe Yu and Wenlong Huang and Yevgen Chebotar and Pierre Sermanet and Daniel Duckworth and Sergey Levine and Vincent Vanhoucke and Karol Hausman and Marc Toussaint and Klaus Greff and Andy Zeng and Igor Mordatch and Pete Florence},
  title         = {PaLM-E: An Embodied Multimodal Language Model},
  year          = {2023},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2303.03378},
  eprint        = {2303.03378},
  file          = {:Driess2023 - PaLM E_ an Embodied Multimodal Language Model.pdf:PDF:https\://arxiv.org/pdf/2303.03378.pdf},
  groups        = {Models},
  primaryclass  = {id='cs.LG' full_name='Machine Learning' is_active=True alt_name=None in_archive='cs' is_general=False description='Papers on all aspects of machine learning research (supervised, unsupervised, reinforcement learning, bandit problems, and so on) including also robustness, explanation, fairness, and methodology. cs.LG is also an appropriate primary category for applications of machine learning methods.'},
}

@InProceedings{Alayrac2022,
  author    = {Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and Ring, Roman and Rutherford, Eliza and Cabi, Serkan and Han, Tengda and Gong, Zhitao and Samangooei, Sina and Monteiro, Marianne and Menick, Jacob L and Borgeaud, Sebastian and Brock, Andy and Nematzadeh, Aida and Sharifzadeh, Sahand and Bi\'{n}kowski, Miko\l aj and Barreira, Ricardo and Vinyals, Oriol and Zisserman, Andrew and Simonyan, Kar\'{e}n},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Flamingo: a {V}isual {L}anguage {M}odel for {F}ew-{S}hot {L}earning},
  year      = {2022},
  editor    = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
  pages     = {23716--23736},
  publisher = {Curran Associates, Inc.},
  volume    = {35},
  file      = {:Alayrac2022 - Flamingo_ a Visual Language Model for Few Shot Learning.pdf:PDF},
  groups    = {Models},
  priority  = {prio2},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2022/file/960a172bc7fbf0177ccccbb411a7d800-Paper-Conference.pdf},
}

@Article{ChameleonTeam2024,
  author        = {{Chameleon Team}},
  journal       = {arXiv preprint},
  title         = {{Chameleon: Mixed-Modal Early-Fusion Foundation Models}},
  year          = {2024},
  month         = may,
  abstract      = {We present Chameleon, a family of early-fusion token-based mixed-modal models capable of understanding and generating images and text in any arbitrary sequence. We outline a stable training approach from inception, an alignment recipe, and an architectural parameterization tailored for the early-fusion, token-based, mixed-modal setting. The models are evaluated on a comprehensive range of tasks, including visual question answering, image captioning, text generation, image generation, and long-form mixed modal generation. Chameleon demonstrates broad and general capabilities, including state-of-the-art performance in image captioning tasks, outperforms Llama-2 in text-only tasks while being competitive with models such as Mixtral 8x7B and Gemini-Pro, and performs non-trivial image generation, all in a single model. It also matches or exceeds the performance of much larger models, including Gemini Pro and GPT-4V, according to human judgments on a new long-form mixed-modal generation evaluation, where either the prompt or outputs contain mixed sequences of both images and text. Chameleon marks a significant step forward in a unified modeling of full multimodal documents.},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution 4.0 International},
  doi           = {10.48550/ARXIV.2405.09818},
  eprint        = {2405.09818},
  file          = {:ChameleonTeam2024 - Chameleon_ Mixed Modal Early Fusion Foundation Models.pdf:PDF:http\://arxiv.org/pdf/2405.09818v1},
  groups        = {Models},
  keywords      = {Computation and Language (cs.CL), FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  priority      = {prio1},
  publisher     = {arXiv},
}

@Article{Bai2023,
  author        = {Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},
  journal       = {arXiv preprint},
  title         = {Qwen-{VL}: {A} {V}ersatile {V}ision-{L}anguage {M}odel for {U}nderstanding, {L}ocalization, {T}ext {R}eading, and {B}eyond},
  year          = {2023},
  month         = aug,
  abstract      = {In this work, we introduce the Qwen-VL series, a set of large-scale vision-language models (LVLMs) designed to perceive and understand both texts and images. Starting from the Qwen-LM as a foundation, we endow it with visual capacity by the meticulously designed (i) visual receptor, (ii) input-output interface, (iii) 3-stage training pipeline, and (iv) multilingual multimodal cleaned corpus. Beyond the conventional image description and question-answering, we implement the grounding and text-reading ability of Qwen-VLs by aligning image-caption-box tuples. The resulting models, including Qwen-VL and Qwen-VL-Chat, set new records for generalist models under similar model scales on a broad range of visual-centric benchmarks (e.g., image captioning, question answering, visual grounding) and different settings (e.g., zero-shot, few-shot). Moreover, on real-world dialog benchmarks, our instruction-tuned Qwen-VL-Chat also demonstrates superiority compared to existing vision-language chatbots. Code, demo and models are available at https://github.com/QwenLM/Qwen-VL.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2308.12966},
  eprint        = {2308.12966},
  file          = {:Bai2023 - Qwen VL_ a Versatile Vision Language Model for Understanding, Localization, Text Reading, and beyond.pdf:PDF:http\://arxiv.org/pdf/2308.12966v3},
  groups        = {Models},
  keywords      = {Computer Vision and Pattern Recognition (cs.CV), Computation and Language (cs.CL), FOS: Computer and information sciences},
  primaryclass  = {cs.CV},
  priority      = {prio1},
  publisher     = {arXiv},
}

@Article{Zhou2024,
  author        = {Zhou, Baichuan and Hu, Ying and Weng, Xi and Jia, Junlong and Luo, Jie and Liu, Xien and Wu, Ji and Huang, Lei},
  title         = {TinyLLaVA: A Framework of Small-scale Large Multimodal Models},
  year          = {2024},
  month         = feb,
  abstract      = {We present the TinyLLaVA framework that provides a unified perspective in designing and analyzing the small-scale Large Multimodal Models (LMMs). We empirically study the effects of different vision encoders, connection modules, language models, training data and training recipes. Our extensive experiments showed that better quality of data combined with better training recipes, smaller LMMs can consistently achieve on-par performances compared to bigger LMMs. Under our framework, we train a family of small-scale LMMs. Our best model, TinyLLaVA-3.1B, achieves better overall performance against existing 7B models such as LLaVA-1.5 and Qwen-VL. We hope our findings can serve as baselines for future research in terms of data scaling, training setups and model selections. Our model weights and codes will be made public.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2402.14289},
  eprint        = {2402.14289},
  file          = {:Zhou2024 - TinyLLaVA_ a Framework of Small Scale Large Multimodal Models.pdf:PDF:http\://arxiv.org/pdf/2402.14289v1},
  groups        = {Models},
  keywords      = {Machine Learning (cs.LG), Computation and Language (cs.CL), FOS: Computer and information sciences},
  primaryclass  = {cs.LG},
  publisher     = {arXiv},
}

@Article{Wang2023,
  author    = {Wang, Xiao and Chen, Guangyao and Qian, Guangwu and Gao, Pengcheng and Wei, Xiao-Yong and Wang, Yaowei and Tian, Yonghong and Gao, Wen},
  journal   = {Machine Intelligence Research},
  title     = {{Large-scale Multi-modal Pre-trained Models: A Comprehensive Survey}},
  year      = {2023},
  issn      = {2731-5398},
  month     = jun,
  number    = {4},
  pages     = {447--482},
  volume    = {20},
  doi       = {10.1007/s11633-022-1410-8},
  file      = {:Wang2023 - Large Scale Multi Modal Pre Trained Models_ a Comprehensive Survey.pdf:PDF},
  priority  = {prio2},
  publisher = {Springer Science and Business Media LLC},
}

@InProceedings{Jia2021,
  author    = {Jia, Chao and Yang, Yinfei and Xia, Ye and Chen, Yi-Ting and Parekh, Zarana and Pham, Hieu and Le, Quoc and Sung, Yun-Hsuan and Li, Zhen and Duerig, Tom},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  title     = {Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision},
  year      = {2021},
  editor    = {Meila, Marina and Zhang, Tong},
  month     = {18--24 Jul},
  pages     = {4904--4916},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {139},
  abstract  = {Pre-trained representations are becoming crucial for many NLP and perception tasks. While representation learning in NLP has transitioned to training on raw text without human annotations, visual and vision-language representations still rely heavily on curated training datasets that are expensive or require expert knowledge. For vision applications, representations are mostly learned using datasets with explicit class labels such as ImageNet or OpenImages. For vision-language, popular datasets like Conceptual Captions, MSCOCO, or CLIP all involve a non-trivial data collection (and cleaning) process. This costly curation process limits the size of datasets and hence hinders the scaling of trained models. In this paper, we leverage a noisy dataset of over one billion image alt-text pairs, obtained without expensive filtering or post-processing steps in the Conceptual Captions dataset. A simple dual-encoder architecture learns to align visual and language representations of the image and text pairs using a contrastive loss. We show that the scale of our corpus can make up for its noise and leads to state-of-the-art representations even with such a simple learning scheme. Our visual representation achieves strong performance when transferred to classification tasks such as ImageNet and VTAB. The aligned visual and language representations enables zero-shot image classification and also set new state-of-the-art results on Flickr30K and MSCOCO image-text retrieval benchmarks, even when compared with more sophisticated cross-attention models. The representations also enable cross-modality search with complex text and text + image queries.},
  file      = {:Jia2021 - Scaling up Visual and Vision Language Representation Learning with Noisy Text Supervision.pdf:PDF;:jia21b-supp.pdf:PDF},
  pdf       = {http://proceedings.mlr.press/v139/jia21b/jia21b.pdf},
  url       = {https://proceedings.mlr.press/v139/jia21b.html},
}

@Misc{Yao2021,
  author        = {Lewei Yao and Runhui Huang and Lu Hou and Guansong Lu and Minzhe Niu and Hang Xu and Xiaodan Liang and Zhenguo Li and Xin Jiang and Chunjing Xu},
  title         = {FILIP: Fine-grained Interactive Language-Image Pre-Training},
  year          = {2021},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2111.07783},
  eprint        = {2111.07783},
  file          = {:Yao2021 - FILIP_ Fine Grained Interactive Language Image Pre Training.pdf:PDF:http\://arxiv.org/pdf/2111.07783v1},
  primaryclass  = {id='cs.CV' full_name='Computer Vision and Pattern Recognition' is_active=True alt_name=None in_archive='cs' is_general=False description='Covers image processing, computer vision, pattern recognition, and scene understanding. Roughly includes material in ACM Subject Classes I.2.10, I.4, and I.5.'},
}

@InProceedings{Li2021,
  author       = {Li, Xiang Lisa and Liang, Percy},
  booktitle    = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  title        = {Prefix-Tuning: Optimizing Continuous Prompts for Generation},
  year         = {2021},
  address      = {Online},
  editor       = {Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto},
  month        = aug,
  pages        = {4582--4597},
  publisher    = {Association for Computational Linguistics},
  abstract     = {Fine-tuning is the de facto way of leveraging large pretrained language models for downstream tasks. However, fine-tuning modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen and instead optimizes a sequence of continuous task-specific vectors, which we call the prefix. Prefix-tuning draws inspiration from prompting for language models, allowing subsequent tokens to attend to this prefix as if it were {``}virtual tokens{''}. We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. We show that by learning only 0.1{\%} of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics that are unseen during training.},
  doi          = {10.18653/v1/2021.acl-long.353},
  file         = {:Li2021 - Prefix Tuning_ Optimizing Continuous Prompts for Generation.pdf:PDF},
  primaryclass = {id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'},
  priority     = {prio1},
  url          = {https://aclanthology.org/2021.acl-long.353},
}

@InProceedings{Wang2022a,
  author    = {Wang, Peng and Yang, An and Men, Rui and Lin, Junyang and Bai, Shuai and Li, Zhikang and Ma, Jianxin and Zhou, Chang and Zhou, Jingren and Yang, Hongxia},
  booktitle = {Proceedings of the 39th International Conference on Machine Learning},
  title     = {{OFA}: {U}nifying {A}rchitectures, {T}asks, and {M}odalities {T}hrough a {S}imple {S}equence-to-{S}equence {L}earning {F}ramework},
  year      = {2022},
  editor    = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  month     = {17--23 Jul},
  pages     = {23318--23340},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {162},
  abstract  = {In this work, we pursue a unified paradigm for multimodal pretraining to break the shackles of complex task/modality-specific customization. We propose OFA, a Task-Agnostic and Modality-Agnostic framework that supports Task Comprehensiveness. OFA unifies a diverse set of cross-modal and unimodal tasks, including image generation, visual grounding, image captioning, image classification, language modeling, etc., in a simple sequence-to-sequence learning framework. OFA follows the instruction-based learning in both pretraining and finetuning stages, requiring no extra task-specific layers for downstream tasks. In comparison with the recent state-of-the-art vision &amp; language models that rely on extremely large cross-modal datasets, OFA is pretrained on only 20M publicly available image-text pairs. Despite its simplicity and relatively small-scale training data, OFA achieves new SOTAs in a series of cross-modal tasks while attaining highly competitive performances on uni-modal tasks. Our further analysis indicates that OFA can also effectively transfer to unseen tasks and unseen domains. Our code and models are publicly available at https://github.com/OFA-Sys/OFA.},
  file      = {:Wang2022a - OFA_ Unifying Architectures, Tasks, and Modalities through a Simple Sequence to Sequence Learning Framework.pdf:PDF},
  pdf       = {https://proceedings.mlr.press/v162/wang22al/wang22al.pdf},
  priority  = {prio3},
  url       = {https://proceedings.mlr.press/v162/wang22al.html},
}

@InProceedings{Wang2023a,
  author    = {Wang, Xinlong and Wang, Wen and Cao, Yue and Shen, Chunhua and Huang, Tiejun},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Images Speak in Images: A Generalist Painter for In-Context Visual Learning},
  year      = {2023},
  month     = {June},
  pages     = {6830-6839},
  file      = {:Wang2023a - Images Speak in Images_ a Generalist Painter for in Context Visual Learning.pdf:PDF;:Wang_Images_Speak_in_CVPR_2023_supplemental.pdf:PDF},
}

@InProceedings{Houlsby2019,
  author    = {Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning},
  title     = {{Parameter-Efficient Transfer Learning for NLP}},
  year      = {2019},
  editor    = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  month     = {09--15 Jun},
  pages     = {2790--2799},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {97},
  abstract  = {Fine-tuning large pretrained models is an effective transfer mechanism in NLP. However, in the presence of many downstream tasks, fine-tuning is parameter inefficient: an entire new model is required for every task. As an alternative, we propose transfer with adapter modules. Adapter modules yield a compact and extensible model; they add only a few trainable parameters per task, and new tasks can be added without revisiting previous ones. The parameters of the original network remain fixed, yielding a high degree of parameter sharing. To demonstrate adapter’s effectiveness, we transfer the recently proposed BERT Transformer model to $26$ diverse text classification tasks, including the GLUE benchmark. Adapters attain near state-of-the-art performance, whilst adding only a few parameters per task. On GLUE, we attain within $0.8%$ of the performance of full fine-tuning, adding only $3.6%$ parameters per task. By contrast, fine-tuning trains $100%$ of the parameters per task.},
  file      = {:Houlsby2019 - Parameter Efficient Transfer Learning for NLP.pdf:PDF:http\://arxiv.org/pdf/1902.00751v2;:houlsby19a-supp.pdf:PDF},
  pdf       = {http://proceedings.mlr.press/v97/houlsby19a/houlsby19a.pdf},
  url       = {https://proceedings.mlr.press/v97/houlsby19a.html},
}

@Article{Han2024,
  author        = {Zeyu Han and Chao Gao and Jinyang Liu and Jeff Zhang and Sai Qian Zhang},
  journal       = {arXiv preprint},
  title         = {{Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey}},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2403.14608},
  eprint        = {2403.14608},
  file          = {:Han2024 - Parameter Efficient Fine Tuning for Large Models_ a Comprehensive Survey.pdf:PDF:http\://arxiv.org/pdf/2403.14608v5},
  primaryclass  = {id='cs.LG' full_name='Machine Learning' is_active=True alt_name=None in_archive='cs' is_general=False description='Papers on all aspects of machine learning research (supervised, unsupervised, reinforcement learning, bandit problems, and so on) including also robustness, explanation, fairness, and methodology. cs.LG is also an appropriate primary category for applications of machine learning methods.'},
}

@InProceedings{Liu2023a,
  author       = {Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  booktitle    = {Advances in Neural Information Processing Systems},
  title        = {Visual Instruction Tuning},
  year         = {2023},
  editor       = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
  pages        = {34892--34916},
  publisher    = {Curran Associates, Inc.},
  volume       = {36},
  file         = {:Liu2023a - Visual Instruction Tuning.pdf:PDF:http\://arxiv.org/pdf/2304.08485v2},
  groups       = {Models},
  primaryclass = {id='cs.CV' full_name='Computer Vision and Pattern Recognition' is_active=True alt_name=None in_archive='cs' is_general=False description='Covers image processing, computer vision, pattern recognition, and scene understanding. Roughly includes material in ACM Subject Classes I.2.10, I.4, and I.5.'},
  url          = {https://proceedings.neurips.cc/paper_files/paper/2023/file/6dcf277ea32ce3288914faf369fe6de0-Paper-Conference.pdf},
}

@InProceedings{Liu2024a,
  author       = {Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae},
  booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title        = {{Improved Baselines with Visual Instruction Tuning}},
  year         = {2024},
  month        = {June},
  pages        = {26296-26306},
  file         = {:Liu2024a - Improved Baselines with Visual Instruction Tuning.pdf:PDF:http\://arxiv.org/pdf/2310.03744v2},
  groups       = {Models},
  primaryclass = {id='cs.CV' full_name='Computer Vision and Pattern Recognition' is_active=True alt_name=None in_archive='cs' is_general=False description='Covers image processing, computer vision, pattern recognition, and scene understanding. Roughly includes material in ACM Subject Classes I.2.10, I.4, and I.5.'},
}

@Misc{Zhang2022,
  author        = {Susan Zhang and Stephen Roller and Naman Goyal and Mikel Artetxe and Moya Chen and Shuohui Chen and Christopher Dewan and Mona Diab and Xian Li and Xi Victoria Lin and Todor Mihaylov and Myle Ott and Sam Shleifer and Kurt Shuster and Daniel Simig and Punit Singh Koura and Anjali Sridhar and Tianlu Wang and Luke Zettlemoyer},
  title         = {OPT: Open Pre-trained Transformer Language Models},
  year          = {2022},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2205.01068},
  eprint        = {2205.01068},
  file          = {:Zhang2022 - OPT_ Open Pre Trained Transformer Language Models.pdf:PDF:http\://arxiv.org/pdf/2205.01068v4},
  groups        = {Models},
  primaryclass  = {id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'},
}

@Misc{Lester2021,
  author        = {Brian Lester and Rami Al-Rfou and Noah Constant},
  title         = {The Power of Scale for Parameter-Efficient Prompt Tuning},
  year          = {2021},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2104.08691},
  eprint        = {2104.08691},
  file          = {:Lester2021 - The Power of Scale for Parameter Efficient Prompt Tuning.pdf:PDF:https\://aclanthology.org/2021.emnlp-main.243.pdf},
  primaryclass  = {id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'},
}

@InProceedings{Zou2023,
  author    = {Zou, Xueyan and Dou, Zi-Yi and Yang, Jianwei and Gan, Zhe and Li, Linjie and Li, Chunyuan and Dai, Xiyang and Behl, Harkirat and Wang, Jianfeng and Yuan, Lu and Peng, Nanyun and Wang, Lijuan and Lee, Yong Jae and Gao, Jianfeng},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Generalized Decoding for Pixel, Image, and Language},
  year      = {2023},
  month     = {June},
  pages     = {15116-15127},
  file      = {:Zou2023 - Generalized Decoding for Pixel, Image, and Language.pdf:PDF;:Zou_Generalized_Decoding_for_CVPR_2023_supplemental.pdf:PDF},
}

@Misc{Huang2024,
  author        = {Chenghao Huang and Yanbo Cao and Yinlong Wen and Tao Zhou and Yanru Zhang},
  title         = {PokerGPT: An End-to-End Lightweight Solver for Multi-Player Texas Hold'em via Large Language Model},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2401.06781},
  eprint        = {2401.06781},
  eprinttype    = {arxiv},
  file          = {:Huang2024 - PokerGPT_ an End to End Lightweight Solver for Multi Player Texas Hold'em Via Large Language Model.pdf:PDF:http\://arxiv.org/pdf/2401.06781v1},
  groups        = {Models},
  primaryclass  = {cs.AI},
}

@Article{Wu2024,
  author    = {Wu, Jiamin and Zhang, Tianzhu and Zhang, Yongdong},
  journal   = {International Journal of Computer Vision},
  title     = {HybridPrompt: Domain-Aware Prompting for Cross-Domain Few-Shot Learning},
  year      = {2024},
  issn      = {1573-1405},
  month     = jun,
  doi       = {10.1007/s11263-024-02086-8},
  file      = {:Wu2024 - HybridPrompt_ Domain Aware Prompting for Cross Domain Few Shot Learning.pdf:PDF:https\://link.springer.com/content/pdf/10.1007/s11263-024-02086-8.pdf},
  publisher = {Springer Science and Business Media LLC},
}

@InProceedings{Guo2020,
  author    = {Guo, Yunhui and Codella, Noel C. and Karlinsky, Leonid and Codella, James V. and Smith, John R. and Saenko, Kate and Rosing, Tajana and Feris, Rogerio},
  booktitle = {Computer Vision -- ECCV 2020},
  title     = {A Broader Study of Cross-Domain Few-Shot Learning},
  year      = {2020},
  address   = {Cham},
  editor    = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
  pages     = {124--141},
  publisher = {Springer International Publishing},
  abstract  = {Recent progress on few-shot learning largely relies on annotated data for meta-learning: base classes sampled from the same domain as the novel classes. However, in many applications, collecting data for meta-learning is infeasible or impossible. This leads to the cross-domain few-shot learning problem, where there is a large shift between base and novel class domains. While investigations of the cross-domain few-shot scenario exist, these works are limited to natural images that still contain a high degree of visual similarity. No work yet exists that examines few-shot learning across different imaging methods seen in real world scenarios, such as aerial and medical imaging. In this paper, we propose the Broader Study of Cross-Domain Few-Shot Learning (BSCD-FSL) benchmark, consisting of image data from a diverse assortment of image acquisition methods. This includes natural images, such as crop disease images, but additionally those that present with an increasing dissimilarity to natural images, such as satellite images, dermatology images, and radiology images. Extensive experiments on the proposed benchmark are performed to evaluate state-of-art meta-learning approaches, transfer learning approaches, and newer methods for cross-domain few-shot learning. The results demonstrate that state-of-art meta-learning methods are surprisingly outperformed by earlier meta-learning approaches, and all meta-learning methods underperform in relation to simple fine-tuning by 12.8{\%} average accuracy. In some cases, meta-learning even underperforms networks with random weights. Performance gains previously observed with methods specialized for cross-domain few-shot learning vanish in this more challenging benchmark. Finally, accuracy of all methods tend to correlate with dataset similarity to natural images, verifying the value of the benchmark to better represent the diversity of data seen in practice and guiding future research. Code for the experiments in this work can be found at https://github.com/IBM/cdfsl-benchmark.},
  file      = {:Guo2020 - A Broader Study of Cross Domain Few Shot Learning.pdf:PDF:https\://link.springer.com/content/pdf/10.1007/978-3-030-58583-9_8.pdf},
  isbn      = {978-3-030-58583-9},
}

@InProceedings{Houben2013,
  author    = {Houben, Sebastian and Stallkamp, Johannes and Salmen, Jan and Schlipsing, Marc and Igel, Christian},
  booktitle = {The 2013 International Joint Conference on Neural Networks (IJCNN)},
  title     = {{Detection of Traffic Signs in Real-World Images: The German Traffic Sign Detection Benchmark}},
  year      = {2013},
  pages     = {1-8},
  doi       = {10.1109/IJCNN.2013.6706807},
  file      = {:Houben2013 - Detection of Traffic Signs in Real World Images_ the German Traffic Sign Detection Benchmark.pdf:PDF},
  groups    = {Datasets},
  keywords  = {Benchmark testing;Detectors;Image color analysis;Training;Shape;Image edge detection;Feature extraction},
}

@Article{Yu2022,
  author        = {Jiahui Yu and Zirui Wang and Vijay Vasudevan and Legg Yeung and Mojtaba Seyedhosseini and Yonghui Wu},
  journal       = {arXiv preprint},
  title         = {{CoCa: Contrastive Captioners are Image-Text Foundation Models}},
  year          = {2022},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2205.01917},
  eprint        = {2205.01917},
  eprinttype    = {arxiv},
  file          = {:Yu2022 - CoCa_ Contrastive Captioners Are Image Text Foundation Models.pdf:PDF:https\://arxiv.org/pdf/2205.01917.pdf},
  groups        = {Models},
  primaryclass  = {cs.CV},
  priority      = {prio1},
}

@Misc{Liu2023b,
  author        = {Shilong Liu and Hao Cheng and Haotian Liu and Hao Zhang and Feng Li and Tianhe Ren and Xueyan Zou and Jianwei Yang and Hang Su and Jun Zhu and Lei Zhang and Jianfeng Gao and Chunyuan Li},
  title         = {LLaVA-Plus: Learning to Use Tools for Creating Multimodal Agents},
  year          = {2023},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2311.05437},
  eprint        = {2311.05437},
  eprinttype    = {arxiv},
  file          = {:Liu2023b - LLaVA Plus_ Learning to Use Tools for Creating Multimodal Agents.pdf:PDF:http\://arxiv.org/pdf/2311.05437v1},
  groups        = {Models},
  primaryclass  = {cs.CV},
}

@InProceedings{Mohta2023,
  author    = {Mohta, Jay and Ak, Kenan and Xu, Yan and Shen, Mingwei},
  booktitle = {Proceedings on "I Can't Believe It's Not Better: Failure Modes in the Age of Foundation Models" at NeurIPS 2023 Workshops},
  title     = {Are large language models good annotators?},
  year      = {2023},
  editor    = {Antorán, Javier and Blaas, Arno and Buchanan, Kelly and Feng, Fan and Fortuin, Vincent and Ghalebikesabi, Sahra and Kriegler, Andreas and Mason, Ian and Rohde, David and Ruiz, Francisco J. R. and Uelwer, Tobias and Xie, Yubin and Yang, Rui},
  month     = {16 Dec},
  pages     = {38--48},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {239},
  abstract  = {Numerous Natural Language Processing (NLP) tasks require precisely labeled data to ensure effective model training and achieve optimal performance. However, data annotation is marked by substantial costs and time requirements, especially when requiring specialized domain expertise or annotating a large number of samples. In this study, we investigate the feasibility of employing large language models (LLMs) as replacements for human annotators. We assess the zero-shot performance of various LLMs of different sizes to determine their viability as substitutes. Furthermore, recognizing that human annotators have access to diverse modalities, we introduce an image-based modality using the BLIP-2 architecture to evaluate LLM annotation performance. Among the tested LLMs, Vicuna-13b demonstrates competitive performance across diverse tasks. To assess the potential for LLMs to replace human annotators, we train a supervised model using labels generated by LLMs and compare its performance with models trained using human-generated labels. However, our findings reveal that models trained with human labels consistently outperform those trained with LLM-generated labels. We also highlights the challenges faced by LLMs in multilingual settings, where their performance significantly diminishes for tasks in languages other than English.},
  file      = {:Mohta2023 - Are Large Language Models Good Annotators_.pdf:PDF},
  pdf       = {https://proceedings.mlr.press/v239/mohta23a/mohta23a.pdf},
  url       = {https://proceedings.mlr.press/v239/mohta23a.html},
}

@Misc{Bommasani2022,
  author        = {Rishi Bommasani and Drew A. Hudson and Ehsan Adeli and Russ Altman and Simran Arora and Sydney von Arx and Michael S. Bernstein and Jeannette Bohg and Antoine Bosselut and Emma Brunskill and Erik Brynjolfsson and Shyamal Buch and Dallas Card and Rodrigo Castellon and Niladri Chatterji and Annie Chen and Kathleen Creel and Jared Quincy Davis and Dora Demszky and Chris Donahue and Moussa Doumbouya and Esin Durmus and Stefano Ermon and John Etchemendy and Kawin Ethayarajh and Li Fei-Fei and Chelsea Finn and Trevor Gale and Lauren Gillespie and Karan Goel and Noah Goodman and Shelby Grossman and Neel Guha and Tatsunori Hashimoto and Peter Henderson and John Hewitt and Daniel E. Ho and Jenny Hong and Kyle Hsu and Jing Huang and Thomas Icard and Saahil Jain and Dan Jurafsky and Pratyusha Kalluri and Siddharth Karamcheti and Geoff Keeling and Fereshte Khani and Omar Khattab and Pang Wei Koh and Mark Krass and Ranjay Krishna and Rohith Kuditipudi and Ananya Kumar and Faisal Ladhak and Mina Lee and Tony Lee and Jure Leskovec and Isabelle Levent and Xiang Lisa Li and Xuechen Li and Tengyu Ma and Ali Malik and Christopher D. Manning and Suvir Mirchandani and Eric Mitchell and Zanele Munyikwa and Suraj Nair and Avanika Narayan and Deepak Narayanan and Ben Newman and Allen Nie and Juan Carlos Niebles and Hamed Nilforoshan and Julian Nyarko and Giray Ogut and Laurel Orr and Isabel Papadimitriou and Joon Sung Park and Chris Piech and Eva Portelance and Christopher Potts and Aditi Raghunathan and Rob Reich and Hongyu Ren and Frieda Rong and Yusuf Roohani and Camilo Ruiz and Jack Ryan and Christopher Ré and Dorsa Sadigh and Shiori Sagawa and Keshav Santhanam and Andy Shih and Krishnan Srinivasan and Alex Tamkin and Rohan Taori and Armin W. Thomas and Florian Tramèr and Rose E. Wang and William Wang and Bohan Wu and Jiajun Wu and Yuhuai Wu and Sang Michael Xie and Michihiro Yasunaga and Jiaxuan You and Matei Zaharia and Michael Zhang and Tianyi Zhang and Xikun Zhang and Yuhui Zhang and Lucia Zheng and Kaitlyn Zhou and Percy Liang},
  title         = {On the Opportunities and Risks of Foundation Models},
  year          = {2022},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2108.07258},
  eprint        = {2108.07258},
  eprinttype    = {arxiv},
  file          = {:Bommasani2022 - On the Opportunities and Risks of Foundation Models.pdf:PDF:http\://arxiv.org/pdf/2108.07258v3},
  primaryclass  = {cs.LG},
}

@Misc{Lepikhin2020,
  author        = {Dmitry Lepikhin and HyoukJoong Lee and Yuanzhong Xu and Dehao Chen and Orhan Firat and Yanping Huang and Maxim Krikun and Noam Shazeer and Zhifeng Chen},
  title         = {GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding},
  year          = {2020},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2006.16668},
  eprint        = {2006.16668},
  eprinttype    = {arxiv},
  file          = {:Lepikhin2020 - GShard_ Scaling Giant Models with Conditional Computation and Automatic Sharding.pdf:PDF:http\://arxiv.org/pdf/2006.16668v1},
  primaryclass  = {cs.CL},
}

@Misc{Efrat2020,
  author        = {Avia Efrat and Omer Levy},
  title         = {The Turking Test: Can Language Models Understand Instructions?},
  year          = {2020},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2010.11982},
  eprint        = {2010.11982},
  eprinttype    = {arxiv},
  file          = {:Efrat2020 - The Turking Test_ Can Language Models Understand Instructions_.pdf:PDF:http\://arxiv.org/pdf/2010.11982v1},
  primaryclass  = {cs.CL},
}

@InProceedings{He2022a,
  author    = {He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll\'ar, Piotr and Girshick, Ross},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Masked Autoencoders Are Scalable Vision Learners},
  year      = {2022},
  month     = {June},
  pages     = {16000-16009},
  file      = {:He2022a - Masked Autoencoders Are Scalable Vision Learners.pdf:PDF;:He_Masked_Autoencoders_Are_CVPR_2022_supplemental.pdf:PDF},
  priority  = {prio3},
}

@Article{Li2023b,
  author        = {Bo Li and Yuanhan Zhang and Liangyu Chen and Jinghao Wang and Fanyi Pu and Jingkang Yang and Chunyuan Li and Ziwei Liu},
  journal       = {arXiv preprint},
  title         = {{MIMIC-IT: Multi-Modal In-Context Instruction Tuning}},
  year          = {2023},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2306.05425},
  eprint        = {2306.05425},
  eprinttype    = {arxiv},
  file          = {:Li2023b - MIMIC IT_ Multi Modal in Context Instruction Tuning.pdf:PDF:http\://arxiv.org/pdf/2306.05425v1},
  primaryclass  = {cs.CV},
}

@Misc{Najdenkoska2023,
  author        = {Ivona Najdenkoska and Xiantong Zhen and Marcel Worring},
  title         = {Meta Learning to Bridge Vision and Language Models for Multimodal Few-Shot Learning},
  year          = {2023},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2302.14794},
  eprint        = {2302.14794},
  eprinttype    = {arxiv},
  file          = {:Najdenkoska2023 - Meta Learning to Bridge Vision and Language Models for Multimodal Few Shot Learning.pdf:PDF:https\://arxiv.org/pdf/2302.14794.pdf},
  primaryclass  = {cs.CV},
}

@InProceedings{Liu2023c,
  author    = {Liu, Xuejing and Tang, Wei and Lu, Jinghui and Zhao, Rui and Guo, Zhaojun and Tan, Fei},
  booktitle = {Findings of the Association for Computational Linguistics: ACL 2023},
  title     = {Deeply Coupled Cross-Modal Prompt Learning},
  year      = {2023},
  address   = {Toronto, Canada},
  editor    = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
  month     = jul,
  pages     = {7957--7970},
  publisher = {Association for Computational Linguistics},
  abstract  = {Recent advancements in multimodal foundation models (e.g., CLIP) have excelled in zero-shot generalization. Prompt tuning involved in the knowledge transfer from foundation models to downstream tasks has gained significant attention recently. Existing prompt-tuning methods in cross-modal learning, however, either solely focus on language branch, or learn vision-language interaction in a shallow mechanism. In this context, we propose a Deeply coupled Cross-modal Prompt learning (DCP) method based on CLIP. DCP flexibly accommodates the interplay between vision and language with a Cross-Modal Prompt Attention (CMPA) mechanism, which enables the mutual exchange of respective representation through a well-connected multi-head attention progressively and strongly. We then conduct comprehensive few-shot learning experiments on 11 image classification datasets and analyze the robustness to domain shift as well. Thorough experimental analysis evidently demonstrates the superb few-shot generalization and compelling domain adaption capacity of a well-executed DCP.},
  doi       = {10.18653/v1/2023.findings-acl.504},
  file      = {:Liu2023c - Deeply Coupled Cross Modal Prompt Learning.pdf:PDF},
  url       = {https://aclanthology.org/2023.findings-acl.504},
}

@Misc{Pahde2018,
  author        = {Frederik Pahde and Patrick Jähnichen and Tassilo Klein and Moin Nabi},
  title         = {Cross-modal Hallucination for Few-shot Fine-grained Recognition},
  year          = {2018},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.1806.05147},
  eprint        = {1806.05147},
  eprinttype    = {arxiv},
  file          = {:Pahde2018 - Cross Modal Hallucination for Few Shot Fine Grained Recognition.pdf:PDF:https\://arxiv.org/pdf/1806.05147.pdf},
  primaryclass  = {cs.CV},
}

@InProceedings{Zhao2020,
  author    = {Zhao, Jiabao and Lin, Xin and Zhou, Jie and Yang, Jing and He, Liang and Yang, Zhaohui},
  booktitle = {2020 IEEE International Conference on Multimedia and Expo (ICME)},
  title     = {Knowledge-Based Fine-Grained Classification For Few-Shot Learning},
  year      = {2020},
  pages     = {1-6},
  doi       = {10.1109/ICME46284.2020.9102809},
  file      = {:Zhao2020 - Knowledge Based Fine Grained Classification for Few Shot Learning.pdf:PDF},
  keywords  = {Feature extraction;Mirrors;Semantics;Visualization;Task analysis;Knowledge engineering;Knowledge based systems;Few-shot learning;External knowledge;Fine-grained;Multimodal},
}

@InProceedings{Pahde2021,
  author    = {Pahde, Frederik and Puscas, Mihai and Klein, Tassilo and Nabi, Moin},
  booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
  title     = {Multimodal Prototypical Networks for Few-Shot Learning},
  year      = {2021},
  month     = {January},
  pages     = {2644-2653},
  file      = {:Pahde2021 - Multimodal Prototypical Networks for Few Shot Learning.pdf:PDF;:Pahde_Multimodal_Prototypical_Networks_WACV_2021_supplemental.pdf:PDF},
}

@InProceedings{Wanyan2023,
  author    = {Wanyan, Yuyang and Yang, Xiaoshan and Chen, Chaofan and Xu, Changsheng},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Active Exploration of Multimodal Complementarity for Few-Shot Action Recognition},
  year      = {2023},
  month     = {June},
  pages     = {6492-6502},
  file      = {:Wanyan2023 - Active Exploration of Multimodal Complementarity for Few Shot Action Recognition.pdf:PDF},
}

@InProceedings{Zhang2024a,
  author    = {Zhang, Xiaohui and Yoon, Jaehong and Bansal, Mohit and Yao, Huaxiu},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Multimodal Representation Learning by Alternating Unimodal Adaptation},
  year      = {2024},
  month     = {June},
  pages     = {27456-27466},
  file      = {:Zhang2024a - Multimodal Representation Learning by Alternating Unimodal Adaptation.pdf:PDF;:Zhang_Multimodal_Representation_Learning_CVPR_2024_supplemental.pdf:PDF},
}

@InProceedings{Gemmeke2017,
  author    = {Gemmeke, Jort F. and Ellis, Daniel P. W. and Freedman, Dylan and Jansen, Aren and Lawrence, Wade and Moore, R. Channing and Plakal, Manoj and Ritter, Marvin},
  booktitle = {2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  title     = {Audio Set: An ontology and human-labeled dataset for audio events},
  year      = {2017},
  pages     = {776-780},
  doi       = {10.1109/ICASSP.2017.7952261},
  file      = {:Gemmeke2017 - Audio Set_ an Ontology and Human Labeled Dataset for Audio Events.pdf:PDF},
  groups    = {Datasets},
  keywords  = {Ontologies;Birds;Music;Taxonomy;Labeling;Audio event detection;sound ontology;audio databases;data collection},
}

@Article{Sun2023,
  author        = {Luoyi Sun and Xuenan Xu and Mengyue Wu and Weidi Xie},
  journal       = {arXiv preprint},
  title         = {{A Large-scale Dataset for Audio-Language Representation Learning}},
  year          = {2023},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2309.11500},
  eprint        = {2309.11500},
  eprinttype    = {arxiv},
  file          = {:Sun2023 - A Large Scale Dataset for Audio Language Representation Learning.pdf:PDF:https\://arxiv.org/pdf/2309.11500.pdf},
  primaryclass  = {cs.SD},
}

@Misc{Kwon2023,
  author        = {Gukyeong Kwon and Zhaowei Cai and Avinash Ravichandran and Erhan Bas and Rahul Bhotika and Stefano Soatto},
  title         = {Masked Vision and Language Modeling for Multi-modal Representation Learning},
  year          = {2023},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2208.02131},
  eprint        = {2208.02131},
  eprinttype    = {arxiv},
  file          = {:Kwon2023 - Masked Vision and Language Modeling for Multi Modal Representation Learning.pdf:PDF:http\://arxiv.org/pdf/2208.02131},
  primaryclass  = {cs.CV},
}

@InProceedings{Rombach2022,
  author    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\"orn},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {High-Resolution Image Synthesis With Latent Diffusion Models},
  year      = {2022},
  month     = {June},
  pages     = {10684-10695},
  file      = {:Rombach2022 - High Resolution Image Synthesis with Latent Diffusion Models.pdf:PDF;:Rombach_High-Resolution_Image_Synthesis_CVPR_2022_supplemental.pdf:PDF},
}

@Article{Yu2023,
  author  = {Yu, Lili and Shi, Bowen and Pasunuru, Ramakanth and Muller, Benjamin and Golovneva, Olga and Wang, Tianlu and Babu, Arun and Tang, Binh and Karrer, Brian and Sheynin, Shelly and others},
  journal = {arXiv preprint arXiv:2309.02591},
  title   = {Scaling autoregressive multi-modal models: Pretraining and instruction tuning},
  year    = {2023},
  number  = {3},
  volume  = {2},
  file    = {:Yu2023 - Scaling Autoregressive Multi Modal Models_ Pretraining and Instruction Tuning.pdf:PDF},
}

@InProceedings{Oord2017,
  author    = {van den Oord, Aaron and Vinyals, Oriol and kavukcuoglu, koray},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Neural Discrete Representation Learning},
  year      = {2017},
  editor    = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  publisher = {Curran Associates, Inc.},
  volume    = {30},
  file      = {:Oord2017 - Neural Discrete Representation Learning.pdf:PDF},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2017/file/7a98af17e63a0ac09ce2e96d03992fbc-Paper.pdf},
}

@InProceedings{Sorscher2022,
  author    = {Sorscher, Ben and Geirhos, Robert and Shekhar, Shashank and Ganguli, Surya and Morcos, Ari},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Beyond neural scaling laws: beating power law scaling via data pruning},
  year      = {2022},
  editor    = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
  pages     = {19523--19536},
  publisher = {Curran Associates, Inc.},
  volume    = {35},
  file      = {:Sorscher2022 - Beyond Neural Scaling Laws_ Beating Power Law Scaling Via Data Pruning.pdf:PDF},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2022/file/7b75da9b61eda40fa35453ee5d077df6-Paper-Conference.pdf},
}

@Misc{Gao2023,
  author        = {Peng Gao and Jiaming Han and Renrui Zhang and Ziyi Lin and Shijie Geng and Aojun Zhou and Wei Zhang and Pan Lu and Conghui He and Xiangyu Yue and Hongsheng Li and Yu Qiao},
  title         = {LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model},
  year          = {2023},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2304.15010},
  eprint        = {2304.15010},
  eprinttype    = {arxiv},
  file          = {:Gao2023 - LLaMA Adapter V2_ Parameter Efficient Visual Instruction Model.pdf:PDF:https\://arxiv.org/pdf/2304.15010.pdf},
  primaryclass  = {cs.CV},
}

@InProceedings{Sung2022,
  author    = {Sung, Yi-Lin and Cho, Jaemin and Bansal, Mohit},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks},
  year      = {2022},
  month     = {June},
  pages     = {5227-5237},
  file      = {:Sung2022 - VL Adapter_ Parameter Efficient Transfer Learning for Vision and Language Tasks.pdf:PDF;:Sung_VL-Adapter_Parameter-Efficient_Transfer_CVPR_2022_supplemental.pdf:PDF},
}

@InProceedings{Thrush2022,
  author    = {Thrush, Tristan and Jiang, Ryan and Bartolo, Max and Singh, Amanpreet and Williams, Adina and Kiela, Douwe and Ross, Candace},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Winoground: Probing Vision and Language Models for Visio-Linguistic Compositionality},
  year      = {2022},
  month     = {June},
  pages     = {5238-5248},
  file      = {:Thrush2022 - Winoground_ Probing Vision and Language Models for Visio Linguistic Compositionality.pdf:PDF;:Thrush_Winoground_Probing_Vision_CVPR_2022_supplemental.pdf:PDF},
  groups    = {Datasets},
}

@InProceedings{Bordes2023,
  author    = {Bordes, Florian and Shekhar, Shashank and Ibrahim, Mark and Bouchacourt, Diane and Vincent, Pascal and Morcos, Ari},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning},
  year      = {2023},
  editor    = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
  pages     = {45020--45054},
  publisher = {Curran Associates, Inc.},
  volume    = {36},
  file      = {:Bordes2023 - PUG_ Photorealistic and Semantically Controllable Synthetic Data for Representation Learning.pdf:PDF},
  groups    = {Datasets},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2023/file/8d352fd0f07fde4a74f9476603b3773b-Paper-Datasets_and_Benchmarks.pdf},
}

@Misc{Albuquerque2020,
  author        = {Isabela Albuquerque and Nikhil Naik and Junnan Li and Nitish Keskar and Richard Socher},
  title         = {Improving out-of-distribution generalization via multi-task self-supervised pretraining},
  year          = {2020},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2003.13525},
  eprint        = {2003.13525},
  eprinttype    = {arxiv},
  file          = {:Albuquerque2020 - Improving Out of Distribution Generalization Via Multi Task Self Supervised Pretraining.pdf:PDF:http\://arxiv.org/pdf/2003.13525v1},
  primaryclass  = {cs.CV},
}

@InProceedings{Jin2022,
  author       = {Jin, Woojeong and Cheng, Yu and Shen, Yelong and Chen, Weizhu and Ren, Xiang},
  booktitle    = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  title        = {{A Good Prompt Is Worth Millions of Parameters: Low-resource Prompt-based Learning for Vision-Language Models}},
  year         = {2022},
  address      = {Dublin, Ireland},
  editor       = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
  month        = may,
  pages        = {2763--2775},
  publisher    = {Association for Computational Linguistics},
  abstract     = {Large pre-trained vision-language (VL) models can learn a new task with a handful of examples and generalize to a new task without fine-tuning. However, these VL models are hard to deploy for real-world applications due to their impractically huge sizes and slow inference speed. To solve this limitation, we study prompt-based low-resource learning of VL tasks with our proposed method, FewVLM, relatively smaller than recent few-shot learners. For FewVLM, we pre-train a sequence-to-sequence transformer model with prefix language modeling (PrefixLM) and masked language modeling (MaskedLM).Furthermore, we analyze the effect of diverse prompts for few-shot tasks. Experimental results on VQA show that FewVLM with prompt-based learning outperforms Frozen which is 31x larger than FewVLM by 18.2{\%} point and achieves comparable results to a 246x larger model, PICa.In our analysis, we observe that (1) prompts significantly affect zero-shot performance but marginally affect few-shot performance, (2) models with noisy prompts learn as quickly as hand-crafted prompts given larger training data, and (3) MaskedLM helps VQA tasks while PrefixLM boosts captioning performance. Our code is publicly available at \url{https://github.com/woojeongjin/FewVLM}},
  doi          = {10.18653/v1/2022.acl-long.197},
  file         = {:Jin2022 - A Good Prompt Is Worth Millions of Parameters_ Low Resource Prompt Based Learning for Vision Language Models.pdf:PDF:https\://aclanthology.org/2022.acl-long.197.pdf},
  primaryclass = {cs.CV},
  priority     = {prio2},
  url          = {https://aclanthology.org/2022.acl-long.197},
}

@Misc{Tan2024a,
  author        = {Sijun Tan and Xiuyu Li and Shishir Patil and Ziyang Wu and Tianjun Zhang and Kurt Keutzer and Joseph E. Gonzalez and Raluca Ada Popa},
  title         = {LLoCO: Learning Long Contexts Offline},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2404.07979},
  eprint        = {2404.07979},
  eprinttype    = {arxiv},
  file          = {:Tan2024a - LLoCO_ Learning Long Contexts Offline.pdf:PDF:https\://arxiv.org/pdf/2404.07979.pdf},
  primaryclass  = {cs.CL},
}

@InProceedings{Cho2021,
  author    = {Cho, Jaemin and Lei, Jie and Tan, Hao and Bansal, Mohit},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  title     = {Unifying {V}ision-and-{L}anguage {T}asks via {T}ext {G}eneration},
  year      = {2021},
  editor    = {Meila, Marina and Zhang, Tong},
  month     = jul,
  pages     = {1931--1942},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {139},
  abstract  = {Existing methods for vision-and-language learning typically require designing task-specific architectures and objectives for each task. For example, a multi-label answer classifier for visual question answering, a region scorer for referring expression comprehension, and a language decoder for image captioning, etc. To alleviate these hassles, in this work, we propose a unified framework that learns different tasks in a single architecture with the same language modeling objective, i.e., multimodal conditional text generation, where our models learn to generate labels in text based on the visual and textual inputs. On 7 popular vision-and-language benchmarks, including visual question answering, referring expression comprehension, visual commonsense reasoning, most of which have been previously modeled as discriminative tasks, our generative approach (with a single unified architecture) reaches comparable performance to recent task-specific state-of-the-art vision-and-language models. Moreover, our generative approach shows better generalization ability on questions that have rare answers. Also, we show that our framework allows multi-task learning in a single architecture with a single set of parameters, achieving similar performance to separately optimized single-task models. Our code is publicly available at: https://github.com/j-min/VL-T5},
  file      = {:Cho2021 - Unifying Vision and Language Tasks Via Text Generation.pdf:PDF;:cho21a-supp.pdf:PDF},
  pdf       = {http://proceedings.mlr.press/v139/cho21a/cho21a.pdf},
  priority  = {prio1},
  url       = {https://proceedings.mlr.press/v139/cho21a.html},
}

@InProceedings{Li2022,
  author    = {Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},
  booktitle = {Proceedings of the 39th International Conference on Machine Learning},
  title     = {{BLIP}: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation},
  year      = {2022},
  editor    = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  month     = {17--23 Jul},
  pages     = {12888--12900},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {162},
  abstract  = {Vision-Language Pre-training (VLP) has advanced the performance for many vision-language tasks. However, most existing pre-trained models only excel in either understanding-based tasks or generation-based tasks. Furthermore, performance improvement has been largely achieved by scaling up the dataset with noisy image-text pairs collected from the web, which is a suboptimal source of supervision. In this paper, we propose BLIP, a new VLP framework which transfers flexibly to both vision-language understanding and generation tasks. BLIP effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. We achieve state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval (+2.7% in average recall@1), image captioning (+2.8% in CIDEr), and VQA (+1.6% in VQA score). BLIP also demonstrates strong generalization ability when directly transferred to video-language tasks in a zero-shot manner. Code and models are available at https://github.com/salesforce/BLIP.},
  file      = {:Li2022 - BLIP_ Bootstrapping Language Image Pre Training for Unified Vision Language Understanding and Generation.pdf:PDF},
  groups    = {Models},
  pdf       = {https://proceedings.mlr.press/v162/li22n/li22n.pdf},
  url       = {https://proceedings.mlr.press/v162/li22n.html},
}

@InProceedings{Tong2024,
  author        = {Shengbang Tong and Zhuang Liu and Yuexiang Zhai and Yi Ma and Yann LeCun and Saining Xie},
  booktitle     = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title         = {{Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs}},
  year          = {2024},
  month         = {June},
  pages         = {9568-9578},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2401.06209},
  eprint        = {2401.06209},
  eprinttype    = {arxiv},
  file          = {:Tong2024 - Eyes Wide Shut_ Exploring the Visual Shortcomings of Multimodal LLMs.pdf:PDF:http\://arxiv.org/pdf/2401.06209v2},
  primaryclass  = {cs.CV},
  priority      = {prio1},
}

@Misc{Oquab2024,
  author        = {Maxime Oquab and Timothée Darcet and Théo Moutakanni and Huy Vo and Marc Szafraniec and Vasil Khalidov and Pierre Fernandez and Daniel Haziza and Francisco Massa and Alaaeldin El-Nouby and Mahmoud Assran and Nicolas Ballas and Wojciech Galuba and Russell Howes and Po-Yao Huang and Shang-Wen Li and Ishan Misra and Michael Rabbat and Vasu Sharma and Gabriel Synnaeve and Hu Xu and Hervé Jegou and Julien Mairal and Patrick Labatut and Armand Joulin and Piotr Bojanowski},
  title         = {DINOv2: Learning Robust Visual Features without Supervision},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2304.07193},
  eprint        = {2304.07193},
  eprinttype    = {arxiv},
  file          = {:Oquab2024 - DINOv2_ Learning Robust Visual Features without Supervision.pdf:PDF:http\://arxiv.org/pdf/2304.07193v2},
  groups        = {Models},
  primaryclass  = {cs.CV},
}

@Misc{Team2024,
  author        = {Gemini Team and Rohan Anil and Sebastian Borgeaud and Jean-Baptiste Alayrac and Jiahui Yu and Radu Soricut and Johan Schalkwyk and Andrew M. Dai and Anja Hauth and Katie Millican and David Silver and Melvin Johnson and Ioannis Antonoglou and Julian Schrittwieser and Amelia Glaese and Jilin Chen and Emily Pitler and Timothy Lillicrap and Angeliki Lazaridou and Orhan Firat and James Molloy and Michael Isard and Paul R. Barham and Tom Hennigan and Benjamin Lee and Fabio Viola and Malcolm Reynolds and Yuanzhong Xu and Ryan Doherty and Eli Collins and Clemens Meyer and Eliza Rutherford and Erica Moreira and Kareem Ayoub and Megha Goel and Jack Krawczyk and Cosmo Du and Ed Chi and Heng-Tze Cheng and Eric Ni and Purvi Shah and Patrick Kane and Betty Chan and Manaal Faruqui and Aliaksei Severyn and Hanzhao Lin and YaGuang Li and Yong Cheng and Abe Ittycheriah and Mahdis Mahdieh and Mia Chen and Pei Sun and Dustin Tran and Sumit Bagri and Balaji Lakshminarayanan and Jeremiah Liu and Andras Orban and Fabian Güra and Hao Zhou and Xinying Song and Aurelien Boffy and Harish Ganapathy and Steven Zheng and HyunJeong Choe and Ágoston Weisz and Tao Zhu and Yifeng Lu and Siddharth Gopal and Jarrod Kahn and Maciej Kula and Jeff Pitman and Rushin Shah and Emanuel Taropa and Majd Al Merey and Martin Baeuml and Zhifeng Chen and Laurent El Shafey and Yujing Zhang and Olcan Sercinoglu and George Tucker and Enrique Piqueras and Maxim Krikun and Iain Barr and Nikolay Savinov and Ivo Danihelka and Becca Roelofs and Anaïs White and Anders Andreassen and Tamara von Glehn and Lakshman Yagati and Mehran Kazemi and Lucas Gonzalez and Misha Khalman and Jakub Sygnowski and Alexandre Frechette and Charlotte Smith and Laura Culp and Lev Proleev and Yi Luan and Xi Chen and James Lottes and Nathan Schucher and Federico Lebron and Alban Rrustemi and Natalie Clay and Phil Crone and Tomas Kocisky and Jeffrey Zhao and Bartek Perz and Dian Yu and Heidi Howard and Adam Bloniarz and Jack W. Rae and Han Lu and Laurent Sifre and Marcello Maggioni and Fred Alcober and Dan Garrette and Megan Barnes and Shantanu Thakoor and Jacob Austin and Gabriel Barth-Maron and William Wong and Rishabh Joshi and Rahma Chaabouni and Deeni Fatiha and Arun Ahuja and Gaurav Singh Tomar and Evan Senter and Martin Chadwick and Ilya Kornakov and Nithya Attaluri and Iñaki Iturrate and Ruibo Liu and Yunxuan Li and Sarah Cogan and Jeremy Chen and Chao Jia and Chenjie Gu and Qiao Zhang and Jordan Grimstad and Ale Jakse Hartman and Xavier Garcia and Thanumalayan Sankaranarayana Pillai and Jacob Devlin and Michael Laskin and Diego de Las Casas and Dasha Valter and Connie Tao and Lorenzo Blanco and Adrià Puigdomènech Badia and David Reitter and Mianna Chen and Jenny Brennan and Clara Rivera and Sergey Brin and Shariq Iqbal and Gabriela Surita and Jane Labanowski and Abhi Rao and Stephanie Winkler and Emilio Parisotto and Yiming Gu and Kate Olszewska and Ravi Addanki and Antoine Miech and Annie Louis and Denis Teplyashin and Geoff Brown and Elliot Catt and Jan Balaguer and Jackie Xiang and Pidong Wang and Zoe Ashwood and Anton Briukhov and Albert Webson and Sanjay Ganapathy and Smit Sanghavi and Ajay Kannan and Ming-Wei Chang and Axel Stjerngren and Josip Djolonga and Yuting Sun and Ankur Bapna and Matthew Aitchison and Pedram Pejman and Henryk Michalewski and Tianhe Yu and Cindy Wang and Juliette Love and Junwhan Ahn and Dawn Bloxwich and Kehang Han and Peter Humphreys and Thibault Sellam and James Bradbury and Varun Godbole and Sina Samangooei and Bogdan Damoc and Alex Kaskasoli and Sébastien M. R. Arnold and Vijay Vasudevan and Shubham Agrawal and Jason Riesa and Dmitry Lepikhin and Richard Tanburn and Srivatsan Srinivasan and Hyeontaek Lim and Sarah Hodkinson and Pranav Shyam and Johan Ferret and Steven Hand and Ankush Garg and Tom Le Paine and Jian Li and Yujia Li and Minh Giang and Alexander Neitz and Zaheer Abbas and Sarah York and Machel Reid and Elizabeth Cole and Aakanksha Chowdhery and Dipanjan Das and Dominika Rogozińska and Vitaliy Nikolaev and Pablo Sprechmann and Zachary Nado and Lukas Zilka and Flavien Prost and Luheng He and Marianne Monteiro and Gaurav Mishra and Chris Welty and Josh Newlan and Dawei Jia and Miltiadis Allamanis and Clara Huiyi Hu and Raoul de Liedekerke and Justin Gilmer and Carl Saroufim and Shruti Rijhwani and Shaobo Hou and Disha Shrivastava and Anirudh Baddepudi and Alex Goldin and Adnan Ozturel and Albin Cassirer and Yunhan Xu and Daniel Sohn and Devendra Sachan and Reinald Kim Amplayo and Craig Swanson and Dessie Petrova and Shashi Narayan and Arthur Guez and Siddhartha Brahma and Jessica Landon and Miteyan Patel and Ruizhe Zhao and Kevin Villela and Luyu Wang and Wenhao Jia and Matthew Rahtz and Mai Giménez and Legg Yeung and James Keeling and Petko Georgiev and Diana Mincu and Boxi Wu and Salem Haykal and Rachel Saputro and Kiran Vodrahalli and James Qin and Zeynep Cankara and Abhanshu Sharma and Nick Fernando and Will Hawkins and Behnam Neyshabur and Solomon Kim and Adrian Hutter and Priyanka Agrawal and Alex Castro-Ros and George van den Driessche and Tao Wang and Fan Yang and Shuo-yiin Chang and Paul Komarek and Ross McIlroy and Mario Lučić and Guodong Zhang and Wael Farhan and Michael Sharman and Paul Natsev and Paul Michel and Yamini Bansal and Siyuan Qiao and Kris Cao and Siamak Shakeri and Christina Butterfield and Justin Chung and Paul Kishan Rubenstein and Shivani Agrawal and Arthur Mensch and Kedar Soparkar and Karel Lenc and Timothy Chung and Aedan Pope and Loren Maggiore and Jackie Kay and Priya Jhakra and Shibo Wang and Joshua Maynez and Mary Phuong and Taylor Tobin and Andrea Tacchetti and Maja Trebacz and Kevin Robinson and Yash Katariya and Sebastian Riedel and Paige Bailey and Kefan Xiao and Nimesh Ghelani and Lora Aroyo and Ambrose Slone and Neil Houlsby and Xuehan Xiong and Zhen Yang and Elena Gribovskaya and Jonas Adler and Mateo Wirth and Lisa Lee and Music Li and Thais Kagohara and Jay Pavagadhi and Sophie Bridgers and Anna Bortsova and Sanjay Ghemawat and Zafarali Ahmed and Tianqi Liu and Richard Powell and Vijay Bolina and Mariko Iinuma and Polina Zablotskaia and James Besley and Da-Woon Chung and Timothy Dozat and Ramona Comanescu and Xiance Si and Jeremy Greer and Guolong Su and Martin Polacek and Raphaël Lopez Kaufman and Simon Tokumine and Hexiang Hu and Elena Buchatskaya and Yingjie Miao and Mohamed Elhawaty and Aditya Siddhant and Nenad Tomasev and Jinwei Xing and Christina Greer and Helen Miller and Shereen Ashraf and Aurko Roy and Zizhao Zhang and Ada Ma and Angelos Filos and Milos Besta and Rory Blevins and Ted Klimenko and Chih-Kuan Yeh and Soravit Changpinyo and Jiaqi Mu and Oscar Chang and Mantas Pajarskas and Carrie Muir and Vered Cohen and Charline Le Lan and Krishna Haridasan and Amit Marathe and Steven Hansen and Sholto Douglas and Rajkumar Samuel and Mingqiu Wang and Sophia Austin and Chang Lan and Jiepu Jiang and Justin Chiu and Jaime Alonso Lorenzo and Lars Lowe Sjösund and Sébastien Cevey and Zach Gleicher and Thi Avrahami and Anudhyan Boral and Hansa Srinivasan and Vittorio Selo and Rhys May and Konstantinos Aisopos and Léonard Hussenot and Livio Baldini Soares and Kate Baumli and Michael B. Chang and Adrià Recasens and Ben Caine and Alexander Pritzel and Filip Pavetic and Fabio Pardo and Anita Gergely and Justin Frye and Vinay Ramasesh and Dan Horgan and Kartikeya Badola and Nora Kassner and Subhrajit Roy and Ethan Dyer and Víctor Campos Campos and Alex Tomala and Yunhao Tang and Dalia El Badawy and Elspeth White and Basil Mustafa and Oran Lang and Abhishek Jindal and Sharad Vikram and Zhitao Gong and Sergi Caelles and Ross Hemsley and Gregory Thornton and Fangxiaoyu Feng and Wojciech Stokowiec and Ce Zheng and Phoebe Thacker and Çağlar Ünlü and Zhishuai Zhang and Mohammad Saleh and James Svensson and Max Bileschi and Piyush Patil and Ankesh Anand and Roman Ring and Katerina Tsihlas and Arpi Vezer and Marco Selvi and Toby Shevlane and Mikel Rodriguez and Tom Kwiatkowski and Samira Daruki and Keran Rong and Allan Dafoe and Nicholas FitzGerald and Keren Gu-Lemberg and Mina Khan and Lisa Anne Hendricks and Marie Pellat and Vladimir Feinberg and James Cobon-Kerr and Tara Sainath and Maribeth Rauh and Sayed Hadi Hashemi and Richard Ives and Yana Hasson and Eric Noland and Yuan Cao and Nathan Byrd and Le Hou and Qingze Wang and Thibault Sottiaux and Michela Paganini and Jean-Baptiste Lespiau and Alexandre Moufarek and Samer Hassan and Kaushik Shivakumar and Joost van Amersfoort and Amol Mandhane and Pratik Joshi and Anirudh Goyal and Matthew Tung and Andrew Brock and Hannah Sheahan and Vedant Misra and Cheng Li and Nemanja Rakićević and Mostafa Dehghani and Fangyu Liu and Sid Mittal and Junhyuk Oh and Seb Noury and Eren Sezener and Fantine Huot and Matthew Lamm and Nicola De Cao and Charlie Chen and Sidharth Mudgal and Romina Stella and Kevin Brooks and Gautam Vasudevan and Chenxi Liu and Mainak Chain and Nivedita Melinkeri and Aaron Cohen and Venus Wang and Kristie Seymore and Sergey Zubkov and Rahul Goel and Summer Yue and Sai Krishnakumaran and Brian Albert and Nate Hurley and Motoki Sano and Anhad Mohananey and Jonah Joughin and Egor Filonov and Tomasz Kępa and Yomna Eldawy and Jiawern Lim and Rahul Rishi and Shirin Badiezadegan and Taylor Bos and Jerry Chang and Sanil Jain and Sri Gayatri Sundara Padmanabhan and Subha Puttagunta and Kalpesh Krishna and Leslie Baker and Norbert Kalb and Vamsi Bedapudi and Adam Kurzrok and Shuntong Lei and Anthony Yu and Oren Litvin and Xiang Zhou and Zhichun Wu and Sam Sobell and Andrea Siciliano and Alan Papir and Robby Neale and Jonas Bragagnolo and Tej Toor and Tina Chen and Valentin Anklin and Feiran Wang and Richie Feng and Milad Gholami and Kevin Ling and Lijuan Liu and Jules Walter and Hamid Moghaddam and Arun Kishore and Jakub Adamek and Tyler Mercado and Jonathan Mallinson and Siddhinita Wandekar and Stephen Cagle and Eran Ofek and Guillermo Garrido and Clemens Lombriser and Maksim Mukha and Botu Sun and Hafeezul Rahman Mohammad and Josip Matak and Yadi Qian and Vikas Peswani and Pawel Janus and Quan Yuan and Leif Schelin and Oana David and Ankur Garg and Yifan He and Oleksii Duzhyi and Anton Älgmyr and Timothée Lottaz and Qi Li and Vikas Yadav and Luyao Xu and Alex Chinien and Rakesh Shivanna and Aleksandr Chuklin and Josie Li and Carrie Spadine and Travis Wolfe and Kareem Mohamed and Subhabrata Das and Zihang Dai and Kyle He and Daniel von Dincklage and Shyam Upadhyay and Akanksha Maurya and Luyan Chi and Sebastian Krause and Khalid Salama and Pam G Rabinovitch and Pavan Kumar Reddy M and Aarush Selvan and Mikhail Dektiarev and Golnaz Ghiasi and Erdem Guven and Himanshu Gupta and Boyi Liu and Deepak Sharma and Idan Heimlich Shtacher and Shachi Paul and Oscar Akerlund and François-Xavier Aubet and Terry Huang and Chen Zhu and Eric Zhu and Elico Teixeira and Matthew Fritze and Francesco Bertolini and Liana-Eleonora Marinescu and Martin Bölle and Dominik Paulus and Khyatti Gupta and Tejasi Latkar and Max Chang and Jason Sanders and Roopa Wilson and Xuewei Wu and Yi-Xuan Tan and Lam Nguyen Thiet and Tulsee Doshi and Sid Lall and Swaroop Mishra and Wanming Chen and Thang Luong and Seth Benjamin and Jasmine Lee and Ewa Andrejczuk and Dominik Rabiej and Vipul Ranjan and Krzysztof Styrc and Pengcheng Yin and Jon Simon and Malcolm Rose Harriott and Mudit Bansal and Alexei Robsky and Geoff Bacon and David Greene and Daniil Mirylenka and Chen Zhou and Obaid Sarvana and Abhimanyu Goyal and Samuel Andermatt and Patrick Siegler and Ben Horn and Assaf Israel and Francesco Pongetti and Chih-Wei "Louis" Chen and Marco Selvatici and Pedro Silva and Kathie Wang and Jackson Tolins and Kelvin Guu and Roey Yogev and Xiaochen Cai and Alessandro Agostini and Maulik Shah and Hung Nguyen and Noah Ó Donnaile and Sébastien Pereira and Linda Friso and Adam Stambler and Adam Kurzrok and Chenkai Kuang and Yan Romanikhin and Mark Geller and ZJ Yan and Kane Jang and Cheng-Chun Lee and Wojciech Fica and Eric Malmi and Qijun Tan and Dan Banica and Daniel Balle and Ryan Pham and Yanping Huang and Diana Avram and Hongzhi Shi and Jasjot Singh and Chris Hidey and Niharika Ahuja and Pranab Saxena and Dan Dooley and Srividya Pranavi Potharaju and Eileen O'Neill and Anand Gokulchandran and Ryan Foley and Kai Zhao and Mike Dusenberry and Yuan Liu and Pulkit Mehta and Ragha Kotikalapudi and Chalence Safranek-Shrader and Andrew Goodman and Joshua Kessinger and Eran Globen and Prateek Kolhar and Chris Gorgolewski and Ali Ibrahim and Yang Song and Ali Eichenbaum and Thomas Brovelli and Sahitya Potluri and Preethi Lahoti and Cip Baetu and Ali Ghorbani and Charles Chen and Andy Crawford and Shalini Pal and Mukund Sridhar and Petru Gurita and Asier Mujika and Igor Petrovski and Pierre-Louis Cedoz and Chenmei Li and Shiyuan Chen and Niccolò Dal Santo and Siddharth Goyal and Jitesh Punjabi and Karthik Kappaganthu and Chester Kwak and Pallavi LV and Sarmishta Velury and Himadri Choudhury and Jamie Hall and Premal Shah and Ricardo Figueira and Matt Thomas and Minjie Lu and Ting Zhou and Chintu Kumar and Thomas Jurdi and Sharat Chikkerur and Yenai Ma and Adams Yu and Soo Kwak and Victor Ähdel and Sujeevan Rajayogam and Travis Choma and Fei Liu and Aditya Barua and Colin Ji and Ji Ho Park and Vincent Hellendoorn and Alex Bailey and Taylan Bilal and Huanjie Zhou and Mehrdad Khatir and Charles Sutton and Wojciech Rzadkowski and Fiona Macintosh and Konstantin Shagin and Paul Medina and Chen Liang and Jinjing Zhou and Pararth Shah and Yingying Bi and Attila Dankovics and Shipra Banga and Sabine Lehmann and Marissa Bredesen and Zifan Lin and John Eric Hoffmann and Jonathan Lai and Raynald Chung and Kai Yang and Nihal Balani and Arthur Bražinskas and Andrei Sozanschi and Matthew Hayes and Héctor Fernández Alcalde and Peter Makarov and Will Chen and Antonio Stella and Liselotte Snijders and Michael Mandl and Ante Kärrman and Paweł Nowak and Xinyi Wu and Alex Dyck and Krishnan Vaidyanathan and Raghavender R and Jessica Mallet and Mitch Rudominer and Eric Johnston and Sushil Mittal and Akhil Udathu and Janara Christensen and Vishal Verma and Zach Irving and Andreas Santucci and Gamaleldin Elsayed and Elnaz Davoodi and Marin Georgiev and Ian Tenney and Nan Hua and Geoffrey Cideron and Edouard Leurent and Mahmoud Alnahlawi and Ionut Georgescu and Nan Wei and Ivy Zheng and Dylan Scandinaro and Heinrich Jiang and Jasper Snoek and Mukund Sundararajan and Xuezhi Wang and Zack Ontiveros and Itay Karo and Jeremy Cole and Vinu Rajashekhar and Lara Tumeh and Eyal Ben-David and Rishub Jain and Jonathan Uesato and Romina Datta and Oskar Bunyan and Shimu Wu and John Zhang and Piotr Stanczyk and Ye Zhang and David Steiner and Subhajit Naskar and Michael Azzam and Matthew Johnson and Adam Paszke and Chung-Cheng Chiu and Jaume Sanchez Elias and Afroz Mohiuddin and Faizan Muhammad and Jin Miao and Andrew Lee and Nino Vieillard and Jane Park and Jiageng Zhang and Jeff Stanway and Drew Garmon and Abhijit Karmarkar and Zhe Dong and Jong Lee and Aviral Kumar and Luowei Zhou and Jonathan Evens and William Isaac and Geoffrey Irving and Edward Loper and Michael Fink and Isha Arkatkar and Nanxin Chen and Izhak Shafran and Ivan Petrychenko and Zhe Chen and Johnson Jia and Anselm Levskaya and Zhenkai Zhu and Peter Grabowski and Yu Mao and Alberto Magni and Kaisheng Yao and Javier Snaider and Norman Casagrande and Evan Palmer and Paul Suganthan and Alfonso Castaño and Irene Giannoumis and Wooyeol Kim and Mikołaj Rybiński and Ashwin Sreevatsa and Jennifer Prendki and David Soergel and Adrian Goedeckemeyer and Willi Gierke and Mohsen Jafari and Meenu Gaba and Jeremy Wiesner and Diana Gage Wright and Yawen Wei and Harsha Vashisht and Yana Kulizhskaya and Jay Hoover and Maigo Le and Lu Li and Chimezie Iwuanyanwu and Lu Liu and Kevin Ramirez and Andrey Khorlin and Albert Cui and Tian LIN and Marcus Wu and Ricardo Aguilar and Keith Pallo and Abhishek Chakladar and Ginger Perng and Elena Allica Abellan and Mingyang Zhang and Ishita Dasgupta and Nate Kushman and Ivo Penchev and Alena Repina and Xihui Wu and Tom van der Weide and Priya Ponnapalli and Caroline Kaplan and Jiri Simsa and Shuangfeng Li and Olivier Dousse and Fan Yang and Jeff Piper and Nathan Ie and Rama Pasumarthi and Nathan Lintz and Anitha Vijayakumar and Daniel Andor and Pedro Valenzuela and Minnie Lui and Cosmin Paduraru and Daiyi Peng and Katherine Lee and Shuyuan Zhang and Somer Greene and Duc Dung Nguyen and Paula Kurylowicz and Cassidy Hardin and Lucas Dixon and Lili Janzer and Kiam Choo and Ziqiang Feng and Biao Zhang and Achintya Singhal and Dayou Du and Dan McKinnon and Natasha Antropova and Tolga Bolukbasi and Orgad Keller and David Reid and Daniel Finchelstein and Maria Abi Raad and Remi Crocker and Peter Hawkins and Robert Dadashi and Colin Gaffney and Ken Franko and Anna Bulanova and Rémi Leblond and Shirley Chung and Harry Askham and Luis C. Cobo and Kelvin Xu and Felix Fischer and Jun Xu and Christina Sorokin and Chris Alberti and Chu-Cheng Lin and Colin Evans and Alek Dimitriev and Hannah Forbes and Dylan Banarse and Zora Tung and Mark Omernick and Colton Bishop and Rachel Sterneck and Rohan Jain and Jiawei Xia and Ehsan Amid and Francesco Piccinno and Xingyu Wang and Praseem Banzal and Daniel J. Mankowitz and Alex Polozov and Victoria Krakovna and Sasha Brown and MohammadHossein Bateni and Dennis Duan and Vlad Firoiu and Meghana Thotakuri and Tom Natan and Matthieu Geist and Ser tan Girgin and Hui Li and Jiayu Ye and Ofir Roval and Reiko Tojo and Michael Kwong and James Lee-Thorp and Christopher Yew and Danila Sinopalnikov and Sabela Ramos and John Mellor and Abhishek Sharma and Kathy Wu and David Miller and Nicolas Sonnerat and Denis Vnukov and Rory Greig and Jennifer Beattie and Emily Caveness and Libin Bai and Julian Eisenschlos and Alex Korchemniy and Tomy Tsai and Mimi Jasarevic and Weize Kong and Phuong Dao and Zeyu Zheng and Frederick Liu and Fan Yang and Rui Zhu and Tian Huey Teh and Jason Sanmiya and Evgeny Gladchenko and Nejc Trdin and Daniel Toyama and Evan Rosen and Sasan Tavakkol and Linting Xue and Chen Elkind and Oliver Woodman and John Carpenter and George Papamakarios and Rupert Kemp and Sushant Kafle and Tanya Grunina and Rishika Sinha and Alice Talbert and Diane Wu and Denese Owusu-Afriyie and Cosmo Du and Chloe Thornton and Jordi Pont-Tuset and Pradyumna Narayana and Jing Li and Saaber Fatehi and John Wieting and Omar Ajmeri and Benigno Uria and Yeongil Ko and Laura Knight and Amélie Héliou and Ning Niu and Shane Gu and Chenxi Pang and Yeqing Li and Nir Levine and Ariel Stolovich and Rebeca Santamaria-Fernandez and Sonam Goenka and Wenny Yustalim and Robin Strudel and Ali Elqursh and Charlie Deck and Hyo Lee and Zonglin Li and Kyle Levin and Raphael Hoffmann and Dan Holtmann-Rice and Olivier Bachem and Sho Arora and Christy Koh and Soheil Hassas Yeganeh and Siim Põder and Mukarram Tariq and Yanhua Sun and Lucian Ionita and Mojtaba Seyedhosseini and Pouya Tafti and Zhiyu Liu and Anmol Gulati and Jasmine Liu and Xinyu Ye and Bart Chrzaszcz and Lily Wang and Nikhil Sethi and Tianrun Li and Ben Brown and Shreya Singh and Wei Fan and Aaron Parisi and Joe Stanton and Vinod Koverkathu and Christopher A. Choquette-Choo and Yunjie Li and TJ Lu and Abe Ittycheriah and Prakash Shroff and Mani Varadarajan and Sanaz Bahargam and Rob Willoughby and David Gaddy and Guillaume Desjardins and Marco Cornero and Brona Robenek and Bhavishya Mittal and Ben Albrecht and Ashish Shenoy and Fedor Moiseev and Henrik Jacobsson and Alireza Ghaffarkhah and Morgane Rivière and Alanna Walton and Clément Crepy and Alicia Parrish and Zongwei Zhou and Clement Farabet and Carey Radebaugh and Praveen Srinivasan and Claudia van der Salm and Andreas Fidjeland and Salvatore Scellato and Eri Latorre-Chimoto and Hanna Klimczak-Plucińska and David Bridson and Dario de Cesare and Tom Hudson and Piermaria Mendolicchio and Lexi Walker and Alex Morris and Matthew Mauger and Alexey Guseynov and Alison Reid and Seth Odoom and Lucia Loher and Victor Cotruta and Madhavi Yenugula and Dominik Grewe and Anastasia Petrushkina and Tom Duerig and Antonio Sanchez and Steve Yadlowsky and Amy Shen and Amir Globerson and Lynette Webb and Sahil Dua and Dong Li and Surya Bhupatiraju and Dan Hurt and Haroon Qureshi and Ananth Agarwal and Tomer Shani and Matan Eyal and Anuj Khare and Shreyas Rammohan Belle and Lei Wang and Chetan Tekur and Mihir Sanjay Kale and Jinliang Wei and Ruoxin Sang and Brennan Saeta and Tyler Liechty and Yi Sun and Yao Zhao and Stephan Lee and Pandu Nayak and Doug Fritz and Manish Reddy Vuyyuru and John Aslanides and Nidhi Vyas and Martin Wicke and Xiao Ma and Evgenii Eltyshev and Nina Martin and Hardie Cate and James Manyika and Keyvan Amiri and Yelin Kim and Xi Xiong and Kai Kang and Florian Luisier and Nilesh Tripuraneni and David Madras and Mandy Guo and Austin Waters and Oliver Wang and Joshua Ainslie and Jason Baldridge and Han Zhang and Garima Pruthi and Jakob Bauer and Feng Yang and Riham Mansour and Jason Gelman and Yang Xu and George Polovets and Ji Liu and Honglong Cai and Warren Chen and XiangHai Sheng and Emily Xue and Sherjil Ozair and Christof Angermueller and Xiaowei Li and Anoop Sinha and Weiren Wang and Julia Wiesinger and Emmanouil Koukoumidis and Yuan Tian and Anand Iyer and Madhu Gurumurthy and Mark Goldenson and Parashar Shah and MK Blake and Hongkun Yu and Anthony Urbanowicz and Jennimaria Palomaki and Chrisantha Fernando and Ken Durden and Harsh Mehta and Nikola Momchev and Elahe Rahimtoroghi and Maria Georgaki and Amit Raul and Sebastian Ruder and Morgan Redshaw and Jinhyuk Lee and Denny Zhou and Komal Jalan and Dinghua Li and Blake Hechtman and Parker Schuh and Milad Nasr and Kieran Milan and Vladimir Mikulik and Juliana Franco and Tim Green and Nam Nguyen and Joe Kelley and Aroma Mahendru and Andrea Hu and Joshua Howland and Ben Vargas and Jeffrey Hui and Kshitij Bansal and Vikram Rao and Rakesh Ghiya and Emma Wang and Ke Ye and Jean Michel Sarr and Melanie Moranski Preston and Madeleine Elish and Steve Li and Aakash Kaku and Jigar Gupta and Ice Pasupat and Da-Cheng Juan and Milan Someswar and Tejvi M. and Xinyun Chen and Aida Amini and Alex Fabrikant and Eric Chu and Xuanyi Dong and Amruta Muthal and Senaka Buthpitiya and Sarthak Jauhari and Nan Hua and Urvashi Khandelwal and Ayal Hitron and Jie Ren and Larissa Rinaldi and Shahar Drath and Avigail Dabush and Nan-Jiang Jiang and Harshal Godhia and Uli Sachs and Anthony Chen and Yicheng Fan and Hagai Taitelbaum and Hila Noga and Zhuyun Dai and James Wang and Chen Liang and Jenny Hamer and Chun-Sung Ferng and Chenel Elkind and Aviel Atias and Paulina Lee and Vít Listík and Mathias Carlen and Jan van de Kerkhof and Marcin Pikus and Krunoslav Zaher and Paul Müller and Sasha Zykova and Richard Stefanec and Vitaly Gatsko and Christoph Hirnschall and Ashwin Sethi and Xingyu Federico Xu and Chetan Ahuja and Beth Tsai and Anca Stefanoiu and Bo Feng and Keshav Dhandhania and Manish Katyal and Akshay Gupta and Atharva Parulekar and Divya Pitta and Jing Zhao and Vivaan Bhatia and Yashodha Bhavnani and Omar Alhadlaq and Xiaolin Li and Peter Danenberg and Dennis Tu and Alex Pine and Vera Filippova and Abhipso Ghosh and Ben Limonchik and Bhargava Urala and Chaitanya Krishna Lanka and Derik Clive and Yi Sun and Edward Li and Hao Wu and Kevin Hongtongsak and Ianna Li and Kalind Thakkar and Kuanysh Omarov and Kushal Majmundar and Michael Alverson and Michael Kucharski and Mohak Patel and Mudit Jain and Maksim Zabelin and Paolo Pelagatti and Rohan Kohli and Saurabh Kumar and Joseph Kim and Swetha Sankar and Vineet Shah and Lakshmi Ramachandruni and Xiangkai Zeng and Ben Bariach and Laura Weidinger and Tu Vu and Alek Andreev and Antoine He and Kevin Hui and Sheleem Kashem and Amar Subramanya and Sissie Hsiao and Demis Hassabis and Koray Kavukcuoglu and Adam Sadovsky and Quoc Le and Trevor Strohman and Yonghui Wu and Slav Petrov and Jeffrey Dean and Oriol Vinyals},
  title         = {Gemini: A Family of Highly Capable Multimodal Models},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2312.11805},
  eprint        = {2312.11805},
  eprinttype    = {arxiv},
  file          = {:Team2024 - Gemini_ a Family of Highly Capable Multimodal Models.pdf:PDF:http\://arxiv.org/pdf/2312.11805v4},
  groups        = {Models},
  primaryclass  = {cs.CL},
}

@Misc{OpenAI2024,
  author        = {OpenAI and Josh Achiam and Steven Adler and Sandhini Agarwal and Lama Ahmad and Ilge Akkaya and Florencia Leoni Aleman and Diogo Almeida and Janko Altenschmidt and Sam Altman and Shyamal Anadkat and Red Avila and Igor Babuschkin and Suchir Balaji and Valerie Balcom and Paul Baltescu and Haiming Bao and Mohammad Bavarian and Jeff Belgum and Irwan Bello and Jake Berdine and Gabriel Bernadett-Shapiro and Christopher Berner and Lenny Bogdonoff and Oleg Boiko and Madelaine Boyd and Anna-Luisa Brakman and Greg Brockman and Tim Brooks and Miles Brundage and Kevin Button and Trevor Cai and Rosie Campbell and Andrew Cann and Brittany Carey and Chelsea Carlson and Rory Carmichael and Brooke Chan and Che Chang and Fotis Chantzis and Derek Chen and Sully Chen and Ruby Chen and Jason Chen and Mark Chen and Ben Chess and Chester Cho and Casey Chu and Hyung Won Chung and Dave Cummings and Jeremiah Currier and Yunxing Dai and Cory Decareaux and Thomas Degry and Noah Deutsch and Damien Deville and Arka Dhar and David Dohan and Steve Dowling and Sheila Dunning and Adrien Ecoffet and Atty Eleti and Tyna Eloundou and David Farhi and Liam Fedus and Niko Felix and Simón Posada Fishman and Juston Forte and Isabella Fulford and Leo Gao and Elie Georges and Christian Gibson and Vik Goel and Tarun Gogineni and Gabriel Goh and Rapha Gontijo-Lopes and Jonathan Gordon and Morgan Grafstein and Scott Gray and Ryan Greene and Joshua Gross and Shixiang Shane Gu and Yufei Guo and Chris Hallacy and Jesse Han and Jeff Harris and Yuchen He and Mike Heaton and Johannes Heidecke and Chris Hesse and Alan Hickey and Wade Hickey and Peter Hoeschele and Brandon Houghton and Kenny Hsu and Shengli Hu and Xin Hu and Joost Huizinga and Shantanu Jain and Shawn Jain and Joanne Jang and Angela Jiang and Roger Jiang and Haozhun Jin and Denny Jin and Shino Jomoto and Billie Jonn and Heewoo Jun and Tomer Kaftan and Łukasz Kaiser and Ali Kamali and Ingmar Kanitscheider and Nitish Shirish Keskar and Tabarak Khan and Logan Kilpatrick and Jong Wook Kim and Christina Kim and Yongjik Kim and Jan Hendrik Kirchner and Jamie Kiros and Matt Knight and Daniel Kokotajlo and Łukasz Kondraciuk and Andrew Kondrich and Aris Konstantinidis and Kyle Kosic and Gretchen Krueger and Vishal Kuo and Michael Lampe and Ikai Lan and Teddy Lee and Jan Leike and Jade Leung and Daniel Levy and Chak Ming Li and Rachel Lim and Molly Lin and Stephanie Lin and Mateusz Litwin and Theresa Lopez and Ryan Lowe and Patricia Lue and Anna Makanju and Kim Malfacini and Sam Manning and Todor Markov and Yaniv Markovski and Bianca Martin and Katie Mayer and Andrew Mayne and Bob McGrew and Scott Mayer McKinney and Christine McLeavey and Paul McMillan and Jake McNeil and David Medina and Aalok Mehta and Jacob Menick and Luke Metz and Andrey Mishchenko and Pamela Mishkin and Vinnie Monaco and Evan Morikawa and Daniel Mossing and Tong Mu and Mira Murati and Oleg Murk and David Mély and Ashvin Nair and Reiichiro Nakano and Rajeev Nayak and Arvind Neelakantan and Richard Ngo and Hyeonwoo Noh and Long Ouyang and Cullen O'Keefe and Jakub Pachocki and Alex Paino and Joe Palermo and Ashley Pantuliano and Giambattista Parascandolo and Joel Parish and Emy Parparita and Alex Passos and Mikhail Pavlov and Andrew Peng and Adam Perelman and Filipe de Avila Belbute Peres and Michael Petrov and Henrique Ponde de Oliveira Pinto and Michael and Pokorny and Michelle Pokrass and Vitchyr H. Pong and Tolly Powell and Alethea Power and Boris Power and Elizabeth Proehl and Raul Puri and Alec Radford and Jack Rae and Aditya Ramesh and Cameron Raymond and Francis Real and Kendra Rimbach and Carl Ross and Bob Rotsted and Henri Roussez and Nick Ryder and Mario Saltarelli and Ted Sanders and Shibani Santurkar and Girish Sastry and Heather Schmidt and David Schnurr and John Schulman and Daniel Selsam and Kyla Sheppard and Toki Sherbakov and Jessica Shieh and Sarah Shoker and Pranav Shyam and Szymon Sidor and Eric Sigler and Maddie Simens and Jordan Sitkin and Katarina Slama and Ian Sohl and Benjamin Sokolowsky and Yang Song and Natalie Staudacher and Felipe Petroski Such and Natalie Summers and Ilya Sutskever and Jie Tang and Nikolas Tezak and Madeleine B. Thompson and Phil Tillet and Amin Tootoonchian and Elizabeth Tseng and Preston Tuggle and Nick Turley and Jerry Tworek and Juan Felipe Cerón Uribe and Andrea Vallone and Arun Vijayvergiya and Chelsea Voss and Carroll Wainwright and Justin Jay Wang and Alvin Wang and Ben Wang and Jonathan Ward and Jason Wei and CJ Weinmann and Akila Welihinda and Peter Welinder and Jiayi Weng and Lilian Weng and Matt Wiethoff and Dave Willner and Clemens Winter and Samuel Wolrich and Hannah Wong and Lauren Workman and Sherwin Wu and Jeff Wu and Michael Wu and Kai Xiao and Tao Xu and Sarah Yoo and Kevin Yu and Qiming Yuan and Wojciech Zaremba and Rowan Zellers and Chong Zhang and Marvin Zhang and Shengjia Zhao and Tianhao Zheng and Juntang Zhuang and William Zhuk and Barret Zoph},
  title         = {GPT-4 Technical Report},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2303.08774},
  eprint        = {2303.08774},
  eprinttype    = {arxiv},
  file          = {:OpenAI2024 - GPT 4 Technical Report.pdf:PDF:http\://arxiv.org/pdf/2303.08774v6},
  groups        = {Models},
  primaryclass  = {cs.CL},
}

@InProceedings{Jouppi2023,
  author    = {Jouppi, Norm and Kurian, George and Li, Sheng and Ma, Peter and Nagarajan, Rahul and Nai, Lifeng and Patil, Nishant and Subramanian, Suvinay and Swing, Andy and Towles, Brian and Young, Clifford and Zhou, Xiang and Zhou, Zongwei and Patterson, David A},
  booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
  title     = {TPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings},
  year      = {2023},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {ISCA '23},
  abstract  = {In response to innovations in machine learning (ML) models, production workloads changed radically and rapidly. TPU v4 is the fifth Google domain specific architecture (DSA) and its third supercomputer for such ML models. Optical circuit switches (OCSes) dynamically reconfigure its interconnect topology to improve scale, availability, utilization, modularity, deployment, security, power, and performance; users can pick a twisted 3D torus topology if desired. Much cheaper, lower power, and faster than Infiniband, OCSes and underlying optical components are <5\% of system cost and <3\% of system power. Each TPU v4 includes SparseCores, dataflow processors that accelerate models that rely on embeddings by 5x--7x yet use only 5\% of die area and power. Deployed since 2020, TPU v4 outperforms TPU v3 by 2.1x and improves performance/Watt by 2.7x. The TPU v4 supercomputer is 4x larger at 4096 chips and thus nearly 10x faster overall, which along with OCS flexibility and availability allows a large language model to train at an average of ~60\% of peak FLOPS/second. For similar sized systems, it is ~4.3x--4.5x faster than the Graphcore IPU Bow and is 1.2x--1.7x faster and uses 1.3x--1.9x less power than the Nvidia A100. TPU v4s inside the energy-optimized warehouse scale computers of Google Cloud use ~2--6x less energy and produce ~20x less CO2e than contemporary DSAs in typical on-premise data centers.},
  articleno = {82},
  doi       = {10.1145/3579371.3589350},
  file      = {:Jouppi2023 - TPU V4_ an Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings.pdf:PDF},
  isbn      = {9798400700958},
  keywords  = {machine learning, domain specific architecture, TPU, GPU, IPU, supercomputer, optical interconnect, reconfigurable, embeddings, large language model, power usage effectiveness, warehouse scale computer, carbon emissions, energy, CO2 equivalent emissions},
  location  = {Orlando, FL, USA},
  numpages  = {14},
}

@InProceedings{Schuhmann2022,
  author    = {Schuhmann, Christoph and Beaumont, Romain and Vencu, Richard and Gordon, Cade and Wightman, Ross and Cherti, Mehdi and Coombes, Theo and Katta, Aarush and Mullis, Clayton and Wortsman, Mitchell and Schramowski, Patrick and Kundurthy, Srivatsa and Crowson, Katherine and Schmidt, Ludwig and Kaczmarczyk, Robert and Jitsev, Jenia},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {LAION-5B: An open large-scale dataset for training next generation image-text models},
  year      = {2022},
  editor    = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
  pages     = {25278--25294},
  publisher = {Curran Associates, Inc.},
  volume    = {35},
  file      = {:Schuhmann2022 - LAION 5B_ an Open Large Scale Dataset for Training Next Generation Image Text Models.pdf:PDF;:NeurIPS-2022-laion-5b-an-open-large-scale-dataset-for-training-next-generation-image-text-models-Supplemental-Datasets_and_Benchmarks.pdf:PDF},
  groups    = {Datasets},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2022/file/a1859debfb3b59d094f3504d5ebb6c25-Paper-Datasets_and_Benchmarks.pdf},
}

@InProceedings{2023,
  author = {OpenAI},
  title  = {GPT-4V(ision) System Card},
  year   = {2023},
  file   = {:2023 - GPT 4V(ision) System Card.pdf:PDF},
  groups = {Models},
  url    = {https://api.semanticscholar.org/CorpusID:263218031},
}

@Misc{Sun2023a,
  author        = {Quan Sun and Yuxin Fang and Ledell Wu and Xinlong Wang and Yue Cao},
  title         = {EVA-CLIP: Improved Training Techniques for CLIP at Scale},
  year          = {2023},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2303.15389},
  eprint        = {2303.15389},
  eprinttype    = {arxiv},
  file          = {:Sun2023a - EVA CLIP_ Improved Training Techniques for CLIP at Scale.pdf:PDF:https\://arxiv.org/pdf/2303.15389.pdf},
  groups        = {Models},
  primaryclass  = {cs.CV},
}

@Misc{Zhu2024,
  author        = {Yichen Zhu and Minjie Zhu and Ning Liu and Zhicai Ou and Xiaofeng Mou and Jian Tang},
  title         = {LLaVA-Phi: Efficient Multi-Modal Assistant with Small Language Model},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2401.02330},
  eprint        = {2401.02330},
  eprinttype    = {arxiv},
  file          = {:Zhu2024 - LLaVA Phi_ Efficient Multi Modal Assistant with Small Language Model.pdf:PDF:http\://arxiv.org/pdf/2401.02330v4},
  groups        = {Models},
  primaryclass  = {cs.CV},
}

@InProceedings{Li2023c,
  author    = {Li, Yifan and Du, Yifan and Zhou, Kun and Wang, Jinpeng and Zhao, Xin and Wen, Ji-Rong},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  title     = {Evaluating Object Hallucination in Large Vision-Language Models},
  year      = {2023},
  address   = {Singapore},
  editor    = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
  month     = dec,
  pages     = {292--305},
  publisher = {Association for Computational Linguistics},
  abstract  = {Inspired by the superior language abilities of large language models (LLM), large vision-language models (LVLM) have been recently proposed by integrating powerful LLMs for improving the performance on complex multimodal tasks. Despite the promising progress on LVLMs, we find that they suffer from object hallucinations, i.e., they tend to generate objects inconsistent with the target images in the descriptions. To investigate it, this work presents the first systematic study on object hallucination of LVLMs. We conduct the evaluation experiments on several representative LVLMs, and show that they mostly suffer from severe object hallucination issues. We further discuss that the visual instructions may influence the hallucination, and find that: objects that frequently appear in the visual instructions or co-occur with the image objects are obviously prone to be hallucinated by LVLMs. Besides, we further design a polling-based query method called POPE for better evaluation of object hallucination. Experiment results show that our POPE can evaluate object hallucination in a more stable and flexible way.},
  doi       = {10.18653/v1/2023.emnlp-main.20},
  file      = {:Li2023c - Evaluating Object Hallucination in Large Vision Language Models.pdf:PDF},
  url       = {https://aclanthology.org/2023.emnlp-main.20},
}

@Misc{Xu2024,
  author        = {Hu Xu and Saining Xie and Xiaoqing Ellen Tan and Po-Yao Huang and Russell Howes and Vasu Sharma and Shang-Wen Li and Gargi Ghosh and Luke Zettlemoyer and Christoph Feichtenhofer},
  title         = {Demystifying CLIP Data},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2309.16671},
  eprint        = {2309.16671},
  eprinttype    = {arxiv},
  file          = {:Xu2024 - Demystifying CLIP Data.pdf:PDF:http\://arxiv.org/pdf/2309.16671v4},
  primaryclass  = {cs.CV},
}

@Misc{Vishniakov2024,
  author        = {Kirill Vishniakov and Zhiqiang Shen and Zhuang Liu},
  title         = {ConvNet vs Transformer, Supervised vs CLIP: Beyond ImageNet Accuracy},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2311.09215},
  eprint        = {2311.09215},
  eprinttype    = {arxiv},
  file          = {:Vishniakov2024 - ConvNet Vs Transformer, Supervised Vs CLIP_ beyond ImageNet Accuracy.pdf:PDF:http\://arxiv.org/pdf/2311.09215v2},
  primaryclass  = {cs.CV},
}

@Article{Paszke2017,
  author = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
  title  = {Automatic differentiation in pytorch},
  year   = {2017},
  file   = {:Paszke2017 - Automatic Differentiation in Pytorch.pdf:PDF},
}








@Article{Gilardi2023,
  author   = {Fabrizio Gilardi and Meysam Alizadeh and Maël Kubli},
  journal  = {Proceedings of the National Academy of Sciences},
  title    = {{ChatGPT} outperforms crowd workers for text-annotation tasks},
  year     = {2023},
  number   = {30},
  pages    = {e2305016120},
  volume   = {120},
  abstract = {Many NLP applications require manual text annotations for a variety of tasks, notably to train classifiers or evaluate the performance of unsupervised models. Depending on the size and degree of complexity, the tasks may be conducted by crowd workers on platforms such as MTurk as well as trained annotators, such as research assistants. Using four samples of tweets and news articles (n = 6,183), we show that ChatGPT outperforms crowd workers for several annotation tasks, including relevance, stance, topics, and frame detection. Across the four datasets, the zero-shot accuracy of ChatGPT exceeds that of crowd workers by about 25 percentage points on average, while ChatGPT’s intercoder agreement exceeds that of both crowd workers and trained annotators for all tasks. Moreover, the per-annotation cost of ChatGPT is less than \$0.003—about thirty times cheaper than MTurk. These results demonstrate the potential of large language models to drastically increase the efficiency of text classification.},
  doi      = {10.1073/pnas.2305016120},
  file     = {:Gilardi2023 - ChatGPT Outperforms Crowd Workers for Text Annotation Tasks.pdf:PDF},
}

@InProceedings{Stiennon2020,
  author    = {Stiennon, Nisan and Ouyang, Long and Wu, Jeffrey and Ziegler, Daniel and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul F},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Learning to summarize with human feedback},
  year      = {2020},
  editor    = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
  pages     = {3008--3021},
  publisher = {Curran Associates, Inc.},
  volume    = {33},
  file      = {:Stiennon2020 - Learning to Summarize with Human Feedback.pdf:PDF;:NeurIPS-2020-learning-to-summarize-with-human-feedback-Supplemental.pdf:PDF},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1f89885d556929e98d3ef9b86448f951-Paper.pdf},
}

@Misc{Press2017,
  author        = {Ofir Press and Lior Wolf},
  title         = {Using the Output Embedding to Improve Language Models},
  year          = {2017},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.1608.05859},
  eprint        = {1608.05859},
  eprinttype    = {arxiv},
  file          = {:Press2017 - Using the Output Embedding to Improve Language Models.pdf:PDF:https\://aclanthology.org/E17-2025.pdf},
  primaryclass  = {cs.CL},
}

@InProceedings{Chollet2017,
  author    = {Chollet, Francois},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Xception: Deep Learning With Depthwise Separable Convolutions},
  year      = {2017},
  month     = {July},
  file      = {:Chollet2017 - Xception_ Deep Learning with Depthwise Separable Convolutions.pdf:PDF},
}

@Misc{Gou2024,
  author        = {Zhibin Gou and Zhihong Shao and Yeyun Gong and Yelong Shen and Yujiu Yang and Minlie Huang and Nan Duan and Weizhu Chen},
  title         = {ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2309.17452},
  eprint        = {2309.17452},
  eprinttype    = {arxiv},
  file          = {:Gou2024 - ToRA_ a Tool Integrated Reasoning Agent for Mathematical Problem Solving.pdf:PDF:http\://arxiv.org/pdf/2309.17452v4},
  primaryclass  = {cs.CL},
}

@Misc{Karras2022,
  author        = {Tero Karras and Miika Aittala and Timo Aila and Samuli Laine},
  title         = {Elucidating the Design Space of Diffusion-Based Generative Models},
  year          = {2022},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2206.00364},
  eprint        = {2206.00364},
  eprinttype    = {arxiv},
  file          = {:Karras2022 - Elucidating the Design Space of Diffusion Based Generative Models.pdf:PDF:http\://arxiv.org/pdf/2206.00364v2},
  primaryclass  = {cs.CV},
}

@Misc{Chern2024,
  author        = {Ethan Chern and Jiadi Su and Yan Ma and Pengfei Liu},
  title         = {ANOLE: An Open, Autoregressive, Native Large Multimodal Models for Interleaved Image-Text Generation},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2407.06135},
  eprint        = {2407.06135},
  eprinttype    = {arxiv},
  file          = {:Chern2024 - ANOLE_ an Open, Autoregressive, Native Large Multimodal Models for Interleaved Image Text Generation.pdf:PDF:http\://arxiv.org/pdf/2407.06135v1},
  primaryclass  = {cs.CL},
}

@InProceedings{Andrychowicz2016,
  author    = {Andrychowicz, Marcin and Denil, Misha and G\'{o}mez, Sergio and Hoffman, Matthew W and Pfau, David and Schaul, Tom and Shillingford, Brendan and de Freitas, Nando},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Learning to learn by gradient descent by gradient descent},
  year      = {2016},
  editor    = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
  publisher = {Curran Associates, Inc.},
  volume    = {29},
  file      = {:Andrychowicz2016 - Learning to Learn by Gradient Descent by Gradient Descent.pdf:PDF;:learning-to-learn-appendix.pdf:PDF},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2016/file/fb87582825f9d28a8d42c5e5e5e8b23d-Paper.pdf},
}

@Misc{Li2017,
  author        = {Zhenguo Li and Fengwei Zhou and Fei Chen and Hang Li},
  title         = {Meta-SGD: Learning to Learn Quickly for Few-Shot Learning},
  year          = {2017},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.1707.09835},
  eprint        = {1707.09835},
  eprinttype    = {arxiv},
  file          = {:Li2017 - Meta SGD_ Learning to Learn Quickly for Few Shot Learning.pdf:PDF:http\://arxiv.org/pdf/1707.09835v2},
  primaryclass  = {cs.LG},
}

@InProceedings{Doveh2023,
  author    = {Doveh, Sivan and Arbelle, Assaf and Harary, Sivan and Schwartz, Eli and Herzig, Roei and Giryes, Raja and Feris, Rogerio and Panda, Rameswar and Ullman, Shimon and Karlinsky, Leonid},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Teaching {S}tructured {V}ision \& {L}anguage {C}oncepts to {V}ision \& {L}anguage {M}odels},
  year      = {2023},
  month     = {June},
  pages     = {2657-2668},
  file      = {:Doveh2023 - Teaching Structured Vision & Language Concepts to Vision & Language Models.pdf:PDF;:Doveh_Teaching_Structured_Vision_CVPR_2023_supplemental.pdf:PDF},
}

@InProceedings{Lamb2016,
  author    = {Lamb, Alex M and ALIAS PARTH GOYAL, Anirudh Goyal and Zhang, Ying and Zhang, Saizheng and Courville, Aaron C and Bengio, Yoshua},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Professor Forcing: A New Algorithm for Training Recurrent Networks},
  year      = {2016},
  editor    = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
  publisher = {Curran Associates, Inc.},
  volume    = {29},
  file      = {:Lamb2016 - Professor Forcing_ a New Algorithm for Training Recurrent Networks.pdf:PDF},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2016/file/16026d60ff9b54410b3435b403afd226-Paper.pdf},
}

@InProceedings{Wei2022,
  author    = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and ichter, brian and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {{Chain-of-Thought Prompting Elicits Reasoning in Large Language Models}},
  year      = {2022},
  editor    = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
  pages     = {24824--24837},
  publisher = {Curran Associates, Inc.},
  volume    = {35},
  file      = {:Wei2022 - Chain of Thought Prompting Elicits Reasoning in Large Language Models.pdf:PDF;:NeurIPS-2022-chain-of-thought-prompting-elicits-reasoning-in-large-language-models-Supplemental-Conference.pdf:PDF},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf},
}

@Misc{Huang2023,
  author        = {Lei Huang and Weijiang Yu and Weitao Ma and Weihong Zhong and Zhangyin Feng and Haotian Wang and Qianglong Chen and Weihua Peng and Xiaocheng Feng and Bing Qin and Ting Liu},
  title         = {A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions},
  year          = {2023},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2311.05232},
  eprint        = {2311.05232},
  eprinttype    = {arxiv},
  file          = {:Huang2023 - A Survey on Hallucination in Large Language Models_ Principles, Taxonomy, Challenges, and Open Questions.pdf:PDF:http\://arxiv.org/pdf/2311.05232v1},
  primaryclass  = {cs.CL},
}

@Misc{Zhang2023b,
  author        = {Yue Zhang and Yafu Li and Leyang Cui and Deng Cai and Lemao Liu and Tingchen Fu and Xinting Huang and Enbo Zhao and Yu Zhang and Yulong Chen and Longyue Wang and Anh Tuan Luu and Wei Bi and Freda Shi and Shuming Shi},
  title         = {Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models},
  year          = {2023},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2309.01219},
  eprint        = {2309.01219},
  eprinttype    = {arxiv},
  file          = {:Zhang2023b - Siren's Song in the AI Ocean_ a Survey on Hallucination in Large Language Models.pdf:PDF:http\://arxiv.org/pdf/2309.01219v2},
  primaryclass  = {cs.CL},
}

@Misc{Liu2024b,
  author        = {Hanchao Liu and Wenyuan Xue and Yifei Chen and Dapeng Chen and Xiutian Zhao and Ke Wang and Liping Hou and Rongjun Li and Wei Peng},
  title         = {A Survey on Hallucination in Large Vision-Language Models},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2402.00253},
  eprint        = {2402.00253},
  eprinttype    = {arxiv},
  file          = {:Liu2024b - A Survey on Hallucination in Large Vision Language Models.pdf:PDF:https\://arxiv.org/pdf/2402.00253.pdf},
  primaryclass  = {cs.CV},
}

@Misc{Bai2024,
  author        = {Zechen Bai and Pichao Wang and Tianjun Xiao and Tong He and Zongbo Han and Zheng Zhang and Mike Zheng Shou},
  title         = {Hallucination of Multimodal Large Language Models: A Survey},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2404.18930},
  eprint        = {2404.18930},
  eprinttype    = {arxiv},
  file          = {:Bai2024 - Hallucination of Multimodal Large Language Models_ a Survey.pdf:PDF:http\://arxiv.org/pdf/2404.18930v1},
  primaryclass  = {cs.CV},
}

@Misc{Adiwardana2020,
  author        = {Daniel Adiwardana and Minh-Thang Luong and David R. So and Jamie Hall and Noah Fiedel and Romal Thoppilan and Zi Yang and Apoorv Kulshreshtha and Gaurav Nemade and Yifeng Lu and Quoc V. Le},
  title         = {Towards a Human-like Open-Domain Chatbot},
  year          = {2020},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2001.09977},
  eprint        = {2001.09977},
  eprinttype    = {arxiv},
  file          = {:Adiwardana2020 - Towards a Human like Open Domain Chatbot.pdf:PDF:https\://arxiv.org/pdf/2001.09977.pdf},
  primaryclass  = {cs.CL},
}

@Misc{Rae2022,
  author        = {Jack W. Rae and Sebastian Borgeaud and Trevor Cai and Katie Millican and Jordan Hoffmann and Francis Song and John Aslanides and Sarah Henderson and Roman Ring and Susannah Young and Eliza Rutherford and Tom Hennigan and Jacob Menick and Albin Cassirer and Richard Powell and George van den Driessche and Lisa Anne Hendricks and Maribeth Rauh and Po-Sen Huang and Amelia Glaese and Johannes Welbl and Sumanth Dathathri and Saffron Huang and Jonathan Uesato and John Mellor and Irina Higgins and Antonia Creswell and Nat McAleese and Amy Wu and Erich Elsen and Siddhant Jayakumar and Elena Buchatskaya and David Budden and Esme Sutherland and Karen Simonyan and Michela Paganini and Laurent Sifre and Lena Martens and Xiang Lorraine Li and Adhiguna Kuncoro and Aida Nematzadeh and Elena Gribovskaya and Domenic Donato and Angeliki Lazaridou and Arthur Mensch and Jean-Baptiste Lespiau and Maria Tsimpoukelli and Nikolai Grigorev and Doug Fritz and Thibault Sottiaux and Mantas Pajarskas and Toby Pohlen and Zhitao Gong and Daniel Toyama and Cyprien de Masson d'Autume and Yujia Li and Tayfun Terzi and Vladimir Mikulik and Igor Babuschkin and Aidan Clark and Diego de Las Casas and Aurelia Guy and Chris Jones and James Bradbury and Matthew Johnson and Blake Hechtman and Laura Weidinger and Iason Gabriel and William Isaac and Ed Lockhart and Simon Osindero and Laura Rimell and Chris Dyer and Oriol Vinyals and Kareem Ayoub and Jeff Stanway and Lorrayne Bennett and Demis Hassabis and Koray Kavukcuoglu and Geoffrey Irving},
  title         = {Scaling Language Models: Methods, Analysis & Insights from Training Gopher},
  year          = {2022},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2112.11446},
  eprint        = {2112.11446},
  eprinttype    = {arxiv},
  file          = {:Rae2022 - Scaling Language Models_ Methods, Analysis & Insights from Training Gopher.pdf:PDF:http\://arxiv.org/pdf/2112.11446v2},
  primaryclass  = {cs.CL},
}

@Misc{Manakul2023,
  author        = {Potsawee Manakul and Adian Liusie and Mark J. F. Gales},
  title         = {SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models},
  year          = {2023},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2303.08896},
  eprint        = {2303.08896},
  eprinttype    = {arxiv},
  file          = {:Manakul2023 - SelfCheckGPT_ Zero Resource Black Box Hallucination Detection for Generative Large Language Models.pdf:PDF:https\://aclanthology.org/2023.emnlp-main.557.pdf},
  primaryclass  = {cs.CL},
}

@InProceedings{Birhane2021,
  author    = {Birhane, Abeba and Prabhu, Vinay Uday},
  booktitle = {2021 IEEE Winter Conference on Applications of Computer Vision (WACV)},
  title     = {Large image datasets: A pyrrhic win for computer vision?},
  year      = {2021},
  pages     = {1536-1546},
  doi       = {10.1109/WACV48630.2021.00158},
  file      = {:Birhane2021 - Large Image Datasets_ a Pyrrhic Win for Computer Vision_.pdf:PDF},
  keywords  = {Computer vision;Conferences;IEEE Constitution;Faces},
}

@InProceedings{Kornblith2019,
  author    = {Kornblith, Simon and Shlens, Jonathon and Le, Quoc V.},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Do Better ImageNet Models Transfer Better?},
  year      = {2019},
  month     = {June},
  file      = {:Kornblith2019 - Do Better ImageNet Models Transfer Better_.pdf:PDF;:Kornblith_Do_Better_ImageNet_CVPR_2019_supplemental.pdf:PDF},
}

@InProceedings{He2019,
  author    = {He, Kaiming and Girshick, Ross and Dollar, Piotr},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  title     = {{Rethinking ImageNet Pre-Training}},
  year      = {2019},
  month     = {October},
  file      = {:He2019 - Rethinking ImageNet Pre Training.pdf:PDF},
}

@InProceedings{Lin2014,
  author    = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C. Lawrence},
  booktitle = {Computer Vision -- ECCV 2014},
  title     = {Microsoft {COCO}: {C}ommon {O}bjects in {C}ontext},
  year      = {2014},
  address   = {Cham},
  editor    = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
  pages     = {740--755},
  publisher = {Springer International Publishing},
  abstract  = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
  file      = {:Lin2014 - Microsoft COCO_ Common Objects in Context.pdf:PDF:https\://link.springer.com/content/pdf/10.1007/978-3-319-10602-1_48.pdf},
  isbn      = {978-3-319-10602-1},
}

@Article{Gao2024,
  author    = {Gao, Peng and Geng, Shijie and Zhang, Renrui and Ma, Teli and Fang, Rongyao and Zhang, Yongfeng and Li, Hongsheng and Qiao, Yu},
  journal   = {International Journal of Computer Vision},
  title     = {{CLIP-Adapter: Better Vision-Language Models with Feature Adapters}},
  year      = {2024},
  issn      = {1573-1405},
  month     = sep,
  number    = {2},
  pages     = {581--595},
  volume    = {132},
  doi       = {10.1007/s11263-023-01891-x},
  file      = {:Gao2024 - CLIP Adapter_ Better Vision Language Models with Feature Adapters.pdf:PDF},
  publisher = {Springer},
}

@Article{LlamaTeam2024,
  author   = {Llama Team, AI @ Meta},
  journal  = {arXiv preprint},
  title    = {{The Llama 3 Herd of Models}},
  year     = {2024},
  month    = jul,
  abstract = {Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.},
  file     = {:LlamaTeam2024 - The Llama 3 Herd of Models.pdf:PDF},
  groups   = {Models},
  url      = {https://ai.meta.com/research/publications/the-llama-3-herd-of-models/},
}

@InProceedings{Mikolov2013a,
  author    = {Mikolov, Tom{\'a}{\v{s}} and Yih, Wen-tau and Zweig, Geoffrey},
  booktitle = {Proceedings of the 2013 conference of the north american chapter of the association for computational linguistics: Human language technologies},
  title     = {Linguistic regularities in continuous space word representations},
  year      = {2013},
  pages     = {746--751},
  file      = {:Mikolov2013a - Linguistic Regularities in Continuous Space Word Representations.pdf:PDF},
}

@Article{Elhage2021,
  author  = {Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
  journal = {Transformer Circuits Thread},
  title   = {A Mathematical Framework for Transformer Circuits},
  year    = {2021},
  file    = {:Elhage2021 - A Mathematical Framework for Transformer Circuits.pdf:PDF},
  url     = {https://transformer-circuits.pub/2021/framework/index.html},
}

@Article{Su2024,
  author   = {Jianlin Su and Murtadha Ahmed and Yu Lu and Shengfeng Pan and Wen Bo and Yunfeng Liu},
  journal  = {Neurocomputing},
  title    = {RoFormer: Enhanced transformer with Rotary Position Embedding},
  year     = {2024},
  issn     = {0925-2312},
  pages    = {127063},
  volume   = {568},
  abstract = {Position encoding has recently been shown to be effective in transformer architecture. It enables valuable supervision for dependency modeling between elements at different positions of the sequence. In this paper, we first investigate various methods to integrate positional information into the learning process of transformer-based language models. Then, we propose a novel method named Rotary Position Embedding (RoPE) to effectively leverage the positional information. Specifically, the proposed RoPE encodes the absolute position with a rotation matrix and meanwhile incorporates the explicit relative position dependency in the self-attention formulation. Notably, RoPE enables valuable properties, including the flexibility of sequence length, decaying inter-token dependency with increasing relative distances, and the capability of equipping linear self-attention with relative position encoding. Finally, we evaluate the enhanced transformer with rotary position embedding, also called RoFormer, on various long text classification benchmark datasets. Our experiments show that it consistently overcomes its alternatives. Furthermore, we provide a theoretical analysis to explain some experimental results. RoFormer is already integrated into Huggingface: https://huggingface.co/docs/transformers/model_doc/roformer.},
  doi      = {10.1016/j.neucom.2023.127063},
  file     = {:Su2024 - RoFormer_ Enhanced Transformer with Rotary Position Embedding.pdf:PDF:https\://arxiv.org/pdf/2104.09864},
  keywords = {Pre-trained language models, Position information encoding, Pre-training, Natural language processing},
  url      = {https://www.sciencedirect.com/science/article/pii/S0925231223011864},
}

@Misc{Press2021,
  author        = {Ofir Press and Noah A. Smith and Mike Lewis},
  title         = {Shortformer: Better Language Modeling using Shorter Inputs},
  year          = {2021},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2012.15832},
  eprint        = {2012.15832},
  eprinttype    = {arxiv},
  file          = {:Press2021 - Shortformer_ Better Language Modeling Using Shorter Inputs.pdf:PDF:https\://aclanthology.org/2021.acl-long.427.pdf},
  primaryclass  = {cs.CL},
}

@InProceedings{Kirillov2023,
  author    = {Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C. and Lo, Wan-Yen and Dollar, Piotr and Girshick, Ross},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  title     = {Segment Anything},
  year      = {2023},
  month     = {October},
  pages     = {4015-4026},
  file      = {:Kirillov2023 - Segment Anything.pdf:PDF;:Kirillov_Segment_Anything_ICCV_2023_supplemental.pdf:PDF},
  groups    = {Models, Datasets},
}

@InProceedings{Zou2023a,
  author    = {Zou, Xueyan and Yang, Jianwei and Zhang, Hao and Li, Feng and Li, Linjie and Wang, Jianfeng and Wang, Lijuan and Gao, Jianfeng and Lee, Yong Jae},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Segment Everything Everywhere All at Once},
  year      = {2023},
  editor    = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
  pages     = {19769--19782},
  publisher = {Curran Associates, Inc.},
  volume    = {36},
  file      = {:Zou2023a - Segment Everything Everywhere All at Once.pdf:PDF;:NeurIPS-2023-segment-everything-everywhere-all-at-once-Supplemental-Conference.pdf:PDF},
  groups    = {Models},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2023/file/3ef61f7e4afacf9a2c5b71c726172b86-Paper-Conference.pdf},
}

@InProceedings{Zhang2023c,
  author    = {Zhang, Hang and Li, Xin and Bing, Lidong},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
  title     = {Video-{LL}a{MA}: An Instruction-tuned Audio-Visual Language Model for Video Understanding},
  year      = {2023},
  address   = {Singapore},
  editor    = {Feng, Yansong and Lefever, Els},
  month     = dec,
  pages     = {543--553},
  publisher = {Association for Computational Linguistics},
  abstract  = {We present Video-LLaMA, a multi-modal framework that empowers Large Language Models (LLMs) with the capability of understanding both visual and auditory content in the video. Video-LLaMA bootstraps cross-modal training from the frozen pre-trained visual {\&} audio encoders and the frozen LLMs. Unlike previous works that complement LLMs to process the visual or audio signals only, Video-LLaMA enables video comprehension by tackling two challenges: (1) capturing the temporal changes in visual scenes, (2) integrating audio-visual signals. To counter the first challenge, we propose a Video Q-former to assemble a pre-trained image encoder into our video encoder and introduce a video-to-text generation task to learn video-language correspondence. For the second challenge, we leverage ImageBind, a universal embedding model aligning multiple modalities, as the pre-trained audio encoder and introduce an Audio Q-former on top of ImageBind to learn reasonable auditory query embeddings for the LLM module. To align the output of both visual {\&} audio encoders with LLM{'}s embedding space, we first train Video-LLaMA on massive video/image-caption pairs and then tune our model with visual-instruction datasets of moderate amount but higher quality. We found Video-LLaMA shows the ability to perceive and comprehend video content and generate meaningful responses grounded in the visual and auditory information presented in the videos.},
  doi       = {10.18653/v1/2023.emnlp-demo.49},
  file      = {:Zhang2023c - Video LLaMA_ an Instruction Tuned Audio Visual Language Model for Video Understanding.pdf:PDF},
  groups    = {Models},
  url       = {https://aclanthology.org/2023.emnlp-demo.49},
}

@InProceedings{Gafni2022,
  author    = {Gafni, Oran and Polyak, Adam and Ashual, Oron and Sheynin, Shelly and Parikh, Devi and Taigman, Yaniv},
  booktitle = {Computer Vision -- ECCV 2022},
  title     = {Make-A-Scene: Scene-Based Text-to-Image Generation with Human Priors},
  year      = {2022},
  address   = {Cham},
  editor    = {Avidan, Shai and Brostow, Gabriel and Ciss{\'e}, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
  pages     = {89--106},
  publisher = {Springer Nature Switzerland},
  abstract  = {Recent text-to-image generation methods provide a simple yet exciting conversion capability between text and image domains. While these methods have incrementally improved the generated image fidelity and text relevancy, several pivotal gaps remain unanswered, limiting applicability and quality. We propose a novel text-to-image method that addresses these gaps by (i) enabling a simple control mechanism complementary to text in the form of a scene, (ii) introducing elements that substantially improve the tokenization process by employing domain-specific knowledge over key image regions (faces and salient objects), and (iii) adapting classifier-free guidance for the transformer use case. Our model achieves state-of-the-art FID and human evaluation results, unlocking the ability to generate high fidelity images in a resolution of {\$}{\$}512{\backslash}times 512{\$}{\$}512{\texttimes}512pixels, significantly improving visual quality. Through scene controllability, we introduce several new capabilities: (i) Scene editing, (ii) text editing with anchor scenes, (iii) overcoming out-of-distribution text prompts, and (iv) story illustration generation, as demonstrated in the story we wrote.},
  file      = {:Gafni2022 - Make a Scene_ Scene Based Text to Image Generation With Human Priors.pdf:PDF},
  isbn      = {978-3-031-19784-0},
}

@Misc{Lin2023a,
  author        = {Bin Lin and Yang Ye and Bin Zhu and Jiaxi Cui and Munan Ning and Peng Jin and Li Yuan},
  title         = {Video-LLaVA: Learning United Visual Representation by Alignment Before Projection},
  year          = {2023},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2311.10122},
  eprint        = {2311.10122},
  eprinttype    = {arxiv},
  file          = {:Lin2023a - Video LLaVA_ Learning United Visual Representation by Alignment before Projection.pdf:PDF:https\://arxiv.org/pdf/2311.10122.pdf},
  groups        = {Models},
  primaryclass  = {cs.CV},
}

@Misc{Xu2024a,
  author        = {Ruyi Xu and Yuan Yao and Zonghao Guo and Junbo Cui and Zanlin Ni and Chunjiang Ge and Tat-Seng Chua and Zhiyuan Liu and Maosong Sun and Gao Huang},
  title         = {LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2403.11703},
  eprint        = {2403.11703},
  eprinttype    = {arxiv},
  file          = {:Xu2024a - LLaVA UHD_ an LMM Perceiving Any Aspect Ratio and High Resolution Images.pdf:PDF:https\://arxiv.org/pdf/2403.11703.pdf},
  groups        = {Models},
  primaryclass  = {cs.CV},
}

@Article{Rogers2021,
  author   = {Rogers, Anna and Kovaleva, Olga and Rumshisky, Anna},
  journal  = {Transactions of the Association for Computational Linguistics},
  title    = {{A Primer in BERTology: What We Know About How BERT Works}},
  year     = {2021},
  issn     = {2307-387X},
  month    = {01},
  pages    = {842-866},
  volume   = {8},
  abstract = {{Transformer-based models have pushed state of the art in many areas of NLP, but our understanding of what is behind their success is still limited. This paper is the first survey of over 150 studies of the popular BERT model. We review the current state of knowledge about how BERT works, what kind of information it learns and how it is represented, common modifications to its training objectives and architecture, the overparameterization issue, and approaches to compression. We then outline directions for future research.}},
  doi      = {10.1162/tacl_a_00349},
  file     = {:Rogers2021 - A Primer in BERTology_ What We Know about How BERT Works.pdf:PDF},
}

@InProceedings{Lindner2023,
  author    = {Lindner, David and Kramar, Janos and Farquhar, Sebastian and Rahtz, Matthew and McGrath, Tom and Mikulik, Vladimir},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Tracr: Compiled Transformers as a Laboratory for Interpretability},
  year      = {2023},
  editor    = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
  pages     = {37876--37899},
  publisher = {Curran Associates, Inc.},
  volume    = {36},
  file      = {:Lindner2023 - Tracr_ Compiled Transformers As a Laboratory for Interpretability.pdf:PDF},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2023/file/771155abaae744e08576f1f3b4b7ac0d-Paper-Conference.pdf},
}

@Misc{Hendrycks2021,
  author        = {Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt},
  title         = {Measuring Massive Multitask Language Understanding},
  year          = {2021},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2009.03300},
  eprint        = {2009.03300},
  eprinttype    = {arxiv},
  file          = {:Hendrycks2021 - Measuring Massive Multitask Language Understanding.pdf:PDF:https\://arxiv.org/pdf/2009.03300.pdf},
  primaryclass  = {cs.CY},
}

@InProceedings{Ainslie2023,
  author    = {Ainslie, Joshua and Lee-Thorp, James and de Jong, Michiel and Zemlyanskiy, Yury and Lebron, Federico and Sanghai, Sumit},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  title     = {{GQA}: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints},
  year      = {2023},
  address   = {Singapore},
  editor    = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
  month     = dec,
  pages     = {4895--4901},
  publisher = {Association for Computational Linguistics},
  abstract  = {Multi-query attention (MQA), which only uses a single key-value head, drastically speeds up decoder inference. However, MQA can lead to quality degradation, and moreover it may not be desirable to train a separate model just for faster inference. We (1) propose a recipe for uptraining existing multi-head language model checkpoints into models with MQA using 5{\%} of original pre-training compute, and (2) introduce grouped-query attention (GQA), a generalization of multi-query attention which uses an intermediate (more than one, less than number of query heads) number of key-value heads. We show that uptrained GQA achieves quality close to multi-head attention with comparable speed to MQA.},
  doi       = {10.18653/v1/2023.emnlp-main.298},
  file      = {:Ainslie2023 - GQA_ Training Generalized Multi Query Transformer Models from Multi Head Checkpoints.pdf:PDF},
  url       = {https://aclanthology.org/2023.emnlp-main.298},
}

@Misc{Chen2024a,
  author        = {Zhe Chen and Weiyun Wang and Hao Tian and Shenglong Ye and Zhangwei Gao and Erfei Cui and Wenwen Tong and Kongzhi Hu and Jiapeng Luo and Zheng Ma and Ji Ma and Jiaqi Wang and Xiaoyi Dong and Hang Yan and Hewei Guo and Conghui He and Botian Shi and Zhenjiang Jin and Chao Xu and Bin Wang and Xingjian Wei and Wei Li and Wenjian Zhang and Bo Zhang and Pinlong Cai and Licheng Wen and Xiangchao Yan and Min Dou and Lewei Lu and Xizhou Zhu and Tong Lu and Dahua Lin and Yu Qiao and Jifeng Dai and Wenhai Wang},
  title         = {{How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites}},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2404.16821},
  eprint        = {2404.16821},
  eprinttype    = {arxiv},
  file          = {:Chen2024a - How Far Are We to GPT 4V_ Closing the Gap to Commercial Multimodal Models with Open Source Suites.pdf:PDF:http\://arxiv.org/pdf/2404.16821v2},
  groups        = {Models},
  primaryclass  = {cs.CV},
}

@Article{Anthropic2024,
  author = {Anthropic},
  title  = {{The Claude 3 Model Family: Opus, Sonnet, Haiku}},
  year   = {2024},
  file   = {:- The Claude 3 Model Family_ Opus, Sonnet, Haiku.pdf:PDF},
  groups = {Models},
  url    = {https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf},
}

@Misc{McKinzie2024,
  author        = {Brandon McKinzie and Zhe Gan and Jean-Philippe Fauconnier and Sam Dodge and Bowen Zhang and Philipp Dufter and Dhruti Shah and Xianzhi Du and Futang Peng and Floris Weers and Anton Belyi and Haotian Zhang and Karanjeet Singh and Doug Kang and Ankur Jain and Hongyu Hè and Max Schwarzer and Tom Gunter and Xiang Kong and Aonan Zhang and Jianyu Wang and Chong Wang and Nan Du and Tao Lei and Sam Wiseman and Guoli Yin and Mark Lee and Zirui Wang and Ruoming Pang and Peter Grasch and Alexander Toshev and Yinfei Yang},
  title         = {MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2403.09611},
  eprint        = {2403.09611},
  eprinttype    = {arxiv},
  file          = {:McKinzie2024 - MM1_ Methods, Analysis & Insights from Multimodal LLM Pre Training.pdf:PDF:https\://arxiv.org/pdf/2403.09611.pdf},
  groups        = {Models},
  primaryclass  = {cs.CV},
}

@Misc{Chen2024b,
  author        = {Guiming Hardy Chen and Shunian Chen and Ruifei Zhang and Junying Chen and Xiangbo Wu and Zhiyi Zhang and Zhihong Chen and Jianquan Li and Xiang Wan and Benyou Wang},
  title         = {ALLaVA: Harnessing GPT4V-Synthesized Data for Lite Vision-Language Models},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2402.11684},
  eprint        = {2402.11684},
  eprinttype    = {arxiv},
  file          = {:Chen2024b - ALLaVA_ Harnessing GPT4V Synthesized Data for Lite Vision Language Models.pdf:PDF:http\://arxiv.org/pdf/2402.11684v2},
  groups        = {Models},
  primaryclass  = {cs.CL},
}

@Misc{Team2024a,
  author        = {Reka Team and Aitor Ormazabal and Che Zheng and Cyprien de Masson d'Autume and Dani Yogatama and Deyu Fu and Donovan Ong and Eric Chen and Eugenie Lamprecht and Hai Pham and Isaac Ong and Kaloyan Aleksiev and Lei Li and Matthew Henderson and Max Bain and Mikel Artetxe and Nishant Relan and Piotr Padlewski and Qi Liu and Ren Chen and Samuel Phua and Yazheng Yang and Yi Tay and Yuqi Wang and Zhongkai Zhu and Zhihui Xie},
  title         = {Reka Core, Flash, and Edge: A Series of Powerful Multimodal Language Models},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2404.12387},
  eprint        = {2404.12387},
  eprinttype    = {arxiv},
  file          = {:Team2024a - Reka Core, Flash, and Edge_ a Series of Powerful Multimodal Language Models.pdf:PDF:http\://arxiv.org/pdf/2404.12387v1},
  groups        = {Models},
  primaryclass  = {cs.CL},
}

@InProceedings{Guan2024,
  author    = {Guan, Tianrui and Liu, Fuxiao and Wu, Xiyang and Xian, Ruiqi and Li, Zongxia and Liu, Xiaoyu and Wang, Xijun and Chen, Lichang and Huang, Furong and Yacoob, Yaser and Manocha, Dinesh and Zhou, Tianyi},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {HallusionBench: An Advanced Diagnostic Suite for Entangled Language Hallucination and Visual Illusion in Large Vision-Language Models},
  year      = {2024},
  month     = {June},
  pages     = {14375-14385},
  file      = {:Guan2024 - HallusionBench_ an Advanced Diagnostic Suite for Entangled Language Hallucination and Visual Illusion in Large Vision Language Models.pdf:PDF;:Guan_HallusionBench_An_Advanced_CVPR_2024_supplemental.pdf:PDF},
}

@Misc{Arditi2024,
  author        = {Andy Arditi and Oscar Obeso and Aaquib Syed and Daniel Paleka and Nina Rimsky and Wes Gurnee and Neel Nanda},
  title         = {Refusal in Language Models Is Mediated by a Single Direction},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2406.11717},
  eprint        = {2406.11717},
  eprinttype    = {arxiv},
  file          = {:Arditi2024 - Refusal in Language Models Is Mediated by a Single Direction.pdf:PDF:http\://arxiv.org/pdf/2406.11717v1},
  primaryclass  = {id='cs.LG' full_name='Machine Learning' is_active=True alt_name=None in_archive='cs' is_general=False description='Papers on all aspects of machine learning research (supervised, unsupervised, reinforcement learning, bandit problems, and so on) including also robustness, explanation, fairness, and methodology. cs.LG is also an appropriate primary category for applications of machine learning methods.'},
}

@Misc{Nanda2023,
  author        = {Neel Nanda and Andrew Lee and Martin Wattenberg},
  title         = {Emergent Linear Representations in World Models of Self-Supervised Sequence Models},
  year          = {2023},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2309.00941},
  eprint        = {2309.00941},
  eprinttype    = {arxiv},
  file          = {:Nanda2023 - Emergent Linear Representations in World Models of Self Supervised Sequence Models.pdf:PDF:https\://aclanthology.org/2023.blackboxnlp-1.2.pdf},
  primaryclass  = {cs.LG},
}

@Misc{Wang2020,
  author        = {Sinong Wang and Belinda Z. Li and Madian Khabsa and Han Fang and Hao Ma},
  title         = {Linformer: Self-Attention with Linear Complexity},
  year          = {2020},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2006.04768},
  eprint        = {2006.04768},
  eprinttype    = {arxiv},
  file          = {:Wang2020 - Linformer_ Self Attention with Linear Complexity.pdf:PDF:https\://arxiv.org/pdf/2006.04768.pdf},
  primaryclass  = {cs.LG},
}

@InProceedings{Shen2021,
  author    = {Shen, Zhuoran and Zhang, Mingyuan and Zhao, Haiyu and Yi, Shuai and Li, Hongsheng},
  booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
  title     = {Efficient Attention: Attention With Linear Complexities},
  year      = {2021},
  month     = {January},
  pages     = {3531-3539},
  file      = {:Shen2021 - Efficient Attention_ Attention with Linear Complexities.pdf:PDF},
}

@Misc{Sun2024,
  author        = {Mingjie Sun and Xinlei Chen and J. Zico Kolter and Zhuang Liu},
  title         = {Massive Activations in Large Language Models},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2402.17762},
  eprint        = {2402.17762},
  eprinttype    = {arxiv},
  file          = {:Sun2024 - Massive Activations in Large Language Models.pdf:PDF:http\://arxiv.org/pdf/2402.17762v2},
  primaryclass  = {cs.CL},
}

@Misc{Lyth2024,
  author        = {Dan Lyth and Simon King},
  title         = {Natural language guidance of high-fidelity text-to-speech with synthetic annotations},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2402.01912},
  eprint        = {2402.01912},
  eprinttype    = {arxiv},
  file          = {:Lyth2024 - Natural Language Guidance of High Fidelity Text to Speech with Synthetic Annotations.pdf:PDF:http\://arxiv.org/pdf/2402.01912v1},
  primaryclass  = {cs.SD},
}

@Misc{Jozefowicz2016,
  author        = {Rafal Jozefowicz and Oriol Vinyals and Mike Schuster and Noam Shazeer and Yonghui Wu},
  title         = {Exploring the Limits of Language Modeling},
  year          = {2016},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.1602.02410},
  eprint        = {1602.02410},
  eprinttype    = {arxiv},
  file          = {:Jozefowicz2016 - Exploring the Limits of Language Modeling.pdf:PDF:https\://arxiv.org/pdf/1602.02410.pdf},
  primaryclass  = {cs.CL},
}

@Article{Raffel2020,
  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  journal = {Journal of Machine Learning Research},
  title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  year    = {2020},
  number  = {140},
  pages   = {1--67},
  volume  = {21},
  file    = {:Raffel2020 - Exploring the Limits of Transfer Learning with a Unified Text to Text Transformer.pdf:PDF},
  url     = {http://jmlr.org/papers/v21/20-074.html},
}

@Misc{Shazeer2017,
  author        = {Noam Shazeer and Azalia Mirhoseini and Krzysztof Maziarz and Andy Davis and Quoc Le and Geoffrey Hinton and Jeff Dean},
  title         = {Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer},
  year          = {2017},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.1701.06538},
  eprint        = {1701.06538},
  eprinttype    = {arxiv},
  file          = {:Shazeer2017 - Outrageously Large Neural Networks_ the Sparsely Gated Mixture of Experts Layer.pdf:PDF:https\://arxiv.org/pdf/1701.06538.pdf},
  primaryclass  = {cs.LG},
}

@InProceedings{Rafailov2023,
  author    = {Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Direct Preference Optimization: Your Language Model is Secretly a Reward Model},
  year      = {2023},
  editor    = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
  pages     = {53728--53741},
  publisher = {Curran Associates, Inc.},
  volume    = {36},
  file      = {:Rafailov2023 - Direct Preference Optimization_ Your Language Model Is Secretly a Reward Model.pdf:PDF;:NeurIPS-2023-direct-preference-optimization-your-language-model-is-secretly-a-reward-model-Supplemental-Conference.pdf:PDF},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2023/file/a85b405ed65c6477a4fe8302b5e06ce7-Paper-Conference.pdf},
}

@Misc{Schulman2017,
  author        = {John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
  title         = {Proximal Policy Optimization Algorithms},
  year          = {2017},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.1707.06347},
  eprint        = {1707.06347},
  eprinttype    = {arxiv},
  file          = {:Schulman2017 - Proximal Policy Optimization Algorithms.pdf:PDF:http\://arxiv.org/pdf/1707.06347v2},
  primaryclass  = {cs.LG},
}

@Misc{Touvron2023a,
  author        = {Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
  title         = {Llama 2: Open Foundation and Fine-Tuned Chat Models},
  year          = {2023},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2307.09288},
  eprint        = {2307.09288},
  eprinttype    = {arxiv},
  file          = {:Touvron2023a - Llama 2_ Open Foundation and Fine Tuned Chat Models.pdf:PDF:https\://arxiv.org/pdf/2307.09288.pdf},
  groups        = {Models},
  primaryclass  = {cs.CL},
}

@InProceedings{He2019a,
  author    = {He, Tong and Zhang, Zhi and Zhang, Hang and Zhang, Zhongyue and Xie, Junyuan and Li, Mu},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Bag of Tricks for Image Classification with Convolutional Neural Networks},
  year      = {2019},
  month     = {June},
  file      = {:He2019a - Bag of Tricks for Image Classification with Convolutional Neural Networks.pdf:PDF},
}

@Misc{Joulin2016,
  author        = {Armand Joulin and Edouard Grave and Piotr Bojanowski and Tomas Mikolov},
  title         = {Bag of Tricks for Efficient Text Classification},
  year          = {2016},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.1607.01759},
  eprint        = {1607.01759},
  eprinttype    = {arxiv},
  file          = {:Joulin2016 - Bag of Tricks for Efficient Text Classification.pdf:PDF:https\://aclanthology.org/E17-2068.pdf},
  primaryclass  = {cs.CL},
}

@Misc{Liu2019a,
  author        = {Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
  title         = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  year          = {2019},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.1907.11692},
  eprint        = {1907.11692},
  eprinttype    = {arxiv},
  file          = {:Liu2019a - RoBERTa_ a Robustly Optimized BERT Pretraining Approach.pdf:PDF:https\://arxiv.org/pdf/1907.11692.pdf},
  primaryclass  = {cs.CL},
}

@Misc{Nanda2023a,
  author        = {Neel Nanda and Lawrence Chan and Tom Lieberum and Jess Smith and Jacob Steinhardt},
  title         = {Progress measures for grokking via mechanistic interpretability},
  year          = {2023},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2301.05217},
  eprint        = {2301.05217},
  eprinttype    = {arxiv},
  file          = {:Nanda2023a - Progress Measures for Grokking Via Mechanistic Interpretability.pdf:PDF:http\://arxiv.org/pdf/2301.05217v3},
  primaryclass  = {cs.LG},
}

@Misc{Sanh2020,
  author        = {Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},
  title         = {DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  year          = {2020},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.1910.01108},
  eprint        = {1910.01108},
  eprinttype    = {arxiv},
  file          = {:Sanh2020 - DistilBERT, a Distilled Version of BERT_ Smaller, Faster, Cheaper and Lighter.pdf:PDF:http\://arxiv.org/pdf/1910.01108v4},
  primaryclass  = {cs.CL},
}

@Misc{DeepSeekAI2024,
  author        = {DeepSeek-AI and Qihao Zhu and Daya Guo and Zhihong Shao and Dejian Yang and Peiyi Wang and Runxin Xu and Y. Wu and Yukun Li and Huazuo Gao and Shirong Ma and Wangding Zeng and Xiao Bi and Zihui Gu and Hanwei Xu and Damai Dai and Kai Dong and Liyue Zhang and Yishi Piao and Zhibin Gou and Zhenda Xie and Zhewen Hao and Bingxuan Wang and Junxiao Song and Deli Chen and Xin Xie and Kang Guan and Yuxiang You and Aixin Liu and Qiushi Du and Wenjun Gao and Xuan Lu and Qinyu Chen and Yaohui Wang and Chengqi Deng and Jiashi Li and Chenggang Zhao and Chong Ruan and Fuli Luo and Wenfeng Liang},
  title         = {DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2406.11931},
  eprint        = {2406.11931},
  eprinttype    = {arxiv},
  file          = {:DeepSeekAI2024 - DeepSeek Coder V2_ Breaking the Barrier of Closed Source Models in Code Intelligence.pdf:PDF:http\://arxiv.org/pdf/2406.11931v1},
  groups        = {Models},
  primaryclass  = {cs.SE},
}

@Misc{Li2024,
  author        = {Jeffrey Li and Alex Fang and Georgios Smyrnis and Maor Ivgi and Matt Jordan and Samir Gadre and Hritik Bansal and Etash Guha and Sedrick Keh and Kushal Arora and Saurabh Garg and Rui Xin and Niklas Muennighoff and Reinhard Heckel and Jean Mercat and Mayee Chen and Suchin Gururangan and Mitchell Wortsman and Alon Albalak and Yonatan Bitton and Marianna Nezhurina and Amro Abbas and Cheng-Yu Hsieh and Dhruba Ghosh and Josh Gardner and Maciej Kilian and Hanlin Zhang and Rulin Shao and Sarah Pratt and Sunny Sanyal and Gabriel Ilharco and Giannis Daras and Kalyani Marathe and Aaron Gokaslan and Jieyu Zhang and Khyathi Chandu and Thao Nguyen and Igor Vasiljevic and Sham Kakade and Shuran Song and Sujay Sanghavi and Fartash Faghri and Sewoong Oh and Luke Zettlemoyer and Kyle Lo and Alaaeldin El-Nouby and Hadi Pouransari and Alexander Toshev and Stephanie Wang and Dirk Groeneveld and Luca Soldaini and Pang Wei Koh and Jenia Jitsev and Thomas Kollar and Alexandros G. Dimakis and Yair Carmon and Achal Dave and Ludwig Schmidt and Vaishaal Shankar},
  title         = {DataComp-LM: In search of the next generation of training sets for language models},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2406.11794},
  eprint        = {2406.11794},
  eprinttype    = {arxiv},
  file          = {:Li2024 - DataComp LM_ in Search of the Next Generation of Training Sets for Language Models.pdf:PDF:http\://arxiv.org/pdf/2406.11794v3},
  primaryclass  = {cs.LG},
}

@Misc{Li2024a,
  author        = {Chen Li and Weiqi Wang and Jingcheng Hu and Yixuan Wei and Nanning Zheng and Han Hu and Zheng Zhang and Houwen Peng},
  title         = {Common 7B Language Models Already Possess Strong Math Capabilities},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2403.04706},
  eprint        = {2403.04706},
  eprinttype    = {arxiv},
  file          = {:Li2024a - Common 7B Language Models Already Possess Strong Math Capabilities.pdf:PDF:http\://arxiv.org/pdf/2403.04706v1},
  primaryclass  = {cs.CL},
}

@Misc{Blakeney2024,
  author        = {Cody Blakeney and Mansheej Paul and Brett W. Larsen and Sean Owen and Jonathan Frankle},
  title         = {Does your data spark joy? Performance gains from domain upsampling at the end of training},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2406.03476},
  eprint        = {2406.03476},
  eprinttype    = {arxiv},
  file          = {:Blakeney2024 - Does Your Data Spark Joy_ Performance Gains from Domain Upsampling at the End of Training.pdf:PDF:http\://arxiv.org/pdf/2406.03476v1},
  primaryclass  = {cs.LG},
}

@Misc{Cobbe2021,
  author        = {Karl Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},
  title         = {Training Verifiers to Solve Math Word Problems},
  year          = {2021},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2110.14168},
  eprint        = {2110.14168},
  eprinttype    = {arxiv},
  file          = {:Cobbe2021 - Training Verifiers to Solve Math Word Problems.pdf:PDF:http\://arxiv.org/pdf/2110.14168v2},
  primaryclass  = {cs.LG},
}

@Misc{Hendrycks2021a,
  author        = {Dan Hendrycks and Collin Burns and Saurav Kadavath and Akul Arora and Steven Basart and Eric Tang and Dawn Song and Jacob Steinhardt},
  title         = {Measuring Mathematical Problem Solving With the MATH Dataset},
  year          = {2021},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2103.03874},
  eprint        = {2103.03874},
  eprinttype    = {arxiv},
  file          = {:Hendrycks2021a - Measuring Mathematical Problem Solving with the MATH Dataset.pdf:PDF:http\://arxiv.org/pdf/2103.03874v2},
  groups        = {Datasets},
  primaryclass  = {cs.LG},
}

@Misc{Xiong2023,
  author        = {Wenhan Xiong and Jingyu Liu and Igor Molybog and Hejia Zhang and Prajjwal Bhargava and Rui Hou and Louis Martin and Rashi Rungta and Karthik Abinav Sankararaman and Barlas Oguz and Madian Khabsa and Han Fang and Yashar Mehdad and Sharan Narang and Kshitiz Malik and Angela Fan and Shruti Bhosale and Sergey Edunov and Mike Lewis and Sinong Wang and Hao Ma},
  title         = {Effective Long-Context Scaling of Foundation Models},
  year          = {2023},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2309.16039},
  eprint        = {2309.16039},
  eprinttype    = {arxiv},
  file          = {:Xiong2023 - Effective Long Context Scaling of Foundation Models.pdf:PDF:https\://aclanthology.org/2024.naacl-long.260.pdf},
  primaryclass  = {cs.CL},
}

@InProceedings{Hoffmann2024,
  author    = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and de Las Casas, Diego and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and Hennigan, Tom and Noland, Eric and Millican, Katie and van den Driessche, George and Damoc, Bogdan and Guy, Aurelia and Osindero, Simon and Simonyan, Karen and Elsen, Erich and Vinyals, Oriol and Rae, Jack W. and Sifre, Laurent},
  booktitle = {Proceedings of the 36th International Conference on Neural Information Processing Systems},
  title     = {Training compute-optimal large language models},
  year      = {2024},
  address   = {Red Hook, NY, USA},
  publisher = {Curran Associates Inc.},
  series    = {NIPS '22},
  abstract  = {We investigate the optimal model size and number of tokens for training a Transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4\texttimes{} more more data. Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5\% on the MMLU benchmark, greater than a 7\% improvement over Gopher.},
  articleno = {2176},
  file      = {:Hoffmann2024 - Training Compute Optimal Large Language Models.pdf:PDF},
  isbn      = {9781713871088},
  location  = {New Orleans, LA, USA},
  numpages  = {15},
}

@Misc{Wei2022a,
  author        = {Jason Wei and Maarten Bosma and Vincent Y. Zhao and Kelvin Guu and Adams Wei Yu and Brian Lester and Nan Du and Andrew M. Dai and Quoc V. Le},
  title         = {Finetuned Language Models Are Zero-Shot Learners},
  year          = {2022},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2109.01652},
  eprint        = {2109.01652},
  eprinttype    = {arxiv},
  file          = {:Wei2022a - Finetuned Language Models Are Zero Shot Learners.pdf:PDF:https\://arxiv.org/pdf/2109.01652.pdf},
  primaryclass  = {cs.CL},
}

@Misc{Ziegler2020,
  author        = {Daniel M. Ziegler and Nisan Stiennon and Jeffrey Wu and Tom B. Brown and Alec Radford and Dario Amodei and Paul Christiano and Geoffrey Irving},
  title         = {Fine-Tuning Language Models from Human Preferences},
  year          = {2020},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.1909.08593},
  eprint        = {1909.08593},
  eprinttype    = {arxiv},
  file          = {:Ziegler2020 - Fine Tuning Language Models from Human Preferences.pdf:PDF:http\://arxiv.org/pdf/1909.08593v2},
  primaryclass  = {cs.CL},
}

@Article{Chowdhery2023,
  author  = {Aakanksha Chowdhery and Sharan Narang and Jacob Devlin and Maarten Bosma and Gaurav Mishra and Adam Roberts and Paul Barham and Hyung Won Chung and Charles Sutton and Sebastian Gehrmann and Parker Schuh and Kensen Shi and Sasha Tsvyashchenko and Joshua Maynez and Abhishek Rao and Parker Barnes and Yi Tay and Noam Shazeer and Vinodkumar Prabhakaran and Emily Reif and Nan Du and Ben Hutchinson and Reiner Pope and James Bradbury and Jacob Austin and Michael Isard and Guy Gur-Ari and Pengcheng Yin and Toju Duke and Anselm Levskaya and Sanjay Ghemawat and Sunipa Dev and Henryk Michalewski and Xavier Garcia and Vedant Misra and Kevin Robinson and Liam Fedus and Denny Zhou and Daphne Ippolito and David Luan and Hyeontaek Lim and Barret Zoph and Alexander Spiridonov and Ryan Sepassi and David Dohan and Shivani Agrawal and Mark Omernick and Andrew M. Dai and Thanumalayan Sankaranarayana Pillai and Marie Pellat and Aitor Lewkowycz and Erica Moreira and Rewon Child and Oleksandr Polozov and Katherine Lee and Zongwei Zhou and Xuezhi Wang and Brennan Saeta and Mark Diaz and Orhan Firat and Michele Catasta and Jason Wei and Kathy Meier-Hellstern and Douglas Eck and Jeff Dean and Slav Petrov and Noah Fiedel},
  journal = {Journal of Machine Learning Research},
  title   = {PaLM: Scaling Language Modeling with Pathways},
  year    = {2023},
  number  = {240},
  pages   = {1--113},
  volume  = {24},
  file    = {:Chowdhery2023 - PaLM_ Scaling Language Modeling with Pathways.pdf:PDF},
  url     = {http://jmlr.org/papers/v24/22-1144.html},
}

@InProceedings{Ansel2024,
  author    = {Ansel, Jason and Yang, Edward and He, Horace and Gimelshein, Natalia and Jain, Animesh and Voznesensky, Michael and Bao, Bin and Bell, Peter and Berard, David and Burovski, Evgeni and Chauhan, Geeta and Chourdia, Anjali and Constable, Will and Desmaison, Alban and DeVito, Zachary and Ellison, Elias and Feng, Will and Gong, Jiong and Gschwind, Michael and Hirsh, Brian and Huang, Sherlock and Kalambarkar, Kshiteej and Kirsch, Laurent and Lazos, Michael and Lezcano, Mario and Liang, Yanbo and Liang, Jason and Lu, Yinghai and Luk, C. K. and Maher, Bert and Pan, Yunjie and Puhrsch, Christian and Reso, Matthias and Saroufim, Mark and Siraichi, Marcos Yukio and Suk, Helen and Zhang, Shunting and Suo, Michael and Tillet, Phil and Zhao, Xu and Wang, Eikan and Zhou, Keren and Zou, Richard and Wang, Xiaodong and Mathews, Ajit and Wen, William and Chanan, Gregory and Wu, Peng and Chintala, Soumith},
  booktitle = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
  title     = {PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation},
  year      = {2024},
  address   = {New York, NY, USA},
  pages     = {929–947},
  publisher = {Association for Computing Machinery},
  series    = {ASPLOS '24},
  abstract  = {This paper introduces two extensions to the popular PyTorch machine learning framework, TorchDynamo and TorchInductor, which implement the torch.compile feature released in PyTorch 2. TorchDynamo is a Python-level just-in-time (JIT) compiler that enables graph compilation in PyTorch programs without sacrificing the flexibility of Python. It achieves this by dynamically modifying Python bytecode before execution and extracting sequences of PyTorch operations into an FX graph, which is then JIT compiled using one of many extensible backends. TorchInductor is the default compiler backend for TorchDynamo, which translates PyTorch programs into OpenAI's Triton for GPUs and C++ for CPUs. Results show that TorchDynamo is able to capture graphs more robustly than prior approaches while adding minimal overhead, and TorchInductor is able to provide a 2.27\texttimes{} inference and 1.41\texttimes{} training geometric mean speedup on an NVIDIA A100 GPU across 180+ real-world models, which outperforms six other compilers. These extensions provide a new way to apply optimizations through compilers in eager mode frameworks like PyTorch.},
  doi       = {10.1145/3620665.3640366},
  file      = {:Ansel2024 - PyTorch 2_ Faster Machine Learning through Dynamic Python Bytecode Transformation and Graph Compilation.pdf:PDF},
  isbn      = {9798400703850},
  location  = {La Jolla, CA, USA},
  numpages  = {19},
}

@Misc{Bai2022,
  author        = {Yuntao Bai and Saurav Kadavath and Sandipan Kundu and Amanda Askell and Jackson Kernion and Andy Jones and Anna Chen and Anna Goldie and Azalia Mirhoseini and Cameron McKinnon and Carol Chen and Catherine Olsson and Christopher Olah and Danny Hernandez and Dawn Drain and Deep Ganguli and Dustin Li and Eli Tran-Johnson and Ethan Perez and Jamie Kerr and Jared Mueller and Jeffrey Ladish and Joshua Landau and Kamal Ndousse and Kamile Lukosuite and Liane Lovitt and Michael Sellitto and Nelson Elhage and Nicholas Schiefer and Noemi Mercado and Nova DasSarma and Robert Lasenby and Robin Larson and Sam Ringer and Scott Johnston and Shauna Kravec and Sheer El Showk and Stanislav Fort and Tamera Lanham and Timothy Telleen-Lawton and Tom Conerly and Tom Henighan and Tristan Hume and Samuel R. Bowman and Zac Hatfield-Dodds and Ben Mann and Dario Amodei and Nicholas Joseph and Sam McCandlish and Tom Brown and Jared Kaplan},
  title         = {Constitutional AI: Harmlessness from AI Feedback},
  year          = {2022},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2212.08073},
  eprint        = {2212.08073},
  eprinttype    = {arxiv},
  file          = {:Bai2022 - Constitutional AI_ Harmlessness from AI Feedback.pdf:PDF:http\://arxiv.org/pdf/2212.08073v1},
  primaryclass  = {cs.CL},
}

@InProceedings{Kwon2023a,
  author    = {Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph and Zhang, Hao and Stoica, Ion},
  booktitle = {Proceedings of the 29th Symposium on Operating Systems Principles},
  title     = {Efficient Memory Management for Large Language Model Serving with PagedAttention},
  year      = {2023},
  address   = {New York, NY, USA},
  pages     = {611–626},
  publisher = {Association for Computing Machinery},
  series    = {SOSP '23},
  abstract  = {High throughput serving of large language models (LLMs) requires batching sufficiently many requests at a time. However, existing systems struggle because the key-value cache (KV cache) memory for each request is huge and grows and shrinks dynamically. When managed inefficiently, this memory can be significantly wasted by fragmentation and redundant duplication, limiting the batch size. To address this problem, we propose PagedAttention, an attention algorithm inspired by the classical virtual memory and paging techniques in operating systems. On top of it, we build vLLM, an LLM serving system that achieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV cache within and across requests to further reduce memory usage. Our evaluations show that vLLM improves the throughput of popular LLMs by 2--4\texttimes{} with the same level of latency compared to the state-of-the-art systems, such as FasterTransformer and Orca. The improvement is more pronounced with longer sequences, larger models, and more complex decoding algorithms. vLLM's source code is publicly available at https://github.com/vllm-project/vllm.},
  doi       = {10.1145/3600006.3613165},
  file      = {:Kwon2023a - Efficient Memory Management for Large Language Model Serving with PagedAttention.pdf:PDF},
  isbn      = {9798400702297},
  location  = {Koblenz, Germany},
  numpages  = {16},
}

@InProceedings{Lu2024,
  author    = {Keming Lu and Hongyi Yuan and Zheng Yuan and Runji Lin and Junyang Lin and Chuanqi Tan and Chang Zhou and Jingren Zhou},
  booktitle = {The Twelfth International Conference on Learning Representations},
  title     = {\#InsTag: Instruction Tagging for Analyzing Supervised Fine-tuning of Large Language Models},
  year      = {2024},
  file      = {:Lu2024 - #InsTag_ Instruction Tagging for Analyzing Supervised Fine Tuning of Large Language Models.pdf:PDF},
  url       = {https://openreview.net/forum?id=pszewhybU9},
}

@Misc{Gururangan2020,
  author        = {Suchin Gururangan and Ana Marasović and Swabha Swayamdipta and Kyle Lo and Iz Beltagy and Doug Downey and Noah A. Smith},
  title         = {Don't Stop Pretraining: Adapt Language Models to Domains and Tasks},
  year          = {2020},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2004.10964},
  eprint        = {2004.10964},
  eprinttype    = {arxiv},
  file          = {:Gururangan2020 - Don't Stop Pretraining_ Adapt Language Models to Domains and Tasks.pdf:PDF:https\://aclanthology.org/2020.acl-main.740.pdf},
  primaryclass  = {cs.CL},
}

@Misc{Roziere2024,
  author        = {Baptiste Rozière and Jonas Gehring and Fabian Gloeckle and Sten Sootla and Itai Gat and Xiaoqing Ellen Tan and Yossi Adi and Jingyu Liu and Romain Sauvestre and Tal Remez and Jérémy Rapin and Artyom Kozhevnikov and Ivan Evtimov and Joanna Bitton and Manish Bhatt and Cristian Canton Ferrer and Aaron Grattafiori and Wenhan Xiong and Alexandre Défossez and Jade Copet and Faisal Azhar and Hugo Touvron and Louis Martin and Nicolas Usunier and Thomas Scialom and Gabriel Synnaeve},
  title         = {Code Llama: Open Foundation Models for Code},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2308.12950},
  eprint        = {2308.12950},
  eprinttype    = {arxiv},
  file          = {:Roziere2024 - Code Llama_ Open Foundation Models for Code.pdf:PDF:http\://arxiv.org/pdf/2308.12950v3},
  groups        = {Models},
  primaryclass  = {cs.CL},
}

@Misc{Chen2023c,
  author        = {Nuo Chen and Zinan Zheng and Ning Wu and Ming Gong and Yangqiu Song and Dongmei Zhang and Jia Li},
  title         = {Breaking Language Barriers in Multilingual Mathematical Reasoning: Insights and Observations},
  year          = {2023},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2310.20246},
  eprint        = {2310.20246},
  eprinttype    = {arxiv},
  file          = {:Chen2023c - Breaking Language Barriers in Multilingual Mathematical Reasoning_ Insights and Observations.pdf:PDF:http\://arxiv.org/pdf/2310.20246v4},
  primaryclass  = {cs.CL},
}

@Article{Cassano2023,
  author   = {Cassano, Federico and Gouwar, John and Nguyen, Daniel and Nguyen, Sydney and Phipps-Costin, Luna and Pinckney, Donald and Yee, Ming-Ho and Zi, Yangtian and Anderson, Carolyn Jane and Feldman, Molly Q and Guha, Arjun and Greenberg, Michael and Jangda, Abhinav},
  journal  = {IEEE Transactions on Software Engineering},
  title    = {MultiPL-E: A Scalable and Polyglot Approach to Benchmarking Neural Code Generation},
  year     = {2023},
  number   = {7},
  pages    = {3675-3691},
  volume   = {49},
  doi      = {10.1109/TSE.2023.3267446},
  file     = {:Cassano2023 - MultiPL E_ a Scalable and Polyglot Approach to Benchmarking Neural Code Generation.pdf:PDF},
  keywords = {Codes;Benchmark testing;Python;Programming;Natural languages;Task analysis;Syntactics;B.2.3 reliability, testing, and fault-tolerance;I.5.1.D neural nets},
}

@Misc{Communication2023,
  author        = {Seamless Communication and Loïc Barrault and Yu-An Chung and Mariano Cora Meglioli and David Dale and Ning Dong and Paul-Ambroise Duquenne and Hady Elsahar and Hongyu Gong and Kevin Heffernan and John Hoffman and Christopher Klaiber and Pengwei Li and Daniel Licht and Jean Maillard and Alice Rakotoarison and Kaushik Ram Sadagopan and Guillaume Wenzek and Ethan Ye and Bapi Akula and Peng-Jen Chen and Naji El Hachem and Brian Ellis and Gabriel Mejia Gonzalez and Justin Haaheim and Prangthip Hansanti and Russ Howes and Bernie Huang and Min-Jae Hwang and Hirofumi Inaguma and Somya Jain and Elahe Kalbassi and Amanda Kallet and Ilia Kulikov and Janice Lam and Daniel Li and Xutai Ma and Ruslan Mavlyutov and Benjamin Peloquin and Mohamed Ramadan and Abinesh Ramakrishnan and Anna Sun and Kevin Tran and Tuan Tran and Igor Tufanov and Vish Vogeti and Carleigh Wood and Yilin Yang and Bokai Yu and Pierre Andrews and Can Balioglu and Marta R. Costa-jussà and Onur Celebi and Maha Elbayad and Cynthia Gao and Francisco Guzmán and Justine Kao and Ann Lee and Alexandre Mourachko and Juan Pino and Sravya Popuri and Christophe Ropers and Safiyyah Saleem and Holger Schwenk and Paden Tomasello and Changhan Wang and Jeff Wang and Skyler Wang},
  title         = {SeamlessM4T: Massively Multilingual & Multimodal Machine Translation},
  year          = {2023},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2308.11596},
  eprint        = {2308.11596},
  eprinttype    = {arxiv},
  file          = {:Communication2023 - SeamlessM4T_ Massively Multilingual & Multimodal Machine Translation.pdf:PDF:https\://arxiv.org/pdf/2308.11596.pdf},
  primaryclass  = {cs.CL},
}

@InProceedings{Bizzoni2020,
  author    = {Bizzoni, Yuri and Juzek, Tom S and Espa{\~n}a-Bonet, Cristina and Dutta Chowdhury, Koel and van Genabith, Josef and Teich, Elke},
  booktitle = {Proceedings of the 17th International Conference on Spoken Language Translation},
  title     = {How Human is Machine Translationese? Comparing Human and Machine Translations of Text and Speech},
  year      = {2020},
  address   = {Online},
  editor    = {Federico, Marcello and Waibel, Alex and Knight, Kevin and Nakamura, Satoshi and Ney, Hermann and Niehues, Jan and St{\"u}ker, Sebastian and Wu, Dekai and Mariani, Joseph and Yvon, Francois},
  month     = jul,
  pages     = {280--290},
  publisher = {Association for Computational Linguistics},
  abstract  = {Translationese is a phenomenon present in human translations, simultaneous interpreting, and even machine translations. Some translationese features tend to appear in simultaneous interpreting with higher frequency than in human text translation, but the reasons for this are unclear. This study analyzes translationese patterns in translation, interpreting, and machine translation outputs in order to explore possible reasons. In our analysis we {--} (i) detail two non-invasive ways of detecting translationese and (ii) compare translationese across human and machine translations from text and speech. We find that machine translation shows traces of translationese, but does not reproduce the patterns found in human translation, offering support to the hypothesis that such patterns are due to the model (human vs machine) rather than to the data (written vs spoken).},
  doi       = {10.18653/v1/2020.iwslt-1.34},
  file      = {:Bizzoni2020 - How Human Is Machine Translationese_ Comparing Human and Machine Translations of Text and Speech.pdf:PDF},
  url       = {https://aclanthology.org/2020.iwslt-1.34},
}

@Misc{Didolkar2024,
  author        = {Aniket Didolkar and Anirudh Goyal and Nan Rosemary Ke and Siyuan Guo and Michal Valko and Timothy Lillicrap and Danilo Rezende and Yoshua Bengio and Michael Mozer and Sanjeev Arora},
  title         = {Metacognitive Capabilities of LLMs: An Exploration in Mathematical Problem Solving},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2405.12205},
  eprint        = {2405.12205},
  eprinttype    = {arxiv},
  file          = {:Didolkar2024 - Metacognitive Capabilities of LLMs_ an Exploration in Mathematical Problem Solving.pdf:PDF:http\://arxiv.org/pdf/2405.12205v1},
  primaryclass  = {cs.AI},
}

@Article{Mialon2023,
  author  = {Gr{\'e}goire Mialon and Roberto Dessi and Maria Lomeli and Christoforos Nalmpantis and Ramakanth Pasunuru and Roberta Raileanu and Baptiste Roziere and Timo Schick and Jane Dwivedi-Yu and Asli Celikyilmaz and Edouard Grave and Yann LeCun and Thomas Scialom},
  journal = {Transactions on Machine Learning Research},
  title   = {Augmented Language Models: a Survey},
  year    = {2023},
  issn    = {2835-8856},
  note    = {Survey Certification},
  file    = {:Mialon2023 - Augmented Language Models_ a Survey.pdf:PDF},
  url     = {https://openreview.net/forum?id=jh7wH2AzKK},
}

@Misc{Samvelyan2024,
  author        = {Mikayel Samvelyan and Sharath Chandra Raparthy and Andrei Lupu and Eric Hambro and Aram H. Markosyan and Manish Bhatt and Yuning Mao and Minqi Jiang and Jack Parker-Holder and Jakob Foerster and Tim Rocktäschel and Roberta Raileanu},
  title         = {Rainbow Teaming: Open-Ended Generation of Diverse Adversarial Prompts},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2402.16822},
  eprint        = {2402.16822},
  eprinttype    = {arxiv},
  file          = {:Samvelyan2024 - Rainbow Teaming_ Open Ended Generation of Diverse Adversarial Prompts.pdf:PDF:https\://arxiv.org/pdf/2402.16822.pdf},
  primaryclass  = {cs.CL},
}

@Misc{Schuhmann2021,
  author        = {Christoph Schuhmann and Richard Vencu and Romain Beaumont and Robert Kaczmarczyk and Clayton Mullis and Aarush Katta and Theo Coombes and Jenia Jitsev and Aran Komatsuzaki},
  title         = {LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs},
  year          = {2021},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2111.02114},
  eprint        = {2111.02114},
  eprinttype    = {arxiv},
  file          = {:Schuhmann2021 - LAION 400M_ Open Dataset of CLIP Filtered 400 Million Image Text Pairs.pdf:PDF:http\://arxiv.org/pdf/2111.02114v1},
  groups        = {Datasets},
  primaryclass  = {cs.CV},
}

@Misc{Zhang2024b,
  author        = {Pan Zhang and Xiaoyi Dong and Yuhang Zang and Yuhang Cao and Rui Qian and Lin Chen and Qipeng Guo and Haodong Duan and Bin Wang and Linke Ouyang and Songyang Zhang and Wenwei Zhang and Yining Li and Yang Gao and Peng Sun and Xinyue Zhang and Wei Li and Jingwen Li and Wenhai Wang and Hang Yan and Conghui He and Xingcheng Zhang and Kai Chen and Jifeng Dai and Yu Qiao and Dahua Lin and Jiaqi Wang},
  title         = {InternLM-XComposer-2.5: A Versatile Large Vision Language Model Supporting Long-Contextual Input and Output},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2407.03320},
  eprint        = {2407.03320},
  eprinttype    = {arxiv},
  file          = {:Zhang2024b - InternLM XComposer 2.5_ a Versatile Large Vision Language Model Supporting Long Contextual Input and Output.pdf:PDF:https\://arxiv.org/pdf/2407.03320.pdf},
  groups        = {Models},
  primaryclass  = {cs.CV},
}

@Misc{Dong2024,
  author        = {Xiaoyi Dong and Pan Zhang and Yuhang Zang and Yuhang Cao and Bin Wang and Linke Ouyang and Songyang Zhang and Haodong Duan and Wenwei Zhang and Yining Li and Hang Yan and Yang Gao and Zhe Chen and Xinyue Zhang and Wei Li and Jingwen Li and Wenhai Wang and Kai Chen and Conghui He and Xingcheng Zhang and Jifeng Dai and Yu Qiao and Dahua Lin and Jiaqi Wang},
  title         = {InternLM-XComposer2-4KHD: A Pioneering Large Vision-Language Model Handling Resolutions from 336 Pixels to 4K HD},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2404.06512},
  eprint        = {2404.06512},
  eprinttype    = {arxiv},
  file          = {:Dong2024 - InternLM XComposer2 4KHD_ a Pioneering Large Vision Language Model Handling Resolutions from 336 Pixels to 4K HD.pdf:PDF:http\://arxiv.org/pdf/2404.06512v1},
  groups        = {Models},
  primaryclass  = {cs.CV},
}

@Misc{Kirchenbauer2024,
  author        = {John Kirchenbauer and Jonas Geiping and Yuxin Wen and Manli Shu and Khalid Saifullah and Kezhi Kong and Kasun Fernando and Aniruddha Saha and Micah Goldblum and Tom Goldstein},
  title         = {On the Reliability of Watermarks for Large Language Models},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2306.04634},
  eprint        = {2306.04634},
  eprinttype    = {arxiv},
  file          = {:Kirchenbauer2024 - On the Reliability of Watermarks for Large Language Models.pdf:PDF:http\://arxiv.org/pdf/2306.04634v4},
  primaryclass  = {cs.LG},
}

@Misc{Yang2023,
  author        = {Jianwei Yang and Hao Zhang and Feng Li and Xueyan Zou and Chunyuan Li and Jianfeng Gao},
  title         = {Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V},
  year          = {2023},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2310.11441},
  eprint        = {2310.11441},
  eprinttype    = {arxiv},
  file          = {:Yang2023 - Set of Mark Prompting Unleashes Extraordinary Visual Grounding in GPT 4V.pdf:PDF:https\://arxiv.org/pdf/2310.11441.pdf},
  primaryclass  = {cs.CV},
}

@InProceedings{Jaegle2021,
  author    = {Jaegle, Andrew and Gimeno, Felix and Brock, Andy and Vinyals, Oriol and Zisserman, Andrew and Carreira, Joao},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  title     = {Perceiver: General Perception with Iterative Attention},
  year      = {2021},
  editor    = {Meila, Marina and Zhang, Tong},
  month     = {18--24 Jul},
  pages     = {4651--4664},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {139},
  abstract  = {Biological systems understand the world by simultaneously processing high-dimensional inputs from modalities as diverse as vision, audition, touch, proprioception, etc. The perception models used in deep learning on the other hand are designed for individual modalities, often relying on domain-specific assumptions such as the local grid structures exploited by virtually all existing vision models. These priors introduce helpful inductive biases, but also lock models to individual modalities. In this paper we introduce the Perceiver {–} a model that builds upon Transformers and hence makes few architectural assumptions about the relationship between its inputs, but that also scales to hundreds of thousands of inputs, like ConvNets. The model leverages an asymmetric attention mechanism to iteratively distill inputs into a tight latent bottleneck, allowing it to scale to handle very large inputs. We show that this architecture is competitive with or outperforms strong, specialized models on classification tasks across various modalities: images, point clouds, audio, video and video+audio. The Perceiver obtains performance comparable to ResNet-50 and ViT on ImageNet without 2D convolutions by directly attending to 50,000 pixels. It is also competitive in all modalities in AudioSet.},
  file      = {:Jaegle2021 - Perceiver_ General Perception with Iterative Attention.pdf:PDF},
  pdf       = {http://proceedings.mlr.press/v139/jaegle21a/jaegle21a.pdf},
  url       = {https://proceedings.mlr.press/v139/jaegle21a.html},
}

@InProceedings{Yue2024,
  author    = {Yue, Xiang and Ni, Yuansheng and Zhang, Kai and Zheng, Tianyu and Liu, Ruoqi and Zhang, Ge and Stevens, Samuel and Jiang, Dongfu and Ren, Weiming and Sun, Yuxuan and Wei, Cong and Yu, Botao and Yuan, Ruibin and Sun, Renliang and Yin, Ming and Zheng, Boyuan and Yang, Zhenzhu and Liu, Yibo and Huang, Wenhao and Sun, Huan and Su, Yu and Chen, Wenhu},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI},
  year      = {2024},
  month     = {June},
  pages     = {9556-9567},
  file      = {:Yue2024 - MMMU_ a Massive Multi Discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI.pdf:PDF;:Yue_MMMU_A_Massive_CVPR_2024_supplemental.pdf:PDF},
}

@Misc{Gulati2020,
  author        = {Anmol Gulati and James Qin and Chung-Cheng Chiu and Niki Parmar and Yu Zhang and Jiahui Yu and Wei Han and Shibo Wang and Zhengdong Zhang and Yonghui Wu and Ruoming Pang},
  title         = {Conformer: Convolution-augmented Transformer for Speech Recognition},
  year          = {2020},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2005.08100},
  eprint        = {2005.08100},
  eprinttype    = {arxiv},
  file          = {:Gulati2020 - Conformer_ Convolution Augmented Transformer for Speech Recognition.pdf:PDF:https\://www.isca-archive.org/interspeech_2020/gulati20_interspeech.pdf},
  primaryclass  = {eess.AS},
}

@Article{Fedus2022,
  author  = {William Fedus and Barret Zoph and Noam Shazeer},
  journal = {Journal of Machine Learning Research},
  title   = {Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity},
  year    = {2022},
  number  = {120},
  pages   = {1--39},
  volume  = {23},
  file    = {:Fedus2022 - Switch Transformers_ Scaling to Trillion Parameter Models with Simple and Efficient Sparsity.pdf:PDF},
  url     = {http://jmlr.org/papers/v23/21-0998.html},
}

@InProceedings{Zhou2023a,
  author    = {Zhou, Chunting and Liu, Pengfei and Xu, Puxin and Iyer, Srinivasan and Sun, Jiao and Mao, Yuning and Ma, Xuezhe and Efrat, Avia and Yu, Ping and YU, LILI and Zhang, Susan and Ghosh, Gargi and Lewis, Mike and Zettlemoyer, Luke and Levy, Omer},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {LIMA: Less Is More for Alignment},
  year      = {2023},
  editor    = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
  pages     = {55006--55021},
  publisher = {Curran Associates, Inc.},
  volume    = {36},
  file      = {:Zhou2023a - LIMA_ Less Is More for Alignment.pdf:PDF;:NeurIPS-2023-lima-less-is-more-for-alignment-Supplemental-Conference.pdf:PDF},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2023/file/ac662d74829e4407ce1d126477f4a03a-Paper-Conference.pdf},
}

@Misc{Liu2024c,
  author        = {Wei Liu and Weihao Zeng and Keqing He and Yong Jiang and Junxian He},
  title         = {What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2312.15685},
  eprint        = {2312.15685},
  eprinttype    = {arxiv},
  file          = {:Liu2024c - What Makes Good Data for Alignment_ a Comprehensive Study of Automatic Data Selection in Instruction Tuning.pdf:PDF:http\://arxiv.org/pdf/2312.15685v2},
  primaryclass  = {cs.CL},
}

@Misc{Laurencon2024,
  author        = {Hugo Laurençon and Andrés Marafioti and Victor Sanh and Léo Tronchon},
  title         = {Building and better understanding vision-language models: insights and future directions},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2408.12637},
  eprint        = {2408.12637},
  eprinttype    = {arxiv},
  file          = {:Laurencon2024 - Building and Better Understanding Vision Language Models_ Insights and Future Directions.pdf:PDF:https\://arxiv.org/pdf/2408.12637.pdf},
  primaryclass  = {cs.CV},
}

@Misc{Stojnic2024,
  author        = {Vladan Stojnić and Yannis Kalantidis and Giorgos Tolias},
  title         = {Label Propagation for Zero-shot Classification with Vision-Language Models},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2404.04072},
  eprint        = {2404.04072},
  eprinttype    = {arxiv},
  file          = {:Stojnic2024 - Label Propagation for Zero Shot Classification with Vision Language Models.pdf:PDF:https\://arxiv.org/pdf/2404.04072.pdf},
  primaryclass  = {cs.CV},
}

@Misc{Goyal2018,
  author        = {Priya Goyal and Piotr Dollár and Ross Girshick and Pieter Noordhuis and Lukasz Wesolowski and Aapo Kyrola and Andrew Tulloch and Yangqing Jia and Kaiming He},
  title         = {Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour},
  year          = {2018},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.1706.02677},
  eprint        = {1706.02677},
  eprinttype    = {arxiv},
  file          = {:Goyal2018 - Accurate, Large Minibatch SGD_ Training ImageNet in 1 Hour.pdf:PDF},
  primaryclass  = {cs.CV},
}

@Misc{Zheng2024a,
  author        = {Yaowei Zheng and Richong Zhang and Junhao Zhang and Yanhan Ye and Zheyan Luo and Zhangchi Feng and Yongqiang Ma},
  title         = {LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2403.13372},
  eprint        = {2403.13372},
  eprinttype    = {arxiv},
  file          = {:Zheng2024a - LlamaFactory_ Unified Efficient Fine Tuning of 100+ Language Models.pdf:PDF:https\://arxiv.org/pdf/2403.13372.pdf},
  primaryclass  = {cs.CL},
}

@Misc{Dao2024,
  author        = {Tri Dao and Albert Gu},
  title         = {Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2405.21060},
  eprint        = {2405.21060},
  eprinttype    = {arxiv},
  file          = {:Dao2024 - Transformers Are SSMs_ Generalized Models and Efficient Algorithms through Structured State Space Duality.pdf:PDF:http\://arxiv.org/pdf/2405.21060v1},
  primaryclass  = {cs.LG},
}

@Misc{Gu2022a,
  author        = {Albert Gu and Karan Goel and Christopher Ré},
  title         = {Efficiently Modeling Long Sequences with Structured State Spaces},
  year          = {2022},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2111.00396},
  eprint        = {2111.00396},
  eprinttype    = {arxiv},
  file          = {:Gu2022a - Efficiently Modeling Long Sequences with Structured State Spaces.pdf:PDF:http\://arxiv.org/pdf/2111.00396v3},
  primaryclass  = {cs.LG},
}

@Misc{Gu2021a,
  author        = {Albert Gu and Isys Johnson and Karan Goel and Khaled Saab and Tri Dao and Atri Rudra and Christopher Ré},
  title         = {Combining Recurrent, Convolutional, and Continuous-time Models with Linear State-Space Layers},
  year          = {2021},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2110.13985},
  eprint        = {2110.13985},
  eprinttype    = {arxiv},
  file          = {:Gu2021a - Combining Recurrent, Convolutional, and Continuous Time Models with Linear State Space Layers.pdf:PDF:http\://arxiv.org/pdf/2110.13985v1},
  primaryclass  = {cs.LG},
}

@Misc{Khirodkar2024,
  author        = {Rawal Khirodkar and Timur Bagautdinov and Julieta Martinez and Su Zhaoen and Austin James and Peter Selednik and Stuart Anderson and Shunsuke Saito},
  title         = {Sapiens: Foundation for Human Vision Models},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2408.12569},
  eprint        = {2408.12569},
  eprinttype    = {arxiv},
  file          = {:Khirodkar2024 - Sapiens_ Foundation for Human Vision Models.pdf:PDF:https\://arxiv.org/pdf/2408.12569.pdf},
  primaryclass  = {cs.CV},
}

@Misc{Laurencon2024a,
  author        = {Hugo Laurençon and Léo Tronchon and Matthieu Cord and Victor Sanh},
  title         = {What matters when building vision-language models?},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2405.02246},
  eprint        = {2405.02246},
  eprinttype    = {arxiv},
  file          = {:Laurencon2024a - What Matters When Building Vision Language Models_.pdf:PDF:http\://arxiv.org/pdf/2405.02246v1},
  primaryclass  = {cs.CV},
}

@Misc{Laurencon2023,
  author        = {Hugo Laurençon and Lucile Saulnier and Léo Tronchon and Stas Bekman and Amanpreet Singh and Anton Lozhkov and Thomas Wang and Siddharth Karamcheti and Alexander M. Rush and Douwe Kiela and Matthieu Cord and Victor Sanh},
  title         = {OBELICS: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents},
  year          = {2023},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2306.16527},
  eprint        = {2306.16527},
  eprinttype    = {arxiv},
  file          = {:Laurencon2023 - OBELICS_ an Open Web Scale Filtered Dataset of Interleaved Image Text Documents.pdf:PDF:https\://arxiv.org/pdf/2306.16527.pdf},
  primaryclass  = {cs.IR},
}

@Misc{Lin2023b,
  author        = {Ziyi Lin and Chris Liu and Renrui Zhang and Peng Gao and Longtian Qiu and Han Xiao and Han Qiu and Chen Lin and Wenqi Shao and Keqin Chen and Jiaming Han and Siyuan Huang and Yichi Zhang and Xuming He and Hongsheng Li and Yu Qiao},
  title         = {SPHINX: The Joint Mixing of Weights, Tasks, and Visual Embeddings for Multi-modal Large Language Models},
  year          = {2023},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2311.07575},
  eprint        = {2311.07575},
  eprinttype    = {arxiv},
  file          = {:Lin2023b - SPHINX_ the Joint Mixing of Weights, Tasks, and Visual Embeddings for Multi Modal Large Language Models.pdf:PDF:https\://arxiv.org/pdf/2311.07575.pdf},
  primaryclass  = {cs.CV},
}

@Misc{Team2024b,
  author        = {Gemma Team and Thomas Mesnard and Cassidy Hardin and Robert Dadashi and Surya Bhupatiraju and Shreya Pathak and Laurent Sifre and Morgane Rivière and Mihir Sanjay Kale and Juliette Love and Pouya Tafti and Léonard Hussenot and Pier Giuseppe Sessa and Aakanksha Chowdhery and Adam Roberts and Aditya Barua and Alex Botev and Alex Castro-Ros and Ambrose Slone and Amélie Héliou and Andrea Tacchetti and Anna Bulanova and Antonia Paterson and Beth Tsai and Bobak Shahriari and Charline Le Lan and Christopher A. Choquette-Choo and Clément Crepy and Daniel Cer and Daphne Ippolito and David Reid and Elena Buchatskaya and Eric Ni and Eric Noland and Geng Yan and George Tucker and George-Christian Muraru and Grigory Rozhdestvenskiy and Henryk Michalewski and Ian Tenney and Ivan Grishchenko and Jacob Austin and James Keeling and Jane Labanowski and Jean-Baptiste Lespiau and Jeff Stanway and Jenny Brennan and Jeremy Chen and Johan Ferret and Justin Chiu and Justin Mao-Jones and Katherine Lee and Kathy Yu and Katie Millican and Lars Lowe Sjoesund and Lisa Lee and Lucas Dixon and Machel Reid and Maciej Mikuła and Mateo Wirth and Michael Sharman and Nikolai Chinaev and Nithum Thain and Olivier Bachem and Oscar Chang and Oscar Wahltinez and Paige Bailey and Paul Michel and Petko Yotov and Rahma Chaabouni and Ramona Comanescu and Reena Jana and Rohan Anil and Ross McIlroy and Ruibo Liu and Ryan Mullins and Samuel L Smith and Sebastian Borgeaud and Sertan Girgin and Sholto Douglas and Shree Pandya and Siamak Shakeri and Soham De and Ted Klimenko and Tom Hennigan and Vlad Feinberg and Wojciech Stokowiec and Yu-hui Chen and Zafarali Ahmed and Zhitao Gong and Tris Warkentin and Ludovic Peran and Minh Giang and Clément Farabet and Oriol Vinyals and Jeff Dean and Koray Kavukcuoglu and Demis Hassabis and Zoubin Ghahramani and Douglas Eck and Joelle Barral and Fernando Pereira and Eli Collins and Armand Joulin and Noah Fiedel and Evan Senter and Alek Andreev and Kathleen Kenealy},
  title         = {Gemma: Open Models Based on Gemini Research and Technology},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2403.08295},
  eprint        = {2403.08295},
  eprinttype    = {arxiv},
  file          = {:Team2024b - Gemma_ Open Models Based on Gemini Research and Technology.pdf:PDF:http\://arxiv.org/pdf/2403.08295v4},
  primaryclass  = {cs.CL},
}

@Misc{Team2024c,
  author        = {Gemma Team and Morgane Riviere and Shreya Pathak and Pier Giuseppe Sessa and Cassidy Hardin and Surya Bhupatiraju and Léonard Hussenot and Thomas Mesnard and Bobak Shahriari and Alexandre Ramé and Johan Ferret and Peter Liu and Pouya Tafti and Abe Friesen and Michelle Casbon and Sabela Ramos and Ravin Kumar and Charline Le Lan and Sammy Jerome and Anton Tsitsulin and Nino Vieillard and Piotr Stanczyk and Sertan Girgin and Nikola Momchev and Matt Hoffman and Shantanu Thakoor and Jean-Bastien Grill and Behnam Neyshabur and Olivier Bachem and Alanna Walton and Aliaksei Severyn and Alicia Parrish and Aliya Ahmad and Allen Hutchison and Alvin Abdagic and Amanda Carl and Amy Shen and Andy Brock and Andy Coenen and Anthony Laforge and Antonia Paterson and Ben Bastian and Bilal Piot and Bo Wu and Brandon Royal and Charlie Chen and Chintu Kumar and Chris Perry and Chris Welty and Christopher A. Choquette-Choo and Danila Sinopalnikov and David Weinberger and Dimple Vijaykumar and Dominika Rogozińska and Dustin Herbison and Elisa Bandy and Emma Wang and Eric Noland and Erica Moreira and Evan Senter and Evgenii Eltyshev and Francesco Visin and Gabriel Rasskin and Gary Wei and Glenn Cameron and Gus Martins and Hadi Hashemi and Hanna Klimczak-Plucińska and Harleen Batra and Harsh Dhand and Ivan Nardini and Jacinda Mein and Jack Zhou and James Svensson and Jeff Stanway and Jetha Chan and Jin Peng Zhou and Joana Carrasqueira and Joana Iljazi and Jocelyn Becker and Joe Fernandez and Joost van Amersfoort and Josh Gordon and Josh Lipschultz and Josh Newlan and Ju-yeong Ji and Kareem Mohamed and Kartikeya Badola and Kat Black and Katie Millican and Keelin McDonell and Kelvin Nguyen and Kiranbir Sodhia and Kish Greene and Lars Lowe Sjoesund and Lauren Usui and Laurent Sifre and Lena Heuermann and Leticia Lago and Lilly McNealus and Livio Baldini Soares and Logan Kilpatrick and Lucas Dixon and Luciano Martins and Machel Reid and Manvinder Singh and Mark Iverson and Martin Görner and Mat Velloso and Mateo Wirth and Matt Davidow and Matt Miller and Matthew Rahtz and Matthew Watson and Meg Risdal and Mehran Kazemi and Michael Moynihan and Ming Zhang and Minsuk Kahng and Minwoo Park and Mofi Rahman and Mohit Khatwani and Natalie Dao and Nenshad Bardoliwalla and Nesh Devanathan and Neta Dumai and Nilay Chauhan and Oscar Wahltinez and Pankil Botarda and Parker Barnes and Paul Barham and Paul Michel and Pengchong Jin and Petko Georgiev and Phil Culliton and Pradeep Kuppala and Ramona Comanescu and Ramona Merhej and Reena Jana and Reza Ardeshir Rokni and Rishabh Agarwal and Ryan Mullins and Samaneh Saadat and Sara Mc Carthy and Sarah Perrin and Sébastien M. R. Arnold and Sebastian Krause and Shengyang Dai and Shruti Garg and Shruti Sheth and Sue Ronstrom and Susan Chan and Timothy Jordan and Ting Yu and Tom Eccles and Tom Hennigan and Tomas Kocisky and Tulsee Doshi and Vihan Jain and Vikas Yadav and Vilobh Meshram and Vishal Dharmadhikari and Warren Barkley and Wei Wei and Wenming Ye and Woohyun Han and Woosuk Kwon and Xiang Xu and Zhe Shen and Zhitao Gong and Zichuan Wei and Victor Cotruta and Phoebe Kirk and Anand Rao and Minh Giang and Ludovic Peran and Tris Warkentin and Eli Collins and Joelle Barral and Zoubin Ghahramani and Raia Hadsell and D. Sculley and Jeanine Banks and Anca Dragan and Slav Petrov and Oriol Vinyals and Jeff Dean and Demis Hassabis and Koray Kavukcuoglu and Clement Farabet and Elena Buchatskaya and Sebastian Borgeaud and Noah Fiedel and Armand Joulin and Kathleen Kenealy and Robert Dadashi and Alek Andreev},
  title         = {Gemma 2: Improving Open Language Models at a Practical Size},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2408.00118},
  eprint        = {2408.00118},
  eprinttype    = {arxiv},
  file          = {:Team2024c - Gemma 2_ Improving Open Language Models at a Practical Size.pdf:PDF:http\://arxiv.org/pdf/2408.00118v2},
  primaryclass  = {cs.CL},
}

@InProceedings{Cha2024,
  author    = {Cha, Junbum and Kang, Wooyoung and Mun, Jonghwan and Roh, Byungseok},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Honeybee: Locality-enhanced Projector for Multimodal LLM},
  year      = {2024},
  month     = {June},
  pages     = {13817-13827},
  file      = {:Cha2024 - Honeybee_ Locality Enhanced Projector for Multimodal LLM.pdf:PDF;:Cha_Honeybee_Locality-enhanced_Projector_CVPR_2024_supplemental.pdf:PDF},
}

@Misc{Lee2024,
  author        = {Byung-Kwan Lee and Chae Won Kim and Beomchan Park and Yong Man Ro},
  title         = {Meteor: Mamba-based Traversal of Rationale for Large Language and Vision Models},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2405.15574},
  eprint        = {2405.15574},
  eprinttype    = {arxiv},
  file          = {:Lee2024 - Meteor_ Mamba Based Traversal of Rationale for Large Language and Vision Models.pdf:PDF:https\://arxiv.org/pdf/2405.15574.pdf},
  primaryclass  = {cs.CV},
}

@Misc{Yu2024,
  author        = {Tianyu Yu and Haoye Zhang and Yuan Yao and Yunkai Dang and Da Chen and Xiaoman Lu and Ganqu Cui and Taiwen He and Zhiyuan Liu and Tat-Seng Chua and Maosong Sun},
  title         = {RLAIF-V: Aligning MLLMs through Open-Source AI Feedback for Super GPT-4V Trustworthiness},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2405.17220},
  eprint        = {2405.17220},
  eprinttype    = {arxiv},
  file          = {:Yu2024 - RLAIF V_ Aligning MLLMs through Open Source AI Feedback for Super GPT 4V Trustworthiness.pdf:PDF:https\://arxiv.org/pdf/2405.17220.pdf},
  primaryclass  = {cs.CL},
}

@Article{Manas2024,
  author       = {Mañas, Oscar and Krojer, Benno and Agrawal, Aishwarya},
  journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
  title        = {Improving Automatic VQA Evaluation Using Large Language Models},
  year         = {2024},
  month        = {Mar.},
  number       = {5},
  pages        = {4171-4179},
  volume       = {38},
  abstractnote = {8 years after the visual question answering (VQA) task was proposed, accuracy remains the primary metric for automatic evaluation. VQA Accuracy has been effective so far in the IID evaluation setting. However, our community is undergoing a shift towards open-ended generative models and OOD evaluation. In this new paradigm, the existing VQA Accuracy metric is overly stringent and underestimates the performance of VQA systems. Thus, there is a need to develop more robust automatic VQA metrics that serve as a proxy for human judgment. In this work, we propose to leverage the in-context learning capabilities of instruction-tuned large language models (LLMs) to build a better VQA metric. We formulate VQA evaluation as an answer-rating task where the LLM is instructed to score the accuracy of a candidate answer given a set of reference answers. We demonstrate the proposed metric better correlates with human judgment compared to existing metrics across several VQA models and benchmarks. We hope wide adoption of our metric will contribute to better estimating the research progress on the VQA task. We plan to release the evaluation code and collected human judgments.},
  doi          = {10.1609/aaai.v38i5.28212},
  file         = {:Manas2024 - Improving Automatic VQA Evaluation Using Large Language Models.pdf:PDF},
  url          = {https://ojs.aaai.org/index.php/AAAI/article/view/28212},
}

@Misc{Liu2024d,
  author        = {Shih-Yang Liu and Chien-Yi Wang and Hongxu Yin and Pavlo Molchanov and Yu-Chiang Frank Wang and Kwang-Ting Cheng and Min-Hung Chen},
  title         = {DoRA: Weight-Decomposed Low-Rank Adaptation},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2402.09353},
  eprint        = {2402.09353},
  eprinttype    = {arxiv},
  file          = {:Liu2024d - DoRA_ Weight Decomposed Low Rank Adaptation.pdf:PDF:https\://arxiv.org/pdf/2402.09353.pdf},
  primaryclass  = {cs.CL},
}

@Misc{Jain2023,
  author        = {Neel Jain and Ping-yeh Chiang and Yuxin Wen and John Kirchenbauer and Hong-Min Chu and Gowthami Somepalli and Brian R. Bartoldson and Bhavya Kailkhura and Avi Schwarzschild and Aniruddha Saha and Micah Goldblum and Jonas Geiping and Tom Goldstein},
  title         = {NEFTune: Noisy Embeddings Improve Instruction Finetuning},
  year          = {2023},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2310.05914},
  eprint        = {2310.05914},
  eprinttype    = {arxiv},
  file          = {:Jain2023 - NEFTune_ Noisy Embeddings Improve Instruction Finetuning.pdf:PDF:http\://arxiv.org/pdf/2310.05914v2},
  primaryclass  = {cs.CL},
}

@Misc{ZhiXuan2024,
  author        = {Tan Zhi-Xuan and Micah Carroll and Matija Franklin and Hal Ashton},
  title         = {Beyond Preferences in AI Alignment},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2408.16984},
  eprint        = {2408.16984},
  eprinttype    = {arxiv},
  file          = {:ZhiXuan2024 - Beyond Preferences in AI Alignment.pdf:PDF:http\://arxiv.org/pdf/2408.16984v1},
  primaryclass  = {cs.AI},
}

@Misc{Schnake2024,
  author        = {Thomas Schnake and Farnoush Rezaei Jafaria and Jonas Lederer and Ping Xiong and Shinichi Nakajima and Stefan Gugler and Grégoire Montavon and Klaus-Robert Müller},
  title         = {Towards Symbolic XAI -- Explanation Through Human Understandable Logical Relationships Between Features},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2408.17198},
  eprint        = {2408.17198},
  eprinttype    = {arxiv},
  file          = {:Schnake2024 - Towards Symbolic XAI Explanation through Human Understandable Logical Relationships between Features.pdf:PDF:http\://arxiv.org/pdf/2408.17198v1},
  primaryclass  = {cs.AI},
}

@Misc{Lee2024a,
  author        = {Rhui Dih Lee and Laura Wynter and Raghu Kiran Ganti},
  title         = {Flexible and Effective Mixing of Large Language Models into a Mixture of Domain Experts},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2408.17280},
  eprint        = {2408.17280},
  eprinttype    = {arxiv},
  file          = {:Lee2024a - Flexible and Effective Mixing of Large Language Models into a Mixture of Domain Experts.pdf:PDF:http\://arxiv.org/pdf/2408.17280v1},
  primaryclass  = {cs.AI},
}

@Misc{Wan2024,
  author        = {Guangya Wan and Yuqi Wu and Jie Chen and Sheng Li},
  title         = {Dynamic Self-Consistency: Leveraging Reasoning Paths for Efficient LLM Sampling},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2408.17017},
  eprint        = {2408.17017},
  eprinttype    = {arxiv},
  file          = {:Wan2024 - Dynamic Self Consistency_ Leveraging Reasoning Paths for Efficient LLM Sampling.pdf:PDF:http\://arxiv.org/pdf/2408.17017v1},
  primaryclass  = {cs.CL},
}

@Misc{Niu2024,
  author        = {Minxue Niu and Mimansa Jaiswal and Emily Mower Provost},
  title         = {From Text to Emotion: Unveiling the Emotion Annotation Capabilities of LLMs},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2408.17026},
  eprint        = {2408.17026},
  eprinttype    = {arxiv},
  file          = {:Niu2024 - From Text to Emotion_ Unveiling the Emotion Annotation Capabilities of LLMs.pdf:PDF:https\://www.isca-archive.org/interspeech_2024/niu24d_interspeech.pdf},
  primaryclass  = {cs.CL},
}

@Misc{Wang2024,
  author        = {Yujing Wang and Hainan Zhang and Liang Pang and Liang Pang and Hongwei Zheng and Zhiming Zheng},
  title         = {MaFeRw: Query Rewriting with Multi-Aspect Feedbacks for Retrieval-Augmented Large Language Models},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2408.17072},
  eprint        = {2408.17072},
  eprinttype    = {arxiv},
  file          = {:Wang2024 - MaFeRw_ Query Rewriting with Multi Aspect Feedbacks for Retrieval Augmented Large Language Models.pdf:PDF:http\://arxiv.org/pdf/2408.17072v1},
  primaryclass  = {cs.CL},
}

@Misc{Xu2024b,
  author        = {Shaojun Xu and Xiaohui Ye and Mengqi Zhang and Pei Wang},
  title         = {Impact of ChatGPT on the writing style of condensed matter physicists},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2408.17325},
  eprint        = {2408.17325},
  eprinttype    = {arxiv},
  file          = {:Xu2024b - Impact of ChatGPT on the Writing Style of Condensed Matter Physicists.pdf:PDF:http\://arxiv.org/pdf/2408.17325v1},
  primaryclass  = {cs.CL},
}

@Misc{Srivatsan2024,
  author        = {Koushik Srivatsan and Fahad Shamshad and Muzammal Naseer and Karthik Nandakumar},
  title         = {STEREO: Towards Adversarially Robust Concept Erasing from Text-to-Image Generation Models},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2408.16807},
  eprint        = {2408.16807},
  eprinttype    = {arxiv},
  file          = {:Srivatsan2024 - STEREO_ Towards Adversarially Robust Concept Erasing from Text to Image Generation Models.pdf:PDF:http\://arxiv.org/pdf/2408.16807v1},
  primaryclass  = {cs.CV},
}

@Misc{Moratelli2024,
  author        = {Nicholas Moratelli and Marcella Cornia and Lorenzo Baraldi and Rita Cucchiara},
  title         = {Fluent and Accurate Image Captioning with a Self-Trained Reward Model},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2408.16827},
  eprint        = {2408.16827},
  eprinttype    = {arxiv},
  file          = {:Moratelli2024 - Fluent and Accurate Image Captioning with a Self Trained Reward Model.pdf:PDF:http\://arxiv.org/pdf/2408.16827v1},
  primaryclass  = {cs.CV},
}

@Misc{Kouzelis2024,
  author        = {Theodoros Kouzelis and Manos Plitsis and Mihalis A. Nikolaou and Yannis Panagakis},
  title         = {Enabling Local Editing in Diffusion Models by Joint and Individual Component Analysis},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2408.16845},
  eprint        = {2408.16845},
  eprinttype    = {arxiv},
  file          = {:Kouzelis2024 - Enabling Local Editing in Diffusion Models by Joint and Individual Component Analysis.pdf:PDF:http\://arxiv.org/pdf/2408.16845v1},
  primaryclass  = {cs.CV},
}

@Misc{Wang2024a,
  author        = {Yonghui Wang and Wengang Zhou and Hao Feng and Houqiang Li},
  title         = {AdaptVision: Dynamic Input Scaling in MLLMs for Versatile Scene Understanding},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2408.16986},
  eprint        = {2408.16986},
  eprinttype    = {arxiv},
  file          = {:Wang2024a - AdaptVision_ Dynamic Input Scaling in MLLMs for Versatile Scene Understanding.pdf:PDF:http\://arxiv.org/pdf/2408.16986v1},
  primaryclass  = {cs.CV},
}

@Misc{Ganz2024,
  author        = {Roy Ganz and Michael Elad},
  title         = {Text-to-Image Generation Via Energy-Based CLIP},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2408.17046},
  eprint        = {2408.17046},
  eprinttype    = {arxiv},
  file          = {:Ganz2024 - Text to Image Generation Via Energy Based CLIP.pdf:PDF:http\://arxiv.org/pdf/2408.17046v1},
  primaryclass  = {cs.CV},
}

@Misc{Qu2024,
  author        = {Xiaoye Qu and Jiashuo Sun and Wei Wei and Yu Cheng},
  title         = {Look, Compare, Decide: Alleviating Hallucination in Large Vision-Language Models via Multi-View Multi-Path Reasoning},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2408.17150},
  eprint        = {2408.17150},
  eprinttype    = {arxiv},
  file          = {:Qu2024 - Look, Compare, Decide_ Alleviating Hallucination in Large Vision Language Models Via Multi View Multi Path Reasoning.pdf:PDF:http\://arxiv.org/pdf/2408.17150v1},
  primaryclass  = {cs.CV},
}

@Misc{Guan2024a,
  author        = {Runwei Guan and Jianan Liu and Liye Jia and Haocheng Zhao and Shanliang Yao and Xiaohui Zhu and Ka Lok Man and Eng Gee Lim and Jeremy Smith and Yutao Yue},
  title         = {NanoMVG: USV-Centric Low-Power Multi-Task Visual Grounding based on Prompt-Guided Camera and 4D mmWave Radar},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2408.17207},
  eprint        = {2408.17207},
  eprinttype    = {arxiv},
  file          = {:Guan2024a - NanoMVG_ USV Centric Low Power Multi Task Visual Grounding Based on Prompt Guided Camera and 4D MmWave Radar.pdf:PDF:http\://arxiv.org/pdf/2408.17207v1},
  primaryclass  = {cs.CV},
}

@Misc{Li2024b,
  author        = {Shen Li and Liuyi Yao and Lan Zhang and Yaliang Li},
  title         = {Safety Layers of Aligned Large Language Models: The Key to LLM Security},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2408.17003},
  eprint        = {2408.17003},
  eprinttype    = {arxiv},
  file          = {:Li2024b - Safety Layers of Aligned Large Language Models_ the Key to LLM Security.pdf:PDF:http\://arxiv.org/pdf/2408.17003v1},
  primaryclass  = {cs.CR},
}

@Misc{Clavie2024,
  author        = {Benjamin Clavié},
  title         = {rerankers: A Lightweight Python Library to Unify Ranking Methods},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2408.17344},
  eprint        = {2408.17344},
  eprinttype    = {arxiv},
  file          = {:Clavie2024 - Rerankers_ a Lightweight Python Library to Unify Ranking Methods.pdf:PDF:http\://arxiv.org/pdf/2408.17344v1},
  primaryclass  = {cs.IR},
}

@InProceedings{Rudinger2015,
  author    = {Rudinger, Rachel and Demberg, Vera and Modi, Ashutosh and Van Durme, Benjamin and Pinkal, Manfred},
  booktitle = {Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics},
  title     = {Learning to predict script events from domain-specific text},
  year      = {2015},
  address   = {Denver, Colorado},
  editor    = {Palmer, Martha and Boleda, Gemma and Rosso, Paolo},
  month     = jun,
  pages     = {205--210},
  publisher = {Association for Computational Linguistics},
  doi       = {10.18653/v1/S15-1024},
  file      = {:Rudinger2015 - Learning to Predict Script Events from Domain Specific Text.pdf:PDF},
  url       = {https://aclanthology.org/S15-1024},
}

@Misc{Zhang2022a,
  author        = {Zhuosheng Zhang and Shuohang Wang and Yichong Xu and Yuwei Fang and Wenhao Yu and Yang Liu and Hai Zhao and Chenguang Zhu and Michael Zeng},
  title         = {Task Compass: Scaling Multi-task Pre-training with Task Prefix},
  year          = {2022},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2210.06277},
  eprint        = {2210.06277},
  eprinttype    = {arxiv},
  file          = {:Zhang2022a - Task Compass_ Scaling Multi Task Pre Training with Task Prefix.pdf:PDF:https\://aclanthology.org/2022.findings-emnlp.416.pdf},
  primaryclass  = {cs.CL},
}

@Article{Mersha2024,
  author    = {Mersha, Melkamu and Lam, Khang and Wood, Joseph and AlShami, Ali K. and Kalita, Jugal},
  journal   = {Neurocomputing},
  title     = {Explainable artificial intelligence: A survey of needs, techniques, applications, and future direction},
  year      = {2024},
  issn      = {0925-2312},
  month     = sep,
  pages     = {128111},
  volume    = {599},
  doi       = {10.1016/j.neucom.2024.128111},
  file      = {:Mersha2024 - Explainable Artificial Intelligence_ a Survey of Needs, Techniques, Applications, and Future Direction.pdf:PDF},
  publisher = {Elsevier BV},
}

@Misc{Nagar2024,
  author        = {Aishik Nagar and Shantanu Jaiswal and Cheston Tan},
  title         = {Zero-Shot Visual Reasoning by Vision-Language Models: Benchmarking and Analysis},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2409.00106},
  eprint        = {2409.00106},
  eprinttype    = {arxiv},
  file          = {:Nagar2024 - Zero Shot Visual Reasoning by Vision Language Models_ Benchmarking and Analysis.pdf:PDF:http\://arxiv.org/pdf/2409.00106v1},
  primaryclass  = {cs.CL},
}

@Misc{Goral2024,
  author        = {Gracjan Góral and Emilia Wiśnios},
  title         = {When All Options Are Wrong: Evaluating Large Language Model Robustness with Incorrect Multiple-Choice Options},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2409.00113},
  eprint        = {2409.00113},
  eprinttype    = {arxiv},
  file          = {:Goral2024 - When All Options Are Wrong_ Evaluating Large Language Model Robustness with Incorrect Multiple Choice Options.pdf:PDF:http\://arxiv.org/pdf/2409.00113v1},
  primaryclass  = {cs.CL},
}

@Misc{Zhou2024a,
  author        = {Jiayi Zhou and Jiaming Ji and Juntao Dai and Yaodong Yang},
  title         = {Sequence to Sequence Reward Modeling: Improving RLHF by Language Feedback},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2409.00162},
  eprint        = {2409.00162},
  eprinttype    = {arxiv},
  file          = {:Zhou2024a - Sequence to Sequence Reward Modeling_ Improving RLHF by Language Feedback.pdf:PDF:http\://arxiv.org/pdf/2409.00162v1},
  primaryclass  = {cs.CL},
}

@Misc{Zhang2024c,
  author        = {Wei Zhang and Chaoqun Wan and Yonggang Zhang and Yiu-ming Cheung and Xinmei Tian and Xu Shen and Jieping Ye},
  title         = {Interpreting and Improving Large Language Models in Arithmetic Calculation},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2409.01659},
  eprint        = {2409.01659},
  eprinttype    = {arxiv},
  file          = {:Zhang2024c - Interpreting and Improving Large Language Models in Arithmetic Calculation.pdf:PDF:http\://arxiv.org/pdf/2409.01659v1},
  primaryclass  = {cs.CL},
}

@Misc{Xie2024,
  author        = {Zhifei Xie and Changqiao Wu},
  title         = {Mini-Omni: Language Models Can Hear, Talk While Thinking in Streaming},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2408.16725},
  eprint        = {2408.16725},
  eprinttype    = {arxiv},
  file          = {:Xie2024 - Mini Omni_ Language Models Can Hear, Talk While Thinking in Streaming.pdf:PDF:http\://arxiv.org/pdf/2408.16725v2},
  primaryclass  = {cs.AI},
}

@Misc{Wei2024,
  author        = {Yuxiang Wei and Hojae Han and Rajhans Samdani},
  title         = {Arctic-SnowCoder: Demystifying High-Quality Data in Code Pretraining},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2409.02326},
  eprint        = {2409.02326},
  eprinttype    = {arxiv},
  file          = {:Wei2024 - Arctic SnowCoder_ Demystifying High Quality Data in Code Pretraining.pdf:PDF:http\://arxiv.org/pdf/2409.02326v1},
  primaryclass  = {cs.CL},
}

@Misc{Xu2024c,
  author        = {Zhe Xu and Jiasheng Ye and Xiangyang Liu and Tianxiang Sun and Xiaoran Liu and Qipeng Guo and Linlin Li and Qun Liu and Xuanjing Huang and Xipeng Qiu},
  title         = {DetectiveQA: Evaluating Long-Context Reasoning on Detective Novels},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2409.02465},
  eprint        = {2409.02465},
  eprinttype    = {arxiv},
  file          = {:Xu2024c - DetectiveQA_ Evaluating Long Context Reasoning on Detective Novels.pdf:PDF:http\://arxiv.org/pdf/2409.02465v1},
  primaryclass  = {cs.CL},
}

@Misc{Yue2024a,
  author        = {Xiang Yue and Tianyu Zheng and Yuansheng Ni and Yubo Wang and Kai Zhang and Shengbang Tong and Yuxuan Sun and Ming Yin and Botao Yu and Ge Zhang and Huan Sun and Yu Su and Wenhu Chen and Graham Neubig},
  title         = {MMMU-Pro: A More Robust Multi-discipline Multimodal Understanding Benchmark},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2409.02813},
  eprint        = {2409.02813},
  eprinttype    = {arxiv},
  file          = {:Yue2024a - MMMU Pro_ a More Robust Multi Discipline Multimodal Understanding Benchmark.pdf:PDF:http\://arxiv.org/pdf/2409.02813v1},
  primaryclass  = {cs.CL},
}

@Misc{Wang2024b,
  author        = {Xidong Wang and Dingjie Song and Shunian Chen and Chen Zhang and Benyou Wang},
  title         = {LongLLaVA: Scaling Multi-modal LLMs to 1000 Images Efficiently via Hybrid Architecture},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2409.02889},
  eprint        = {2409.02889},
  eprinttype    = {arxiv},
  file          = {:Wang2024b - LongLLaVA_ Scaling Multi Modal LLMs to 1000 Images Efficiently Via Hybrid Architecture.pdf:PDF:http\://arxiv.org/pdf/2409.02889v1},
  primaryclass  = {cs.CL},
}

@Misc{Zhang2024d,
  author        = {jiajie Zhang and Yushi Bai and Xin Lv and Wanjun Gu and Danqing Liu and Minhao Zou and Shulin Cao and Lei Hou and Yuxiao Dong and Ling Feng and Juanzi Li},
  title         = {LongCite: Enabling LLMs to Generate Fine-grained Citations in Long-context QA},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2409.02897},
  eprint        = {2409.02897},
  eprinttype    = {arxiv},
  file          = {:Zhang2024d - LongCite_ Enabling LLMs to Generate Fine Grained Citations in Long Context QA.pdf:PDF:http\://arxiv.org/pdf/2409.02897v1},
  primaryclass  = {cs.CL},
}

@Misc{Li2024c,
  author        = {Bo Li and Yuanhan Zhang and Dong Guo and Renrui Zhang and Feng Li and Hao Zhang and Kaichen Zhang and Yanwei Li and Ziwei Liu and Chunyuan Li},
  title         = {LLaVA-OneVision: Easy Visual Task Transfer},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2408.03326},
  eprint        = {2408.03326},
  eprinttype    = {arxiv},
  file          = {:Li2024c - LLaVA OneVision_ Easy Visual Task Transfer.pdf:PDF:https\://arxiv.org/pdf/2408.03326.pdf},
  primaryclass  = {cs.CV},
}

@Misc{Zhu2024a,
  author        = {Lianghui Zhu and Bencheng Liao and Qian Zhang and Xinlong Wang and Wenyu Liu and Xinggang Wang},
  title         = {Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2401.09417},
  eprint        = {2401.09417},
  eprinttype    = {arxiv},
  file          = {:Zhu2024a - Vision Mamba_ Efficient Visual Representation Learning with Bidirectional State Space Model.pdf:PDF:https\://arxiv.org/pdf/2401.09417.pdf},
  primaryclass  = {cs.CV},
}

@Misc{Zhao2024,
  author        = {Han Zhao and Min Zhang and Wei Zhao and Pengxiang Ding and Siteng Huang and Donglin Wang},
  title         = {Cobra: Extending Mamba to Multi-Modal Large Language Model for Efficient Inference},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2403.14520},
  eprint        = {2403.14520},
  eprinttype    = {arxiv},
  file          = {:Zhao2024 - Cobra_ Extending Mamba to Multi Modal Large Language Model for Efficient Inference.pdf:PDF:https\://arxiv.org/pdf/2403.14520.pdf},
  primaryclass  = {cs.CV},
}

@Misc{Qiao2024,
  author        = {Yanyuan Qiao and Zheng Yu and Longteng Guo and Sihan Chen and Zijia Zhao and Mingzhen Sun and Qi Wu and Jing Liu},
  title         = {VL-Mamba: Exploring State Space Models for Multimodal Learning},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2403.13600},
  eprint        = {2403.13600},
  eprinttype    = {arxiv},
  file          = {:Qiao2024 - VL Mamba_ Exploring State Space Models for Multimodal Learning.pdf:PDF:http\://arxiv.org/pdf/2403.13600v1},
  primaryclass  = {cs.CV},
}

@Misc{Prakriya2024,
  author        = {Neha Prakriya and Jui-Nan Yen and Cho-Jui Hsieh and Jason Cong},
  title         = {Accelerating Large Language Model Pretraining via LFR Pedagogy: Learn, Focus, and Review},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2409.06131},
  eprint        = {2409.06131},
  eprinttype    = {arxiv},
  file          = {:Prakriya2024 - Accelerating Large Language Model Pretraining Via LFR Pedagogy_ Learn, Focus, and Review.pdf:PDF:http\://arxiv.org/pdf/2409.06131v1},
  primaryclass  = {cs.CL},
}

@Misc{Bui2024,
  author        = {Nhat-Tan Bui and Dinh-Hieu Hoang and Quoc-Huy Trinh and Minh-Triet Tran and Truong Nguyen and Susan Gauch},
  title         = {NeIn: Telling What You Don't Want},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2409.06481},
  eprint        = {2409.06481},
  eprinttype    = {arxiv},
  file          = {:Bui2024 - NeIn_ Telling What You Don't Want.pdf:PDF:http\://arxiv.org/pdf/2409.06481v1},
  primaryclass  = {cs.CV},
}

@Misc{Dai2024,
  author        = {Ruiting Dai and Yuqiao Tan and Lisi Mo and Tao He and Ke Qin and Shuang Liang},
  title         = {MuAP: Multi-step Adaptive Prompt Learning for Vision-Language Model with Missing Modality},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2409.04693},
  eprint        = {2409.04693},
  eprinttype    = {arxiv},
  file          = {:Dai2024 - MuAP_ Multi Step Adaptive Prompt Learning for Vision Language Model with Missing Modality.pdf:PDF:https\://arxiv.org/pdf/2409.04693.pdf},
  primaryclass  = {cs.AI},
}

@Article{OpenAI2024a,
  author = {OpenAI},
  title  = {OpenAI o1 System Card},
  year   = {2024},
  month  = sep,
  file   = {:OpenAI2024a - OpenAI O1 System Card.pdf:PDF},
  url    = {https://openai.com/index/openai-o1-system-card/},
}

@Misc{Zhang2024e,
  author        = {Xiang Zhang and Muhammad Abdul-Mageed and Laks V. S. Lakshmanan},
  title         = {Autoregressive + Chain of Thought (CoT) $\simeq$ Recurrent: Recurrence's Role in Language Models and a Revist of Recurrent Transformer},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2409.09239},
  eprint        = {2409.09239},
  eprinttype    = {arxiv},
  file          = {:Zhang2024e - Autoregressive + Chain of Thought (CoT) $$ Recurrent_ Recurrence's Role in Language Models and a Revist of Recurrent Transformer.pdf:PDF:http\://arxiv.org/pdf/2409.09239v1},
  primaryclass  = {cs.CL},
}

@Misc{Lv2024,
  author        = {Ang Lv and Ruobing Xie and Xingwu Sun and Zhanhui Kang and Rui Yan},
  title         = {Language Models "Grok" to Copy},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2409.09281},
  eprint        = {2409.09281},
  eprinttype    = {arxiv},
  file          = {:Lv2024 - Language Models _Grok_ to Copy.pdf:PDF:http\://arxiv.org/pdf/2409.09281v1},
  primaryclass  = {cs.CL},
}

@Misc{Gong2024,
  author        = {Dongyu Gong and Hantao Zhang},
  title         = {Self-Attention Limits Working Memory Capacity of Transformer-Based Models},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2409.10715},
  eprint        = {2409.10715},
  eprinttype    = {arxiv},
  file          = {:Gong2024 - Self Attention Limits Working Memory Capacity of Transformer Based Models.pdf:PDF:http\://arxiv.org/pdf/2409.10715v1},
  primaryclass  = {cs.CL},
}

@Misc{Power2022,
  author        = {Alethea Power and Yuri Burda and Harri Edwards and Igor Babuschkin and Vedant Misra},
  title         = {Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets},
  year          = {2022},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2201.02177},
  eprint        = {2201.02177},
  eprinttype    = {arxiv},
  file          = {:Power2022 - Grokking_ Generalization beyond Overfitting on Small Algorithmic Datasets.pdf:PDF:http\://arxiv.org/pdf/2201.02177v1},
  primaryclass  = {cs.LG},
}

@Misc{Wang2024c,
  author        = {Boshi Wang and Xiang Yue and Yu Su and Huan Sun},
  title         = {Grokked Transformers are Implicit Reasoners: A Mechanistic Journey to the Edge of Generalization},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2405.15071},
  eprint        = {2405.15071},
  eprinttype    = {arxiv},
  file          = {:Wang2024c - Grokked Transformers Are Implicit Reasoners_ a Mechanistic Journey to the Edge of Generalization.pdf:PDF:https\://arxiv.org/pdf/2405.15071.pdf},
  primaryclass  = {cs.CL},
}

@InProceedings{Arriaga2024,
  author    = {Arriaga, Octavio and Guo, Jichen and Adam, Rebecca and Houben, Sebastian and Kirchner, Frank},
  booktitle = {Neural-Symbolic Learning and Reasoning},
  title     = {Bayesian Inverse Graphics for Few-Shot Concept Learning},
  year      = {2024},
  address   = {Cham},
  editor    = {Besold, Tarek R. and d'Avila Garcez, Artur and Jimenez-Ruiz, Ernesto and Confalonieri, Roberto and Madhyastha, Pranava and Wagner, Benedikt},
  pages     = {141--165},
  publisher = {Springer Nature Switzerland},
  abstract  = {Humans excel at building generalizations of new concepts from just one single example. Contrary to this, current computer vision models typically require large amount of training samples to achieve a comparable accuracy. In this work we present a Bayesian model of perception that learns using only minimal data, a prototypical probabilistic program of an object. Specifically, we propose a generative inverse graphics model of primitive shapes, to infer posterior distributions over physically consistent parameters from one or several images. We show how this representation can be used for downstream tasks such as few-shot classification and pose estimation. Our model outperforms existing few-shot neural-only classification algorithms and demonstrates generalization across varying lighting conditions, backgrounds, and out-of-distribution shapes. By design, our model is uncertainty-aware and uses our new differentiable renderer for optimizing global scene parameters through gradient descent, sampling posterior distributions over object parameters with Markov Chain Monte Carlo (MCMC), and using a neural based likelihood function. The code and datasets are available at github.com/oarriaga/bayesian-inverse-graphics).},
  file      = {:Arriaga2024 - Bayesian Inverse Graphics For Few Shot Concept Learning.pdf:PDF},
  isbn      = {978-3-031-71167-1},
}

@Misc{Jain2024,
  author        = {Abhinav Jain and Chris Jermaine and Vaibhav Unhelkar},
  title         = {RAG-Modulo: Solving Sequential Tasks using Experience, Critics, and Language Models},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2409.12294},
  eprint        = {2409.12294},
  eprinttype    = {arxiv},
  file          = {:Jain2024 - RAG Modulo_ Solving Sequential Tasks Using Experience, Critics, and Language Models.pdf:PDF:http\://arxiv.org/pdf/2409.12294v1},
  primaryclass  = {cs.AI},
}

@Misc{Shang2024,
  author        = {Yuzhang Shang and Bingxin Xu and Weitai Kang and Mu Cai and Yuheng Li and Zehao Wen and Zhen Dong and Kurt Keutzer and Yong Jae Lee and Yan Yan},
  title         = {Interpolating Video-LLMs: Toward Longer-sequence LMMs in a Training-free Manner},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2409.12963},
  eprint        = {2409.12963},
  eprinttype    = {arxiv},
  file          = {:Shang2024 - Interpolating Video LLMs_ toward Longer Sequence LMMs in a Training Free Manner.pdf:PDF:http\://arxiv.org/pdf/2409.12963v1},
  primaryclass  = {cs.CV},
}

@Misc{Sun2024a,
  author        = {Manxi Sun and Wei Liu and Jian Luan and Pengzhi Gao and Bin Wang},
  title         = {Mixture of Diverse Size Experts},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2409.12210},
  eprint        = {2409.12210},
  eprinttype    = {arxiv},
  file          = {:Sun2024a - Mixture of Diverse Size Experts.pdf:PDF:http\://arxiv.org/pdf/2409.12210v1},
  primaryclass  = {cs.LG},
}

@Misc{Monroe2024,
  author        = {Daniel Monroe and The Leela Chess Zero Team},
  title         = {Mastering Chess with a Transformer Model},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2409.12272},
  eprint        = {2409.12272},
  eprinttype    = {arxiv},
  file          = {:Monroe2024 - Mastering Chess with a Transformer Model.pdf:PDF:http\://arxiv.org/pdf/2409.12272v1},
  primaryclass  = {cs.LG},
}

@Misc{Velayuthan2024,
  author        = {Menan Velayuthan and Kengatharaiyer Sarveswaran},
  title         = {Egalitarian Language Representation in Language Models: It All Begins with Tokenizers},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2409.11501},
  eprint        = {2409.11501},
  eprinttype    = {arxiv},
  file          = {:Velayuthan2024 - Egalitarian Language Representation in Language Models_ It All Begins with Tokenizers.pdf:PDF:https\://arxiv.org/pdf/2409.11501.pdf},
  primaryclass  = {cs.CL},
}

@Misc{Winata2024,
  author        = {Genta Indra Winata and Hanyang Zhao and Anirban Das and Wenpin Tang and David D. Yao and Shi-Xiong Zhang and Sambit Sahu},
  title         = {Preference Tuning with Human Feedback on Language, Speech, and Vision Tasks: A Survey},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2409.11564},
  eprint        = {2409.11564},
  eprinttype    = {arxiv},
  file          = {:Winata2024 - Preference Tuning with Human Feedback on Language, Speech, and Vision Tasks_ a Survey.pdf:PDF:http\://arxiv.org/pdf/2409.11564v1},
  primaryclass  = {cs.CL},
}

@Misc{Kamruzzaman2024,
  author        = {Mahammed Kamruzzaman and Hieu Nguyen and Nazmul Hassan and Gene Louis Kim},
  title         = {"A Woman is More Culturally Knowledgeable than A Man?": The Effect of Personas on Cultural Norm Interpretation in LLMs},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2409.11636},
  eprint        = {2409.11636},
  eprinttype    = {arxiv},
  file          = {:Kamruzzaman2024 - _A Woman Is More Culturally Knowledgeable Than a Man___ the Effect of Personas on Cultural Norm Interpretation in LLMs.pdf:PDF:http\://arxiv.org/pdf/2409.11636v1},
  primaryclass  = {cs.CL},
}

@Misc{Srivastava2024,
  author        = {Nikit Srivastava and Denis Kuchelev and Tatiana Moteu Ngoli and Kshitij Shetty and Michael Röder and Diego Moussallem and Hamada Zahera and Axel-Cyrille Ngonga Ngomo},
  title         = {LOLA -- An Open-Source Massively Multilingual Large Language Model},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2409.11272},
  eprint        = {2409.11272},
  eprinttype    = {arxiv},
  file          = {:Srivastava2024 - LOLA an Open Source Massively Multilingual Large Language Model.pdf:PDF:http\://arxiv.org/pdf/2409.11272v3},
  primaryclass  = {cs.CL},
}

@Misc{Lee2024b,
  author        = {Jemin Lee and Sihyeong Park and Jinse Kwon and Jihun Oh and Yongin Kwon},
  title         = {A Comprehensive Evaluation of Quantized Instruction-Tuned Large Language Models: An Experimental Analysis up to 405B},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2409.11055},
  eprint        = {2409.11055},
  eprinttype    = {arxiv},
  file          = {:Lee2024b - A Comprehensive Evaluation of Quantized Instruction Tuned Large Language Models_ an Experimental Analysis up to 405B.pdf:PDF:http\://arxiv.org/pdf/2409.11055v1},
  primaryclass  = {cs.CL},
}

@Misc{Song2024,
  author        = {Dingjie Song and Wenjun Wang and Shunian Chen and Xidong Wang and Michael Guan and Benyou Wang},
  title         = {Less is More: A Simple yet Effective Token Reduction Method for Efficient Multi-modal LLMs},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2409.10994},
  eprint        = {2409.10994},
  eprinttype    = {arxiv},
  file          = {:Song2024 - Less Is More_ a Simple yet Effective Token Reduction Method for Efficient Multi Modal LLMs.pdf:PDF:http\://arxiv.org/pdf/2409.10994v1},
  primaryclass  = {cs.CL},
}

@Misc{Zhang2024f,
  author        = {Qingru Zhang and Xiaodong Yu and Chandan Singh and Xiaodong Liu and Liyuan Liu and Jianfeng Gao and Tuo Zhao and Dan Roth and Hao Cheng},
  title         = {Model Tells Itself Where to Attend: Faithfulness Meets Automatic Attention Steering},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2409.10790},
  eprint        = {2409.10790},
  eprinttype    = {arxiv},
  file          = {:Zhang2024f - Model Tells Itself Where to Attend_ Faithfulness Meets Automatic Attention Steering.pdf:PDF:http\://arxiv.org/pdf/2409.10790v1},
  primaryclass  = {cs.CL},
}

@Misc{Valmeekam2024,
  author        = {Karthik Valmeekam and Kaya Stechly and Subbarao Kambhampati},
  title         = {LLMs Still Can't Plan; Can LRMs? A Preliminary Evaluation of OpenAI's o1 on PlanBench},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2409.13373},
  eprint        = {2409.13373},
  eprinttype    = {arxiv},
  file          = {:Valmeekam2024 - LLMs Still Can't Plan\; Can LRMs_ a Preliminary Evaluation of OpenAI's O1 on PlanBench.pdf:PDF:http\://arxiv.org/pdf/2409.13373v1},
  primaryclass  = {cs.AI},
}

@Misc{Goral2024a,
  author        = {Gracjan Góral and Alicja Ziarko and Michal Nauman and Maciej Wołczyk},
  title         = {Seeing Through Their Eyes: Evaluating Visual Perspective Taking in Vision Language Models},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2409.12969},
  eprint        = {2409.12969},
  eprinttype    = {arxiv},
  file          = {:Goral2024a - Seeing through Their Eyes_ Evaluating Visual Perspective Taking in Vision Language Models.pdf:PDF:http\://arxiv.org/pdf/2409.12969v1},
  primaryclass  = {cs.CL},
}

@Misc{Kundu2024,
  author        = {Anindita Kundu and Denilson Barbosa},
  title         = {Are Large Language Models Good Essay Graders?},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2409.13120},
  eprint        = {2409.13120},
  eprinttype    = {arxiv},
  file          = {:Kundu2024 - Are Large Language Models Good Essay Graders_.pdf:PDF:http\://arxiv.org/pdf/2409.13120v1},
  primaryclass  = {cs.CL},
}

@Misc{Liu2024e,
  author        = {Tianqi Liu and Wei Xiong and Jie Ren and Lichang Chen and Junru Wu and Rishabh Joshi and Yang Gao and Jiaming Shen and Zhen Qin and Tianhe Yu and Daniel Sohn and Anastasiia Makarova and Jeremiah Liu and Yuan Liu and Bilal Piot and Abe Ittycheriah and Aviral Kumar and Mohammad Saleh},
  title         = {RRM: Robust Reward Model Training Mitigates Reward Hacking},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2409.13156},
  eprint        = {2409.13156},
  eprinttype    = {arxiv},
  file          = {:Liu2024e - RRM_ Robust Reward Model Training Mitigates Reward Hacking.pdf:PDF:http\://arxiv.org/pdf/2409.13156v1},
  primaryclass  = {cs.CL},
}

@Misc{He2024,
  author        = {Qiaozhi He and Xiaomin Zhuang and Zhihua Wu},
  title         = {Exploring Scaling Laws for Local SGD in Large Language Model Training},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2409.13198},
  eprint        = {2409.13198},
  eprinttype    = {arxiv},
  file          = {:He2024 - Exploring Scaling Laws for Local SGD in Large Language Model Training.pdf:PDF:http\://arxiv.org/pdf/2409.13198v1},
  primaryclass  = {cs.CL},
}

@Misc{Chen2024c,
  author        = {Yuyan Chen and Hao Wang and Songzhou Yan and Sijia Liu and Yueze Li and Yi Zhao and Yanghua Xiao},
  title         = {EmotionQueen: A Benchmark for Evaluating Empathy of Large Language Models},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2409.13359},
  eprint        = {2409.13359},
  eprinttype    = {arxiv},
  file          = {:Chen2024c - EmotionQueen_ a Benchmark for Evaluating Empathy of Large Language Models.pdf:PDF:https\://aclanthology.org/2024.findings-acl.128.pdf},
  primaryclass  = {cs.CL},
}

@Misc{Mekala2024,
  author        = {Anmol Mekala and Vineeth Dorna and Shreya Dubey and Abhishek Lalwani and David Koleczek and Mukund Rungta and Sadid Hasan and Elita Lobo},
  title         = {Alternate Preference Optimization for Unlearning Factual Knowledge in Large Language Models},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2409.13474},
  eprint        = {2409.13474},
  eprinttype    = {arxiv},
  file          = {:Mekala2024 - Alternate Preference Optimization for Unlearning Factual Knowledge in Large Language Models.pdf:PDF:http\://arxiv.org/pdf/2409.13474v1},
  primaryclass  = {cs.CL},
}

@Misc{Vedantam2015,
  author        = {Ramakrishna Vedantam and C. Lawrence Zitnick and Devi Parikh},
  title         = {CIDEr: Consensus-based Image Description Evaluation},
  year          = {2015},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.1411.5726},
  eprint        = {1411.5726},
  eprinttype    = {arxiv},
  file          = {:Vedantam2015 - CIDEr_ Consensus Based Image Description Evaluation.pdf:PDF:http\://arxiv.org/pdf/1411.5726.pdf},
  primaryclass  = {cs.CV},
}

@Misc{Ng2024,
  author        = {Kei-Sing Ng and Qingchen Wang},
  title         = {Loop-Residual Neural Networks for Iterative Refinement},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2409.14199},
  eprint        = {2409.14199},
  eprinttype    = {arxiv},
  file          = {:Ng2024 - Loop Residual Neural Networks for Iterative Refinement.pdf:PDF:http\://arxiv.org/pdf/2409.14199v1},
  primaryclass  = {cs.AI},
}

@Misc{Bhargava2024,
  author        = {Aman Bhargava and Cameron Witkowski and Alexander Detkov and Matt Thomson},
  title         = {Prompt Baking},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2409.13697},
  eprint        = {2409.13697},
  eprinttype    = {arxiv},
  file          = {:Bhargava2024 - Prompt Baking.pdf:PDF:http\://arxiv.org/pdf/2409.13697v1},
  primaryclass  = {cs.CL},
}

@Misc{Shi2024,
  author        = {Zhenmei Shi and Yifei Ming and Xuan-Phi Nguyen and Yingyu Liang and Shafiq Joty},
  title         = {Discovering the Gems in Early Layers: Accelerating Long-Context LLMs with 1000x Input Token Reduction},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2409.17422},
  eprint        = {2409.17422},
  eprinttype    = {arxiv},
  file          = {:Shi2024 - Discovering the Gems in Early Layers_ Accelerating Long Context LLMs with 1000x Input Token Reduction.pdf:PDF:http\://arxiv.org/pdf/2409.17422v1},
  primaryclass  = {cs.CL},
}

@Misc{Kamath2024,
  author        = {Amita Kamath and Cheng-Yu Hsieh and Kai-Wei Chang and Ranjay Krishna},
  title         = {The Hard Positive Truth about Vision-Language Compositionality},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2409.17958},
  eprint        = {2409.17958},
  eprinttype    = {arxiv},
  file          = {:Kamath2024 - The Hard Positive Truth about Vision Language Compositionality.pdf:PDF:http\://arxiv.org/pdf/2409.17958v1},
  primaryclass  = {cs.CL},
}

@Misc{Zhai2023,
  author        = {Xiaohua Zhai and Basil Mustafa and Alexander Kolesnikov and Lucas Beyer},
  title         = {Sigmoid Loss for Language Image Pre-Training},
  year          = {2023},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2303.15343},
  eprint        = {2303.15343},
  eprinttype    = {arxiv},
  file          = {:Zhai2023 - Sigmoid Loss for Language Image Pre Training.pdf:PDF:http\://arxiv.org/pdf/2303.15343v4},
  primaryclass  = {cs.CV},
}

@Misc{Karamcheti2024,
  author        = {Siddharth Karamcheti and Suraj Nair and Ashwin Balakrishna and Percy Liang and Thomas Kollar and Dorsa Sadigh},
  title         = {Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2402.07865},
  eprint        = {2402.07865},
  eprinttype    = {arxiv},
  file          = {:Karamcheti2024 - Prismatic VLMs_ Investigating the Design Space of Visually Conditioned Language Models.pdf:PDF:http\://arxiv.org/pdf/2402.07865v2},
  primaryclass  = {cs.CV},
}

@Misc{Huang2024a,
  author        = {Hongzhe Huang and Zhewen Yu and Jiang Liu and Li Cai and Dian Jiao and Wenqiao Zhang and Siliang Tang and Juncheng Li and Hao Jiang and Haoyuan Li and Yueting Zhuang},
  title         = {Align$^2$LLaVA: Cascaded Human and Large Language Model Preference Alignment for Multi-modal Instruction Curation},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2409.18541},
  eprint        = {2409.18541},
  eprinttype    = {arxiv},
  file          = {:Huang2024a - Align$^2$LLaVA_ Cascaded Human and Large Language Model Preference Alignment for Multi Modal Instruction Curation.pdf:PDF:http\://arxiv.org/pdf/2409.18541v1},
  primaryclass  = {cs.AI},
}

@Misc{Zhang2024g,
  author        = {Haowei Zhang and Jianzhe Liu and Zhen Han and Shuo Chen and Bailan He and Volker Tresp and Zhiqiang Xu and Jindong Gu},
  title         = {Visual Question Decomposition on Multimodal Large Language Models},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2409.19339},
  eprint        = {2409.19339},
  eprinttype    = {arxiv},
  file          = {:Zhang2024g - Visual Question Decomposition on Multimodal Large Language Models.pdf:PDF:http\://arxiv.org/pdf/2409.19339v1},
  primaryclass  = {cs.CL},
}

@Misc{Du2024,
  author        = {Yexing Du and Ziyang Ma and Yifan Yang and Keqi Deng and Xie Chen and Bo Yang and Yang Xiang and Ming Liu and Bing Qin},
  title         = {CoT-ST: Enhancing LLM-based Speech Translation with Multimodal Chain-of-Thought},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2409.19510},
  eprint        = {2409.19510},
  eprinttype    = {arxiv},
  file          = {:Du2024 - CoT ST_ Enhancing LLM Based Speech Translation with Multimodal Chain of Thought.pdf:PDF:http\://arxiv.org/pdf/2409.19510v1},
  primaryclass  = {cs.CL},
}

@Misc{Zhao2023,
  author        = {Yanli Zhao and Andrew Gu and Rohan Varma and Liang Luo and Chien-Chin Huang and Min Xu and Less Wright and Hamid Shojanazeri and Myle Ott and Sam Shleifer and Alban Desmaison and Can Balioglu and Pritam Damania and Bernard Nguyen and Geeta Chauhan and Yuchen Hao and Ajit Mathews and Shen Li},
  title         = {PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel},
  year          = {2023},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2304.11277},
  eprint        = {2304.11277},
  eprinttype    = {arxiv},
  file          = {:Zhao2023 - PyTorch FSDP_ Experiences on Scaling Fully Sharded Data Parallel.pdf:PDF:https\://dl.acm.org/doi/pdf/10.14778/3611540.3611569},
  primaryclass  = {cs.DC},
}

@Misc{Karamcheti2021,
  author = {Karamcheti, Siddharth* and Orr, Laurel* and Bolton, Jason and Zhang, Tianyi and Goel, Karan and Narayan, Avanika and Bommasani, Rishi and Narayanan, Deepak and Hashimoto, Tatsunori and Jurafsky, Dan and Manning, Christopher D. and Potts, Christopher and Ré, Christopher and Liang, Percy},
  title  = {Mistral - A Journey towards Reproducible Language Model Training},
  year   = {2021},
  url    = {https://github.com/stanford-crfm/mistral},
}

@Misc{Li2020,
  author        = {Zhuohan Li and Eric Wallace and Sheng Shen and Kevin Lin and Kurt Keutzer and Dan Klein and Joseph E. Gonzalez},
  title         = {Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers},
  year          = {2020},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2002.11794},
  eprint        = {2002.11794},
  eprinttype    = {arxiv},
  file          = {:Li2020 - Train Large, Then Compress_ Rethinking Model Size for Efficient Training and Inference of Transformers.pdf:PDF:http\://arxiv.org/pdf/2002.11794v2},
  primaryclass  = {cs.CL},
}

@Article{Kang2024,
  author        = {Kang, Weitai and Huang, Haifeng and Shang, Yuzhang and Shah, Mubarak and Yan, Yan},
  title         = {Robin3D: Improving 3D Large Language Model via Robust Instruction Tuning},
  year          = {2024},
  month         = sep,
  abstract      = {Recent advancements in 3D Large Language Models (3DLLMs) have highlighted their potential in building general-purpose agents in the 3D real world, yet challenges remain due to the lack of high-quality robust instruction-following data, leading to limited discriminative power and generalization of 3DLLMs. In this paper, we introduce Robin3D, a powerful 3DLLM trained on large-scale instruction-following data generated by our novel data engine, Robust Instruction Generation (RIG) engine. RIG generates two key instruction data: 1) the Adversarial Instruction-following data, which features mixed negative and positive samples to enhance the model's discriminative understanding. 2) the Diverse Instruction-following data, which contains various instruction styles to enhance model's generalization. As a result, we construct 1 million instruction-following data, consisting of 344K Adversarial samples, 508K Diverse samples, and 165K benchmark training set samples. To better handle these complex instructions, Robin3D first incorporates Relation-Augmented Projector to enhance spatial understanding, and then strengthens the object referring and grounding ability through ID-Feature Bonding. Robin3D consistently outperforms previous methods across five widely-used 3D multimodal learning benchmarks, without the need for task-specific fine-tuning. Notably, we achieve a 7.8\% improvement in the grounding task (Multi3DRefer) and a 6.9\% improvement in the captioning task (Scan2Cap).},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2410.00255},
  eprint        = {2410.00255},
  file          = {:Kang2024 - Robin3D_ Improving 3D Large Language Model Via Robust Instruction Tuning.pdf:PDF:http\://arxiv.org/pdf/2410.00255v1},
  keywords      = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
  primaryclass  = {cs.AI},
  publisher     = {arXiv},
}

@Article{AIA2024,
  author = {{Allen Institute for AI}},
  title  = {Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Multimodal Models},
  year   = {2024},
  month  = sep,
  file   = {:- Molmo and PixMo_ Open Weights and Open Data for State of the Art Multimodal Models.pdf:PDF},
}

@InProceedings{Touvron2021,
  author    = {Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and Jegou, Herve},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  title     = {Training data-efficient image transformers &amp; distillation through attention},
  year      = {2021},
  editor    = {Meila, Marina and Zhang, Tong},
  month     = {18--24 Jul},
  pages     = {10347--10357},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {139},
  abstract  = {Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. These high-performing vision transformers are pre-trained with hundreds of millions of images using a large infrastructure, thereby limiting their adoption. In this work, we produce competitive convolution-free transformers trained on ImageNet only using a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1% (single-crop) on ImageNet with no external data. We also introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention, typically from a convnet teacher. The learned transformers are competitive (85.2% top-1 acc.) with the state of the art on ImageNet, and similarly when transferred to other tasks. We will share our code and models.},
  file      = {:Touvron2021 - Training Data Efficient Image Transformers &amp\; Distillation through Attention.pdf:PDF;:touvron21a-supp.pdf:PDF},
  pdf       = {http://proceedings.mlr.press/v139/touvron21a/touvron21a.pdf},
  url       = {https://proceedings.mlr.press/v139/touvron21a.html},
}

@Misc{Qin2024,
  author        = {Yiwei Qin and Xuefeng Li and Haoyang Zou and Yixiu Liu and Shijie Xia and Zhen Huang and Yixin Ye and Weizhe Yuan and Hector Liu and Yuanzhi Li and Pengfei Liu},
  title         = {O1 Replication Journey: A Strategic Progress Report -- Part 1},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2410.18982},
  eprint        = {2410.18982},
  eprinttype    = {arxiv},
  file          = {:Qin2024 - O1 Replication Journey_ a Strategic Progress Report Part 1.pdf:PDF:http\://arxiv.org/pdf/2410.18982v1},
  primaryclass  = {cs.AI},
}

@Misc{Gkountouras2024,
  author        = {John Gkountouras and Matthias Lindemann and Phillip Lippe and Efstratios Gavves and Ivan Titov},
  title         = {Language Agents Meet Causality -- Bridging LLMs and Causal World Models},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2410.19923},
  eprint        = {2410.19923},
  eprinttype    = {arxiv},
  file          = {:Gkountouras2024 - Language Agents Meet Causality Bridging LLMs and Causal World Models.pdf:PDF:http\://arxiv.org/pdf/2410.19923v1},
  primaryclass  = {cs.AI},
}

@Misc{Chen2024d,
  author        = {Haolin Chen and Yihao Feng and Zuxin Liu and Weiran Yao and Akshara Prabhakar and Shelby Heinecke and Ricky Ho and Phil Mui and Silvio Savarese and Caiming Xiong and Huan Wang},
  title         = {Language Models are Hidden Reasoners: Unlocking Latent Reasoning Capabilities via Self-Rewarding},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2411.04282},
  eprint        = {2411.04282},
  eprinttype    = {arxiv},
  file          = {:Chen2024d - Language Models Are Hidden Reasoners_ Unlocking Latent Reasoning Capabilities Via Self Rewarding.pdf:PDF:http\://arxiv.org/pdf/2411.04282v1},
  primaryclass  = {cs.AI},
}

@Article{Kawahara2019,
  author   = {Kawahara, Jeremy and Daneshvar, Sara and Argenziano, Giuseppe and Hamarneh, Ghassan},
  journal  = {IEEE Journal of Biomedical and Health Informatics},
  title    = {{Seven-Point Checklist and Skin Lesion Classification Using Multitask Multimodal Neural Nets}},
  year     = {2019},
  number   = {2},
  pages    = {538-546},
  volume   = {23},
  doi      = {10.1109/JBHI.2018.2824327},
  file     = {:Kawahara2019 - Seven Point Checklist and Skin Lesion Classification Using Multitask Multimodal Neural Nets.pdf:PDF;:jbhi2018a.pdf:PDF},
  keywords = {Lesions;Skin;Malignant tumors;Feature extraction;Pattern analysis;Convolutional neural networks;Classification;convolutional neural networks;deep learning;dermatology;melanoma;skin;7-point checklist},
}

@Misc{Abraham2024,
  author        = {Savitha Sam Abraham and Sourav Garg and Feras Dayoub},
  title         = {To Ask or Not to Ask? Detecting Absence of Information in Vision and Language Navigation},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2411.05831},
  eprint        = {2411.05831},
  eprinttype    = {arxiv},
  file          = {:Abraham2024 - To Ask or Not to Ask_ Detecting Absence of Information in Vision and Language Navigation.pdf:PDF:http\://arxiv.org/pdf/2411.05831v1},
  primaryclass  = {cs.AI},
}

@Misc{Gu2024,
  author        = {Yu Gu and Boyuan Zheng and Boyu Gou and Kai Zhang and Cheng Chang and Sanjari Srivastava and Yanan Xie and Peng Qi and Huan Sun and Yu Su},
  title         = {Is Your LLM Secretly a World Model of the Internet? Model-Based Planning for Web Agents},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2411.06559},
  eprint        = {2411.06559},
  eprinttype    = {arxiv},
  file          = {:Gu2024 - Is Your LLM Secretly a World Model of the Internet_ Model Based Planning for Web Agents.pdf:PDF:http\://arxiv.org/pdf/2411.06559v1},
  primaryclass  = {cs.AI},
}

@Misc{Ali2024,
  author        = {Mehdi Ali and Michael Fromm and Klaudia Thellmann and Jan Ebert and Alexander Arno Weber and Richard Rutmann and Charvi Jain and Max Lübbering and Daniel Steinigen and Johannes Leveling and Katrin Klug and Jasper Schulze Buschhoff and Lena Jurkschat and Hammam Abdelwahab and Benny Jörg Stein and Karl-Heinz Sylla and Pavel Denisov and Nicolo' Brandizzi and Qasid Saleem and Anirban Bhowmick and Lennard Helmer and Chelsea John and Pedro Ortiz Suarez and Malte Ostendorff and Alex Jude and Lalith Manjunath and Samuel Weinbach and Carolin Penke and Oleg Filatov and Shima Asaadi and Fabio Barth and Rafet Sifa and Fabian Küch and Andreas Herten and René Jäkel and Georg Rehm and Stefan Kesselheim and Joachim Köhler and Nicolas Flores-Herr},
  title         = {Teuken-7B-Base & Teuken-7B-Instruct: Towards European LLMs},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2410.03730},
  eprint        = {2410.03730},
  eprinttype    = {arxiv},
  file          = {:Ali2024 - Teuken 7B Base & Teuken 7B Instruct_ Towards European LLMs.pdf:PDF:http\://arxiv.org/pdf/2410.03730v2},
  primaryclass  = {cs.CL},
}

@InProceedings{Mirzaalian2012,
  author    = {Mirzaalian, Hengameh and Lee, {Tim K} and Hamarneh, Ghassan},
  booktitle = {2012 IEEE Workshop on Mathematical Methods in Biomedical Image Analysis},
  title     = {{Learning features for streak detection in dermoscopic color images using localized radial flux of principal intensity curvature}},
  year      = {2012},
  pages     = {97-101},
  doi       = {10.1109/MMBIA.2012.6164758},
  file      = {:Mirzaalian2012 - Learning Features for Streak Detection in Dermoscopic Color Images Using Localized Radial Flux of Principal Intensity Curvature.pdf:PDF},
  keywords  = {Lesions;Accuracy;Feature extraction;Cancer;Skin;Vectors;Image color analysis},
}

@InProceedings{Madooei2013,
  author    = {Madooei, Ali and Drew, Mark S. and Sadeghi, Maryam and Atkins, M. Stella},
  booktitle = {Medical Image Computing and Computer-Assisted Intervention -- MICCAI 2013},
  title     = {Automatic Detection of Blue-White Veil by Discrete Colour Matching in Dermoscopy Images},
  year      = {2013},
  address   = {Berlin, Heidelberg},
  editor    = {Mori, Kensaku and Sakuma, Ichiro and Sato, Yoshinobu and Barillot, Christian and Navab, Nassir},
  pages     = {453--460},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {Skin lesions are often comprised of various colours. The presence of multiple colours with an irregular distribution can signal malignancy. Among common colours under dermoscopy, blue-grey (blue-white veil) is a strong indicator of malignant melanoma. Since it is not always easy to visually identify and recognize this feature, a computerised automatic colour analysis method can provide the clinician with an objective second opinion. In this paper, we put forward an innovative method, through colour analysis and computer vision techniques, to automatically detect and segment blue-white veil areas in dermoscopy images. The proposed method is an attempt to mimic the human perception of lesion colours, and improves and outperforms the state-of-the-art as shown in our experiments.},
  file      = {:Madooei2013 - Automatic Detection of Blue White Veil by Discrete Colour Matching in Dermoscopy Images.pdf:PDF},
  isbn      = {978-3-642-40760-4},
}

@InBook{Fabbrocini2014,
  author    = {Fabbrocini, Gabriella and Vita, Valerio De and Cacciapuoti, Sara and Leo, Giuseppe Di and Liguori, Consolatina and Paolillo, Alfredo and Pietrosanto, Antonio and Sommella, Paolo},
  editor    = {Scharcanski, Jacob and Celebi, M. Emre},
  pages     = {71--107},
  publisher = {Springer Berlin Heidelberg},
  title     = {Automatic Diagnosis of Melanoma Based on the 7-Point Checklist},
  year      = {2014},
  address   = {Berlin, Heidelberg},
  isbn      = {978-3-642-39608-3},
  abstract  = {An image based system implementing a well-known diagnostic method is disclosed for the automatic detection of melanomas as support to clinicians. The software procedure is able to recognize automatically the skin lesion within the digital image, measure morphological and chromatic parameters, carry out a suitable classification for detecting the dermoscopic structures provided by the 7-Point Checklist. Advanced techniques are introduced at different stages of the image processing pipeline, including the border detection, the extraction of low-level features and scoring of high order features.},
  booktitle = {Computer Vision Techniques for the Diagnosis of Skin Cancer},
  doi       = {10.1007/978-3-642-39608-3_4},
  file      = {:Fabbrocini2014 - Automatic Diagnosis of Melanoma Based on the 7 Point Checklist.pdf:PDF},
}

@Misc{ContactDoctor2024,
  author = {ContactDoctor},
  title  = {{Bio-Medical-MultiModal-Llama-3-8B-V1: A High-Performance Biomedical Multimodal LLM}},
  year   = {2024},
  url    = {https://huggingface.co/ContactDoctor/Bio-Medical-MultiModal-Llama-3-8B-V1},
}

@Misc{DeepSeekAI2024a,
  author        = {DeepSeek-AI and Aixin Liu and Bei Feng and Bing Xue and Bingxuan Wang and Bochao Wu and Chengda Lu and Chenggang Zhao and Chengqi Deng and Chenyu Zhang and Chong Ruan and Damai Dai and Daya Guo and Dejian Yang and Deli Chen and Dongjie Ji and Erhang Li and Fangyun Lin and Fucong Dai and Fuli Luo and Guangbo Hao and Guanting Chen and Guowei Li and H. Zhang and Han Bao and Hanwei Xu and Haocheng Wang and Haowei Zhang and Honghui Ding and Huajian Xin and Huazuo Gao and Hui Li and Hui Qu and J. L. Cai and Jian Liang and Jianzhong Guo and Jiaqi Ni and Jiashi Li and Jiawei Wang and Jin Chen and Jingchang Chen and Jingyang Yuan and Junjie Qiu and Junlong Li and Junxiao Song and Kai Dong and Kai Hu and Kaige Gao and Kang Guan and Kexin Huang and Kuai Yu and Lean Wang and Lecong Zhang and Lei Xu and Leyi Xia and Liang Zhao and Litong Wang and Liyue Zhang and Meng Li and Miaojun Wang and Mingchuan Zhang and Minghua Zhang and Minghui Tang and Mingming Li and Ning Tian and Panpan Huang and Peiyi Wang and Peng Zhang and Qiancheng Wang and Qihao Zhu and Qinyu Chen and Qiushi Du and R. J. Chen and R. L. Jin and Ruiqi Ge and Ruisong Zhang and Ruizhe Pan and Runji Wang and Runxin Xu and Ruoyu Zhang and Ruyi Chen and S. S. Li and Shanghao Lu and Shangyan Zhou and Shanhuang Chen and Shaoqing Wu and Shengfeng Ye and Shengfeng Ye and Shirong Ma and Shiyu Wang and Shuang Zhou and Shuiping Yu and Shunfeng Zhou and Shuting Pan and T. Wang and Tao Yun and Tian Pei and Tianyu Sun and W. L. Xiao and Wangding Zeng and Wanjia Zhao and Wei An and Wen Liu and Wenfeng Liang and Wenjun Gao and Wenqin Yu and Wentao Zhang and X. Q. Li and Xiangyue Jin and Xianzu Wang and Xiao Bi and Xiaodong Liu and Xiaohan Wang and Xiaojin Shen and Xiaokang Chen and Xiaokang Zhang and Xiaosha Chen and Xiaotao Nie and Xiaowen Sun and Xiaoxiang Wang and Xin Cheng and Xin Liu and Xin Xie and Xingchao Liu and Xingkai Yu and Xinnan Song and Xinxia Shan and Xinyi Zhou and Xinyu Yang and Xinyuan Li and Xuecheng Su and Xuheng Lin and Y. K. Li and Y. Q. Wang and Y. X. Wei and Y. X. Zhu and Yang Zhang and Yanhong Xu and Yanhong Xu and Yanping Huang and Yao Li and Yao Zhao and Yaofeng Sun and Yaohui Li and Yaohui Wang and Yi Yu and Yi Zheng and Yichao Zhang and Yifan Shi and Yiliang Xiong and Ying He and Ying Tang and Yishi Piao and Yisong Wang and Yixuan Tan and Yiyang Ma and Yiyuan Liu and Yongqiang Guo and Yu Wu and Yuan Ou and Yuchen Zhu and Yuduan Wang and Yue Gong and Yuheng Zou and Yujia He and Yukun Zha and Yunfan Xiong and Yunxian Ma and Yuting Yan and Yuxiang Luo and Yuxiang You and Yuxuan Liu and Yuyang Zhou and Z. F. Wu and Z. Z. Ren and Zehui Ren and Zhangli Sha and Zhe Fu and Zhean Xu and Zhen Huang and Zhen Zhang and Zhenda Xie and Zhengyan Zhang and Zhewen Hao and Zhibin Gou and Zhicheng Ma and Zhigang Yan and Zhihong Shao and Zhipeng Xu and Zhiyu Wu and Zhongyu Zhang and Zhuoshu Li and Zihui Gu and Zijia Zhu and Zijun Liu and Zilin Li and Ziwei Xie and Ziyang Song and Ziyi Gao and Zizheng Pan},
  title         = {DeepSeek-V3 Technical Report},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2412.19437},
  eprint        = {2412.19437},
  eprinttype    = {arxiv},
  file          = {:DeepSeekAI2024a - DeepSeek V3 Technical Report.pdf:PDF:https\://arxiv.org/pdf/2412.19437.pdf},
  primaryclass  = {cs.CL},
}

@InProceedings{He2016,
  author    = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {{Deep Residual Learning for Image Recognition}},
  year      = {2016},
  month     = {June},
  file      = {:He2016 - Deep Residual Learning for Image Recognition.pdf:PDF;:He_Deep_Residual_Learning_2016_CVPR_supplemental.pdf:PDF},
}

@Misc{Chen2025,
  author        = {Zhe Chen and Weiyun Wang and Yue Cao and Yangzhou Liu and Zhangwei Gao and Erfei Cui and Jinguo Zhu and Shenglong Ye and Hao Tian and Zhaoyang Liu and Lixin Gu and Xuehui Wang and Qingyun Li and Yimin Ren and Zixuan Chen and Jiapeng Luo and Jiahao Wang and Tan Jiang and Bo Wang and Conghui He and Botian Shi and Xingcheng Zhang and Han Lv and Yi Wang and Wenqi Shao and Pei Chu and Zhongying Tu and Tong He and Zhiyong Wu and Huipeng Deng and Jiaye Ge and Kai Chen and Kaipeng Zhang and Limin Wang and Min Dou and Lewei Lu and Xizhou Zhu and Tong Lu and Dahua Lin and Yu Qiao and Jifeng Dai and Wenhai Wang},
  title         = {Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling},
  year          = {2025},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2412.05271},
  eprint        = {2412.05271},
  eprinttype    = {arxiv},
  file          = {:Chen2025 - Expanding Performance Boundaries of Open Source Multimodal Models with Model, Data, and Test Time Scaling.pdf:PDF:http\://arxiv.org/pdf/2412.05271v4},
  primaryclass  = {cs.CV},
}

@Article{French1999,
  author    = {Robert M. French},
  journal   = {Trends in Cognitive Sciences},
  title     = {Catastrophic forgetting in connectionist networks},
  year      = {1999},
  month     = apr,
  number    = {4},
  pages     = {128--135},
  volume    = {3},
  abstract  = {All natural cognitive systems, and, in particular, our own, gradually forget previously learned information. Plausible models of human cognition should therefore exhibit similar patterns of gradual forgetting of old information as new information is acquired. Only rarely does new learning in natural cognitive systems completely disrupt or erase previously learned information; that is, natural cognitive systems do not, in general, forget "catastrophically". Unfortunately, though, catastrophic forgetting does occur under certain circumstances in distributed connectionist networks. The very features that give these networks their remarkable abilities to generalize, to function in the presence of degraded input, and so on, are found to be the root cause of catastrophic forgetting. The challenge in this field is to discover how to keep the advantages of distributed connectionist networks while avoiding the problem of catastrophic forgetting. In this article the causes, consequences and numerous solutions to the problem of catastrophic forgetting in neural networks are examined. The review will consider how the brain might have overcome this problem and will also explore the consequences of this solution for distributed connectionist networks.},
  doi       = {10.1016/S1364-6613(99)01294-2},
  file      = {:French1999 - Catastrophic Forgetting in Connectionist Networks.pdf:PDF},
  publisher = {Elsevier},
}

@Misc{Wei2023,
  author        = {Jerry Wei and Jason Wei and Yi Tay and Dustin Tran and Albert Webson and Yifeng Lu and Xinyun Chen and Hanxiao Liu and Da Huang and Denny Zhou and Tengyu Ma},
  title         = {Larger language models do in-context learning differently},
  year          = {2023},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2303.03846},
  eprint        = {2303.03846},
  eprinttype    = {arxiv},
  file          = {:Wei2023 - Larger Language Models Do in Context Learning Differently.pdf:PDF:http\://arxiv.org/pdf/2303.03846v2},
  primaryclass  = {cs.CL},
}

@Misc{Chochlakis2025,
  author        = {Georgios Chochlakis and Alexandros Potamianos and Kristina Lerman and Shrikanth Narayanan},
  title         = {Aggregation Artifacts in Subjective Tasks Collapse Large Language Models' Posteriors},
  year          = {2025},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2410.13776},
  eprint        = {2410.13776},
  eprinttype    = {arxiv},
  file          = {:Chochlakis2025 - Aggregation Artifacts in Subjective Tasks Collapse Large Language Models' Posteriors.pdf:PDF:http\://arxiv.org/pdf/2410.13776v3},
  primaryclass  = {cs.CL},
}

@Misc{Chochlakis2024,
  author        = {Georgios Chochlakis and Alexandros Potamianos and Kristina Lerman and Shrikanth Narayanan},
  title         = {{The Strong Pull of Prior Knowledge in Large Language Models and Its Impact on Emotion Recognition}},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2403.17125},
  eprint        = {2403.17125},
  eprinttype    = {arxiv},
  file          = {:Chochlakis2024 - The Strong Pull of Prior Knowledge in Large Language Models and Its Impact on Emotion Recognition.pdf:PDF:http\://arxiv.org/pdf/2403.17125v1},
  primaryclass  = {cs.CL},
}

@InProceedings{Pan2023,
  author    = {Pan, Jane and Gao, Tianyu and Chen, Howard and Chen, Danqi},
  booktitle = {Findings of the Association for Computational Linguistics: ACL 2023},
  title     = {{What In-Context Learning {\textquotedblleft}Learns{\textquotedblright} In-Context: Disentangling Task Recognition and Task Learning}},
  year      = {2023},
  address   = {Toronto, Canada},
  editor    = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
  month     = jul,
  pages     = {8298--8319},
  publisher = {Association for Computational Linguistics},
  abstract  = {Large language models (LLMs) exploit in-context learning (ICL) to solve tasks with only a few demonstrations, but its mechanisms are not yet well-understood. Some works suggest that LLMs only recall already learned concepts from pre-training, while others hint that ICL performs implicit learning over demonstrations. We characterize two ways through which ICL leverages demonstrations. Task recognition (TR) captures the extent to which LLMs can recognize a task through demonstrations {--} even without ground-truth labels {--} and apply their pre-trained priors, whereas task learning (TL) is the ability to capture new input-label mappings unseen in pre-training. Using a wide range of classification datasets and three LLM families (GPT-3, LLaMA and OPT), we design controlled experiments to disentangle the roles of TR and TL in ICL. We show that (1) models can achieve non-trivial performance with only TR, and TR does not further improve with larger models or more demonstrations; (2) LLMs acquire TL as the model scales, and TL`s performance consistently improves with more demonstrations in context. Our findings unravel two different forces behind ICL and we advocate for discriminating them in future ICL research due to their distinct nature.},
  doi       = {10.18653/v1/2023.findings-acl.527},
  file      = {:Pan2023 - What in Context Learning _Learns_ in Context_ Disentangling Task Recognition and Task Learning.pdf:PDF},
  url       = {https://aclanthology.org/2023.findings-acl.527/},
}

@Article{Firoozi2023,
  author        = {Firoozi, Roya and Tucker, Johnathan and Tian, Stephen and Majumdar, Anirudha and Sun, Jiankai and Liu, Weiyu and Zhu, Yuke and Song, Shuran and Kapoor, Ashish and Hausman, Karol and Ichter, Brian and Driess, Danny and Wu, Jiajun and Lu, Cewu and Schwager, Mac},
  journal       = {The International Journal of Robotics Research},
  title         = {Foundation Models in Robotics: Applications, Challenges, and the Future},
  year          = {2023},
  month         = dec,
  pages         = {02783649241281508},
  abstract      = {We survey applications of pretrained foundation models in robotics. Traditional deep learning models in robotics are trained on small datasets tailored for specific tasks, which limits their adaptability across diverse applications. In contrast, foundation models pretrained on internet-scale data appear to have superior generalization capabilities, and in some instances display an emergent ability to find zero-shot solutions to problems that are not present in the training data. Foundation models may hold the potential to enhance various components of the robot autonomy stack, from perception to decision-making and control. For example, large language models can generate code or provide common sense reasoning, while vision-language models enable open-vocabulary visual recognition. However, significant open research challenges remain, particularly around the scarcity of robot-relevant training data, safety guarantees and uncertainty quantification, and real-time execution. In this survey, we study recent papers that have used or built foundation models to solve robotics problems. We explore how foundation models contribute to improving robot capabilities in the domains of perception, decision-making, and control. We discuss the challenges hindering the adoption of foundation models in robot autonomy and provide opportunities and potential pathways for future advancements. The GitHub project corresponding to this paper (Preliminary release. We are committed to further enhancing and updating this work to ensure its quality and relevance) can be found here: https://github.com/robotics-survey/Awesome-Robotics-Foundation-Models},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution 4.0 International},
  doi           = {10.48550/ARXIV.2312.07843},
  eprint        = {2312.07843},
  file          = {:Firoozi2023 - Foundation Models in Robotics_ Applications, Challenges, and the Future.pdf:PDF:http\://arxiv.org/pdf/2312.07843v1},
  groups        = {To Check},
  keywords      = {Robotics (cs.RO), FOS: Computer and information sciences},
  primaryclass  = {cs.RO},
  publisher     = {SAGE Publications Sage UK: London, England},
}

@Article{Kawaharazuka2024,
  author        = {Kawaharazuka, Kento and Matsushima, Tatsuya and Gambardella, Andrew and Guo, Jiaxian and Paxton, Chris and Zeng, Andy},
  journal       = {Advanced Robotics},
  title         = {Real-World Robot Applications of Foundation Models: A Review},
  year          = {2024},
  month         = feb,
  number        = {18},
  pages         = {1232--1254},
  volume        = {38},
  abstract      = {Recent developments in foundation models, like Large Language Models (LLMs) and Vision-Language Models (VLMs), trained on extensive data, facilitate flexible application across different tasks and modalities. Their impact spans various fields, including healthcare, education, and robotics. This paper provides an overview of the practical application of foundation models in real-world robotics, with a primary emphasis on the replacement of specific components within existing robot systems. The summary encompasses the perspective of input-output relationships in foundation models, as well as their role in perception, motion planning, and control within the field of robotics. This paper concludes with a discussion of future challenges and implications for practical robot applications.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2402.05741},
  eprint        = {2402.05741},
  file          = {:Kawaharazuka2024 - Real World Robot Applications of Foundation Models_ a Review.pdf:PDF:http\://arxiv.org/pdf/2402.05741v1},
  groups        = {To Check},
  keywords      = {Robotics (cs.RO), Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences},
  primaryclass  = {cs.RO},
  publisher     = {Taylor \& Francis},
}

@InProceedings{Xiao2023a,
  author        = {Bin Xiao and Haiping Wu and Weijian Xu and Xiyang Dai and Houdong Hu and Yumao Lu and Michael Zeng and Ce Liu and Lu Yuan},
  booktitle     = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title         = {{Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks}},
  year          = {2023},
  month         = {June},
  pages         = {4818-4829},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2311.06242},
  eprint        = {2311.06242},
  eprinttype    = {arxiv},
  file          = {:Xiao2023a - Florence 2_ Advancing a Unified Representation for a Variety of Vision Tasks.pdf:PDF:https\://arxiv.org/pdf/2311.06242.pdf},
  groups        = {Models},
  journal       = {arXiv preprint},
  primaryclass  = {cs.CV},
}

@InProceedings{Zhu2023,
  author        = {Deyao Zhu and Jun Chen and Xiaoqian Shen and Xiang Li and Mohamed Elhoseiny},
  booktitle     = {The Twelfth International Conference on Learning Representations},
  title         = {{MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models}},
  year          = {2023},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2304.10592},
  eprint        = {2304.10592},
  file          = {:Zhu2023 - MiniGPT 4_ Enhancing Vision Language Understanding with Advanced Large Language Models.pdf:PDF:http\://arxiv.org/pdf/2304.10592v2},
  groups        = {Models},
  journal       = {arXiv preprint},
  primaryclass  = {id='cs.CV' full_name='Computer Vision and Pattern Recognition' is_active=True alt_name=None in_archive='cs' is_general=False description='Covers image processing, computer vision, pattern recognition, and scene understanding. Roughly includes material in ACM Subject Classes I.2.10, I.4, and I.5.'},
  url           = {https://openreview.net/forum?id=1tZbq88f27},
}

@Article{He2023,
  author       = {He, Xuehai and Li, Chunyuan and Zhang, Pengchuan and Yang, Jianwei and Wang, Xin Eric},
  journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
  title        = {{Parameter-Efficient Model Adaptation for Vision Transformers}},
  year         = {2023},
  month        = {Jun.},
  number       = {1},
  pages        = {817-825},
  volume       = {37},
  abstractnote = {In computer vision, it has achieved great transfer learning performance via adapting large-scale pretrained vision models (e.g., vision transformers) to downstream tasks. Common approaches for model adaptation either update all model parameters or leverage linear probes. In this paper, we aim to study parameter-efficient model adaptation strategies for vision transformers on the image classification task. We formulate efficient model adaptation as a subspace training problem and perform a comprehensive benchmarking over different efficient adaptation methods. We conduct an empirical study on each efficient model adaptation method focusing on its performance alongside parameter cost. Furthermore, we propose a parameter-efficient model adaptation framework, which first selects submodules by measuring local intrinsic dimensions and then projects them into subspace for further decomposition via a novel Kronecker Adaptation method. We analyze and compare our method with a diverse set of baseline model adaptation methods (including state-of-the-art methods for pretrained language models). Our method performs the best in terms of the tradeoff between accuracy and parameter efficiency across 20 datasets under the few-shot setting and 7 image classification datasets under the full-shot setting.},
  doi          = {10.1609/aaai.v37i1.25160},
  file         = {:He2023 - Parameter Efficient Model Adaptation for Vision Transformers.pdf:PDF},
  url          = {https://ojs.aaai.org/index.php/AAAI/article/view/25160},
}

@InProceedings{Jia2022,
  author        = {Jia, Menglin and Tang, Luming and Chen, Bor-Chun and Cardie, Claire and Belongie, Serge and Hariharan, Bharath and Lim, Ser-Nam},
  booktitle     = {Computer Vision -- ECCV 2022},
  title         = {{Visual Prompt Tuning}},
  year          = {2022},
  address       = {Cham},
  editor        = {Avidan, Shai and Brostow, Gabriel and Ciss{\'e}, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
  pages         = {709--727},
  publisher     = {Springer Nature Switzerland},
  abstract      = {The current modus operandi in adapting pre-trained models involves updating all the backbone parameters, i.e., full fine-tuning. This paper introduces Visual Prompt Tuning (VPT) as an efficient and effective alternative to full fine-tuning for large-scale Transformer models in vision. Taking inspiration from recent advances in efficiently tuning large language models, VPT introduces only a small amount (less than 1{\%} of model parameters) of trainable parameters in the input space while keeping the model backbone frozen. Via extensive experiments on a wide variety of downstream recognition tasks, we show that VPT achieves significant performance gains compared to other parameter efficient tuning protocols. Most importantly, VPT even outperforms full fine-tuning in many cases across model capacities and training data scales, while reducing per-task storage cost. Code is available at github.com/kmnp/vpt.},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2203.12119},
  eprint        = {2203.12119},
  file          = {:Jia2022 - Visual Prompt Tuning.pdf:PDF:https\://link.springer.com/content/pdf/10.1007/978-3-031-19827-4_41.pdf},
  groups        = {Few-Shot Transfer},
  isbn          = {978-3-031-19827-4},
  journal       = {arXiv preprint},
  primaryclass  = {id='cs.CV' full_name='Computer Vision and Pattern Recognition' is_active=True alt_name=None in_archive='cs' is_general=False description='Covers image processing, computer vision, pattern recognition, and scene understanding. Roughly includes material in ACM Subject Classes I.2.10, I.4, and I.5.'},
}

@InProceedings{Yang2023a,
  author        = {Yang, Hao and Lin, Junyang and Yang, An and Wang, Peng and Zhou, Chang},
  booktitle     = {Findings of the Association for Computational Linguistics: ACL 2023},
  title         = {Prompt {T}uning for {G}enerative {M}ultimodal {P}retrained {M}odels},
  year          = {2023},
  address       = {Toronto, Canada},
  editor        = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
  month         = jul,
  pages         = {402--416},
  publisher     = {Association for Computational Linguistics},
  abstract      = {Prompt tuning has become a new paradigm for model tuning and it has demonstrated success in natural language pretraining and even vision pretraining. The parameter-efficient prompt tuning methods that optimize soft embeddings while keeping the pretrained model frozen demonstrate advantages in low computation costs and almost lossless performance. In this work, we explore the transfer of prompt tuning to multimodal pretrained models. Specifically, we implement prompt tuning to a unified sequence-to-sequence pretrained model by adding a sequence of learnable embeddings to each layer and finetuning the pretrained model on downstream task with only the learnable embeddings being optimized. Experimental results on a series of multimodal understanding and generation tasks demonstrate that our method OFA-PT can achieve comparable performance with finetuning across a series of multimodal generation and understanding tasks. Additionally, it significantly outperforms the unified multimodal pretrained model with other parameter-efficient tuning methods, e.g., Adapter, BitFit. etc. Besides, in comparison with finetuned models, the prompt-tuned models demonstrate improved robustness against adversarial attacks. We further figure out that experimental factors, including prompt length, prompt depth, and reparameteratization, have great impacts on the model performance, and thus we empirically provide a recommendation for the setups of prompt tuning.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.18653/v1/2023.findings-acl.27},
  eprint        = {2208.02532},
  file          = {:Yang2023a - Prompt Tuning for Generative Multimodal Pretrained Models.pdf:PDF:http\://arxiv.org/pdf/2208.02532v1},
  groups        = {Few-Shot Transfer},
  journal       = {arXiv preprint},
  keywords      = {Computation and Language (cs.CL), FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  priority      = {prio2},
  url           = {https://aclanthology.org/2023.findings-acl.27/},
}

@InProceedings{Jaderberg2017,
  author        = {Max Jaderberg and Volodymyr Mnih and Wojciech Marian Czarnecki and Tom Schaul and Joel Z Leibo and David Silver and Koray Kavukcuoglu},
  booktitle     = {International Conference on Learning Representations},
  title         = {{Reinforcement Learning with Unsupervised Auxiliary Tasks}},
  year          = {2017},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.1611.05397},
  eprint        = {1611.05397},
  eprinttype    = {arxiv},
  file          = {:Jaderberg2017 - Reinforcement Learning with Unsupervised Auxiliary Tasks.pdf:PDF:http\://arxiv.org/pdf/1611.05397v1},
  journal       = {arXiv preprint},
  primaryclass  = {cs.LG},
  url           = {https://openreview.net/forum?id=SJ6yPD5xg},
}

@Misc{Doveh2024,
  author        = {Sivan Doveh and Shaked Perek and M. Jehanzeb Mirza and Wei Lin and Amit Alfassy and Assaf Arbelle and Shimon Ullman and Leonid Karlinsky},
  title         = {Towards Multimodal In-Context Learning for Vision & Language Models},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2403.12736},
  eprint        = {2403.12736},
  eprinttype    = {arxiv},
  primaryclass  = {cs.CV},
}

@InProceedings{Tan2024,
  author        = {Tan, Zhen and Li, Dawei and Wang, Song and Beigi, Alimohammad and Jiang, Bohan and Bhattacharjee, Amrita and Karami, Mansooreh and Li, Jundong and Cheng, Lu and Liu, Huan},
  booktitle     = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  title         = {{Large Language Models for Data Annotation and Synthesis: A Survey}},
  year          = {2024},
  address       = {Miami, Florida, USA},
  editor        = {Al-Onaizan, Yaser and Bansal, Mohit and Chen, Yun-Nung},
  month         = nov,
  pages         = {930--957},
  publisher     = {Association for Computational Linguistics},
  abstract      = {Data annotation and synthesis generally refers to the labeling or generating of raw data with relevant information, which could be used for improving the efficacy of machine learning models. The process, however, is labor-intensive and costly. The emergence of advanced Large Language Models (LLMs), exemplified by GPT-4, presents an unprecedented opportunity to automate the complicated process of data annotation and synthesis. While existing surveys have extensively covered LLM architecture, training, and general applications, we uniquely focus on their specific utility for data annotation. This survey contributes to three core aspects: LLM-Based Annotation Generation, LLM-Generated Annotations Assessment, and LLM-Generated Annotations Utilization. Furthermore, this survey includes an in-depth taxonomy of data types that LLMs can annotate, a comprehensive review of learning strategies for models utilizing LLM-generated annotations, and a detailed discussion of the primary challenges and limitations associated with using LLMs for data annotation and synthesis. Serving as a key guide, this survey aims to assist researchers and practitioners in exploring the potential of the latest LLMs for data annotation, thereby fostering future advancements in this critical field.},
  archiveprefix = {arXiv},
  doi           = {10.18653/v1/2024.emnlp-main.54},
  eprint        = {2402.13446},
  eprinttype    = {arxiv},
  file          = {:Tan2024 - Large Language Models for Data Annotation and Synthesis_ a Survey.pdf:PDF:https\://arxiv.org/pdf/2402.13446.pdf},
  journal       = {arXiv preprint},
  primaryclass  = {cs.CL},
  url           = {https://aclanthology.org/2024.emnlp-main.54/},
}

@Article{Yao2024,
  author        = {Yuan Yao and Tianyu Yu and Ao Zhang and Chongyi Wang and Junbo Cui and Hongji Zhu and Tianchi Cai and Haoyu Li and Weilin Zhao and Zhihui He and Qianyu Chen and Huarong Zhou and Zhensheng Zou and Haoye Zhang and Shengding Hu and Zhi Zheng and Jie Zhou and Jie Cai and Xu Han and Guoyang Zeng and Dahai Li and Zhiyuan Liu and Maosong Sun},
  journal       = {CoRR},
  title         = {{MiniCPM-V: A GPT-4V Level MLLM on Your Phone}},
  year          = {2024},
  volume        = {abs/2408.01800},
  archiveprefix = {arXiv},
  cdate         = {1704067200000},
  doi           = {10.48550/arxiv.2408.01800},
  eprint        = {2408.01800},
  eprinttype    = {arxiv},
  file          = {:Yao2024 - MiniCPM V_ a GPT 4V Level MLLM on Your Phone.pdf:PDF:https\://arxiv.org/pdf/2408.01800.pdf},
  groups        = {Models},
  primaryclass  = {cs.CV},
  publtype      = {informal},
}

@Misc{Abdin2024a,
  author        = {Marah Abdin and Jyoti Aneja and Harkirat Behl and Sébastien Bubeck and Ronen Eldan and Suriya Gunasekar and Michael Harrison and Russell J. Hewett and Mojan Javaheripi and Piero Kauffmann and James R. Lee and Yin Tat Lee and Yuanzhi Li and Weishung Liu and Caio C. T. Mendes and Anh Nguyen and Eric Price and Gustavo de Rosa and Olli Saarikivi and Adil Salim and Shital Shah and Xin Wang and Rachel Ward and Yue Wu and Dingli Yu and Cyril Zhang and Yi Zhang},
  title         = {Phi-4 Technical Report},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2412.08905},
  eprint        = {2412.08905},
  eprinttype    = {arxiv},
  primaryclass  = {cs.CL},
}

@Misc{Wu2024a,
  author        = {Junda Wu and Zhehao Zhang and Yu Xia and Xintong Li and Zhaoyang Xia and Aaron Chang and Tong Yu and Sungchul Kim and Ryan A. Rossi and Ruiyi Zhang and Subrata Mitra and Dimitris N. Metaxas and Lina Yao and Jingbo Shang and Julian McAuley},
  title         = {Visual Prompting in Multimodal Large Language Models: A Survey},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2409.15310},
  eprint        = {2409.15310},
  eprinttype    = {arxiv},
  primaryclass  = {cs.LG},
}

@InProceedings{Dosovitskiy2021,
  author        = {Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
  booktitle     = {International Conference on Learning Representations},
  title         = {An {Im}age is {W}orth 16x16 {W}ords: {T}ransformers for {I}mage {R}ecognition at {S}cale},
  year          = {2021},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2010.11929},
  eprint        = {2010.11929},
  eprinttype    = {arxiv},
  file          = {:Dosovitskiy2021 - An Image Is Worth 16x16 Words_ Transformers for Image Recognition at Scale.pdf:PDF:http\://arxiv.org/pdf/2010.11929v2},
  groups        = {Models},
  journal       = {arXiv preprint},
  primaryclass  = {cs.CV},
  url           = {https://openreview.net/forum?id=YicbFdNTTy},
}

@Article{Khan2022,
  author     = {Khan, Salman and Naseer, Muzammal and Hayat, Munawar and Zamir, Syed Waqas and Khan, Fahad Shahbaz and Shah, Mubarak},
  journal    = {ACM Comput. Surv.},
  title      = {Transformers in Vision: A Survey},
  year       = {2022},
  issn       = {0360-0300},
  month      = sep,
  number     = {10s},
  volume     = {54},
  abstract   = {Astounding results from Transformer models on natural language tasks have intrigued the vision community to study their application to computer vision problems. Among their salient benefits, Transformers enable modeling long dependencies between input sequence elements and support parallel processing of sequence as compared to recurrent networks, e.g., Long short-term memory. Different from convolutional networks, Transformers require minimal inductive biases for their design and are naturally suited as set-functions. Furthermore, the straightforward design of Transformers allows processing multiple modalities (e.g., images, videos, text, and speech) using similar processing blocks and demonstrates excellent scalability to very large capacity networks and huge datasets. These strengths have led to exciting progress on a number of vision tasks using Transformer networks. This survey aims to provide a comprehensive overview of the Transformer models in the computer vision discipline. We start with an introduction to fundamental concepts behind the success of Transformers, i.e., self-attention, large-scale pre-training, and bidirectional feature encoding. We then cover extensive applications of transformers in vision including popular recognition tasks (e.g., image classification, object detection, action recognition, and segmentation), generative modeling, multi-modal tasks (e.g., visual-question answering, visual reasoning, and visual grounding), video processing (e.g., activity recognition, video forecasting), low-level vision (e.g., image super-resolution, image enhancement, and colorization), and three-dimensional analysis (e.g., point cloud classification and segmentation). We compare the respective advantages and limitations of popular techniques both in terms of architectural design and their experimental value. Finally, we provide an analysis on open research directions and possible future works. We hope this effort will ignite further interest in the community to solve current challenges toward the application of transformer models in computer vision.},
  address    = {New York, NY, USA},
  articleno  = {200},
  doi        = {10.1145/3505244},
  file       = {:Khan2022 - Transformers in Vision_ a Survey.pdf:PDF:http\://arxiv.org/pdf/2101.01169v5},
  issue_date = {January 2022},
  keywords   = {Self-attention, transformers, bidirectional encoders, deep neural networks, convolutional networks, self-supervision, literature survey},
  numpages   = {41},
  publisher  = {Association for Computing Machinery},
}

@Article{Zhou2024b,
  author        = {Zhou, Ce and Li, Qian and Li, Chen and Yu, Jun and Liu, Yixin and Wang, Guangjing and Zhang, Kai and Ji, Cheng and Yan, Qiben and He, Lifang and Peng, Hao and Li, Jianxin and Wu, Jia and Liu, Ziwei and Xie, Pengtao and Xiong, Caiming and Pei, Jian and Yu, Philip S. and Sun, Lichao},
  journal       = {International Journal of Machine Learning and Cybernetics},
  title         = {{A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT}},
  year          = {2024},
  issn          = {1868-808X},
  month         = {November},
  abstract      = {Pretrained Foundation Models (PFMs) are regarded as the foundation for various downstream tasks across different data modalities. A PFM (e.g., BERT, ChatGPT, GPT-4) is trained on large-scale data, providing a solid parameter initialization for a wide range of downstream applications. In contrast to earlier methods that use convolution and recurrent modules for feature extraction, BERT learns bidirectional encoder representations from Transformers, trained on large datasets as contextual language models. Similarly, the Generative Pretrained Transformer (GPT) method employs Transformers as feature extractors and is trained on large datasets using an autoregressive paradigm. Recently, ChatGPT has demonstrated significant success in large language models, utilizing autoregressive language models with zero-shot or few-shot prompting. The remarkable success of PFMs has driven significant breakthroughs in AI, leading to numerous studies proposing various methods, datasets, and evaluation metrics, which increases the demand for an updated survey. This study provides a comprehensive review of recent research advancements, challenges, and opportunities for PFMs in text, image, graph, and other data modalities. It covers the basic components and existing pretraining methods used in natural language processing, computer vision, and graph learning, while also exploring advanced PFMs for different data modalities and unified PFMs that address data quality and quantity. Additionally, the review discusses key aspects such as model efficiency, security, and privacy, and provides insights into future research directions and challenges in PFMs. Overall, this survey aims to shed light on the research of the PFMs on scalability, security, logical reasoning ability, cross-domain learning ability, and user-friendly interactive ability for artificial general intelligence.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  day           = {24},
  doi           = {10.1007/s13042-024-02443-6},
  eprint        = {2302.09419},
  file          = {:Zhou2024b - A Comprehensive Survey on Pretrained Foundation Models_ a History from BERT to ChatGPT.pdf:PDF:http\://arxiv.org/pdf/2302.09419v3},
  keywords      = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences},
  primaryclass  = {cs.AI},
  publisher     = {arXiv},
}

@Unpublished{Agrawal2024,
  author        = {Pravesh Agrawal and Szymon Antoniak and Emma Bou Hanna and Baptiste Bout and Devendra Chaplot and Jessica Chudnovsky and Diogo Costa and Baudouin De Monicault and Saurabh Garg and Theophile Gervet and Soham Ghosh and Amélie Héliou and Paul Jacob and Albert Q. Jiang and Kartik Khandelwal and Timothée Lacroix and Guillaume Lample and Diego Las Casas and Thibaut Lavril and Teven Le Scao and Andy Lo and William Marshall and Louis Martin and Arthur Mensch and Pavankumar Muddireddy and Valera Nemychnikova and Marie Pellat and Patrick Von Platen and Nikhil Raghuraman and Baptiste Rozière and Alexandre Sablayrolles and Lucile Saulnier and Romain Sauvestre and Wendy Shang and Roman Soletskyi and Lawrence Stewart and Pierre Stock and Joachim Studnia and Sandeep Subramanian and Sagar Vaze and Thomas Wang and Sophia Yang},
  title         = {{Pixtral 12B}},
  year          = {2024},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.2410.07073},
  eprint        = {2410.07073},
  eprinttype    = {arxiv},
  file          = {:Agrawal2024 - Pixtral 12B.pdf:PDF:http\://arxiv.org/pdf/2410.07073v2},
  primaryclass  = {cs.CV},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:To Check\;2\;1\;0x00ffffff\;SEARCH\;\;;
1 StaticGroup:Writing Queue\;2\;1\;0x8000ffff\;MDI_ARROW_DOWN_BOLD\;\;;
1 StaticGroup:Reading Reports\;2\;1\;0xff00ffff\;NEWSPAPER_VARIANT_OUTLINE\;\;;
1 StaticGroup:Essays\;2\;0\;0x00ff00ff\;NEWSPAPER_VARIANT_MULTIPLE_OUTLINE\;\;;
1 StaticGroup:VIsual Question Answering\;2\;1\;0xffff00ff\;MDI_CAMERA\;\;;
1 StaticGroup:Auxiliary Training Objectives\;2\;0\;0x0000ffff\;GEAR_FILL\;\;;
1 StaticGroup:Few-Shot Transfer\;2\;1\;0xff0000ff\;MDI_BOOK\;\;;
1 StaticGroup:Datasets\;2\;1\;0xffffffff\;MDI_DATABASE\;\;;
1 StaticGroup:Models\;2\;0\;0x00ff00ff\;GRAPHQL\;\;;
1 StaticGroup:Possible Conflicts\;2\;1\;0xffff00ff\;WARNING\;\;;
}
