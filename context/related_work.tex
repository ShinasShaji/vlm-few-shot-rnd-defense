%!TEX root = ../ShajiS_RnDReport.tex
\documentclass[../ShajiS_RnDReport.tex]{subfiles}

\begin{document}
\section{Related Work}
\label{sec:related_work}

This section reviews relevant literature across four main areas: the development of vision-language models, transfer learning and adaptation techniques, applications and datasets, and current limitations in the field. We identify current research gaps and limitations, and position our work within the broader research landscape.

\subsection{Development of Vision-Language Models}
Previous works have developed scalable language and image pre-training techniques to train Transformer~\cite{Vaswani2017} language models such as the \gls{gpt}~\cite{Radford2019,Brown2020} and image foundation models such as ViT~\cite{Dosovitskiy2021}. This research has also led to the development of multi-modal pre-training techniques for \glspl{vlm} such as CLIP~\cite{Radford2021}, FLAVA~\cite{Singh2021}, BLIP~\cite{Li2022}, Frozen~\cite{Tsimpoukelli2021}, and others~\cite{Zhou2024b}. CLIP and FLAVA are \emph{alignment} models, learning to generate unified text-image representations (often from unsupervised web-scale pre-training), while the latter are \emph{generative} models, trained to generate text conditioned on image-text input. \Glspl{vlm} are often trained on image-language contrastive, masking and modeling objectives~\cite{Li2022}.

The idea of framing \gls{vl} tasks as text generation tasks was formally introduced by Cho et al.~\cite{Cho2021}. This approach simplifies the design of task-specific model architectures, enabling the use of a unified \gls{vlm} across diverse tasks~\cite{Yuan2021,Alayrac2022,Xiao2023a}. Furthermore, it streamlines task transfer compared to alignment models like CLIP and FLAVA, as tasks can now be described in natural language.

The development of generative \glspl{vlm} gave rise to various architectures. Tsimpoukelli et al.~\cite{Tsimpoukelli2021}, Zhu et al.~\cite{Zhu2023}, Li et al.~\cite{Li2023}, Bai et al.~\cite{Bai2023}, Chen et al.~\cite{Chen2023}, Alayrac et al.~\cite{Alayrac2022} and others leverage separate pre-trained \glspl{llm} and vision encoders~\cite{Khan2022}. Novel unified multimodal architectures have also been proposed and developed~\cite{ChameleonTeam2024,Wang2022a}. Several of these models are available in various sizes or parameter counts~\cite{Liu2023a,ChameleonTeam2024}.

\subsection{Transfer Learning and Adaptation Techniques}
Here, we review various techniques used to transfer pre-trained language and vision-language models to downstream tasks.

\subsubsection{Prompting, Prompt-Tuning and Prompt Engineering}
Prompting techniques involve engineering prompts to a model to improve task performance, and have been widely studied in \gls{nlp}~\cite{Liu2023}. Prompts can be manually crafted or even optimized using gradient descent~\cite{Li2021} to optimize task performance~\cite{Liu2023,Reynolds2021}. Providing solved examples of a task into a language model's context has also been shown to enhance performance as a form of few-shot transfer~\cite{Brown2020}, and is referred to as in-context learning. These prompting and prompt-tuning methods have been extended to \glspl{vlm}. Works such as CoOp~\cite{Zhou2022}, CoCoOp~\cite{Zhou2022a} and VPT~\cite{Jia2022} focus on learning prompts for \gls{vl} \emph{alignment} models, while MaPLe~\cite{Khattak2023} investigates using multimodal examples for such alignment models to boost few-shot performance. Prompt learning for \gls{vl} \emph{generative} models was explored by Jin et al.~\cite{Jin2022} and Yang et al.~\cite{Yang2023a}.

The prompt may be hand-engineered, which requires significant domain expertise and time investment as even slight changes in wording and ordering of examples in the prompt can substantially impact model performance~\cite{Liu2023}. The prompt may also be used as a parameter-efficient fine-tuning method, where the prompt is represented as a set of learnable vectors and optimized for the task at hand. For example, Zhou et al.~\cite{Zhou2022} adapts CLIP-like models to downstream vision tasks by learning context words in prompts while keeping the pre-trained parameters fixed, and showed improvements of their approach over hand-crafted prompts. Their approach has also been extended to generate input-conditional prompt tokens, for better robustness to class shifts~\cite{Zhou2022a}.

In prefix-tuning, the parameters of the model are kept frozen, and instead a sequence of continuous task-specific vectors (called the prefix) is prepended to the input and optimized for the task at hand~\cite{Li2021}. Li et al.~\cite{Li2021} notes that prefix-tuning requires storing only 0.1\% of the parameters compared to full fine-tuning while achieving comparable performance, and even outperforms fine-tuning in low-data settings. However, it has also been shown that the benefit of prefix-tuning and fine-tuning diminishes with the increase in size of the model~\cite{Raffel2020}.

\subsubsection{In-Context Learning}
In-context learning is a form of prompting introduced by Brown et al.~\cite{Brown2020} that enables task adaptation without modifying any model parameters. It works by prepending natural language task instructions and a few examples to the input, allowing the model to learn from these examples to generate appropriate outputs. However, due to the bounded context window of transformer models, in-context learning can be limited to a few examples.

Recent studies in \glspl{llm} have revealed that in-context learning operates primarily by leveraging pre-existing knowledge rather than learning new tasks from scratch. Chochlakis et al.~\cite{Chochlakis2024} found that models heavily rely on background knowledge (task priors) when performing in-context learning, and often struggle to integrate new information that contradicts these priors, and can lead to performance saturation. Pan et al.~\cite{Pan2023} further distinguish between task recognition (using pre-trained priors) and actual task learning (gaining new knowledge from demonstrations), suggesting that models often perform well by recognizing familiar tasks rather than learning from the provided context.

\subsubsection{Chain-of-Thought Prompting}
Chain-of-thought prompting is a technique explored by Wei et al.~\cite{Wei2022} that improves complex reasoning in large language models by generating a series of intermediate reasoning steps. Unlike standard prompting, which directly maps inputs to outputs, chain-of-thought prompting manifests the reasoning process through exemplars that include the intermediate steps to solving the task. This approach has shown significant improvements in arithmetic, common sense, and symbolic reasoning tasks, allowing models to decompose complex problems into manageable steps, and providing interpretable insights into the model's reasoning process.

\subsubsection{Parameter-Efficient Fine-Tuning Methods}
A class of methods explore \gls{peft}~\cite{Houlsby2019} of \glspl{vlm}~\cite{He2023,Han2024}. However, these methods typically require more training examples beyond the few-shot transfer regime and are not explored in this work.

In this work, we explore engineering the prompt to \glspl{vlm} with examples of the task at hand as well as with chain-of-thought prompting, and investigate whether the model is able to perform the task from the examples provided in-context.

\subsection{Applications and Datasets}
Zhang et al.~\cite{Zhang2024} provides a survey of \gls{vlm} applications in vision tasks, contrasting the traditional paradigm of training \gls{dnn} models for specific tasks with task-specific datasets with the large-scale general pre-training paradigm employed by \glspl{vlm}~\cite{Radford2021}. Several image datasets exist for various vision tasks, such as ImageNet~\cite{Deng2009} and CIFAR-10~\cite{Krizhevsky2009} (object recognition), Microsoft COCO~\cite{Lin2014} (object detection, segmentation, and captioning), the German Traffic Sign Recognition and Detection Benchmarks~\cite{Houben2013} (recognition and detection), the MVTec Anomaly Detection dataset~\cite{Bergmann2021} (anomaly detection), and the Seven-Point Checklist Dermatology Dataset (Derm7Pt)~\cite{Kawahara2019} (dermatological diagnosis), among others.

Furthermore, within the traditional \gls{dnn} training paradigm, auxiliary learning tasks have been shown to improve performance on a given main task despite being generally unrelated to it~\cite{Jaderberg2017,Liebel2018}. To address the challenge of manually labeling auxiliary data, Liu et al.~\cite{Liu2019} explores the self-supervised generation of auxiliary labels for auxiliary tasks as a form of meta learning.

Liu et al.~\cite{Liu2023a,Liu2024a} and Li et al.~\cite{Li2023b} investigate visual instruction tuning to enable \gls{vl} generative models to follow natural language instructions. The use of a language-only \gls{gpt} model was explored by Liu et al.~\cite{Liu2023a} to generate multimodal instruction-following data from image captions to train their multimodal assistant \gls{llm}, LLaVa. This builds on the use of \glspl{llm} for data annotation in \gls{nlp}~\cite{Tan2024} and the observation that \glspl{llm} often outperform crowd-workers on data annotation tasks~\cite{Gilardi2023}.

\subsection{Limitations and Research Gaps}
Few-shot transfer with \gls{vl} \emph{generative} models remains a relatively underexplored area compared to its counterpart in \gls{nlp}~\cite{Liu2023}. However, methods from \gls{nlp} can still be applicable and have been successfully applied to this domain~\cite{Jia2022,Jin2022}. In contrast, zero-shot and few-shot performance of \gls{vl} \emph{alignment} models (such as CLIP) have been extensively studied, with numerous proposed techniques, as summarized by Zhang et al.~\cite{Zhang2024}.

\Glspl{vlm} are however not without their limitations. They can exhibit systematic shortcomings in certain vision tasks and may be 'blind' to crucial visual differences such as position and perspective~\cite{Doveh2023,Tong2024}. They can also inherit issues like ungrounded predictions and hallucinations from \glspl{llm}~\cite{Alayrac2022,Li2023c}.

Despite progress, there is a gap in the research regarding the application of few-shot transfer to \gls{vl} generative models to dataset annotation for vision tasks~\cite{Tan2024,Zhang2024}. On the other hand, self-supervised generation of auxiliary labels investigated by Liu et al.~\cite{Liu2019} produces labels that may not be human interpretable. Previous work also does not evaluate the performance of downstream vision models trained on such pseudolabeled datasets~\cite{Liu2019,Liu2023a}. Notably, Liu et al.~\cite{Liu2023a} only utilizes a language-only model to generate `multimodal' instruction-following data from image caption text.

In our work, we address some of these gaps by providing a comprehensive evaluation of \glspl{vlm}' few-shot transfer capabilities for dataset annotation. We examine both general (CIFAR-10 Dataset) and specialized (Seven-Point Checklist Dermatology Dataset) domains, evaluating zero-shot and few-shot performance across multiple prompting strategies. Uniquely, we assess the practical viability of this approach by training and evaluating downstream models on \gls{vlm}-generated pseudolabels, providing insights into the feasibility of using \glspl{vlm} for efficient dataset annotation in resource-constrained environments.

\end{document}
