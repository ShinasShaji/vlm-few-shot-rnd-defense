\section{Related Work}
\subsection{Background}
\begin{frame}{Related Work}
\framesubtitle{Vision-Language Models}
  \vspace{-1em}
  \begin{columns}[T]
    \column{\customcolumnwidth}
    \textbf{Vision-Language Model Classes}
    \vspace{-0.4em}
    \begin{itemize}
      \item \emph{Alignment models}: Generate unified text-image embeddings (CLIP~\footfullciteieee{Radford2021}, FLAVA) \vspace{-0.2em}
      \item \emph{Generative models}: Generate text conditioned on multimodal inputs (Flamingo, Frozen~\footfullciteieee{Tsimpoukelli2021}, MiniCPM~\footfullciteieee{Yao2024}, GPT-4o, Claude, etc.)
    \end{itemize}
    \column{\customcolumnwidth}
    \textbf{Prompting Techniques}: Crafting prompts to improve task performance~\footfullciteieee{Liu2023}
    \begin{itemize}
      \item In-context learning: Providing \emph{examples} in context~\footfullciteieee{Brown2020}
      \item Chain-of-thought prompting for complex \emph{reasoning}~\footfullciteieee{Wei2022}
    \end{itemize}
  \end{columns}
\end{frame}
\note{
- Vision-Language Models (VLMs) have progressed significantly since the introduction of CLIP \\
- Alignment models, such as CLIP, are designed to learn joint representations but do not generate textual outputs. What are joint representations? Text and images are processed into vectors or embeddings such that for similar concepts, the embeddings are close to each other in vector space \\
- In contrast, generative models are capable of producing text responses based on visual inputs \\
- Contemporary VLMs often incorporate separate pre-trained components for vision and language, connected through adapters or connectors \\
- Additionally, unified architectures that process both modalities simultaneously have emerged, enhancing the integration of vision and language tasks \\
- A pivotal advancement facilitating few-shot transfer is the conceptualization of vision tasks as text generation tasks. This paradigm allows us to articulate tasks using natural language, eliminating the need for specialized architectures \\
- Prompting techniques are crucial for adapting models to new tasks without retraining \\
- In-context learning allows models to learn from examples without parameter updates \\
}


\begin{frame}{Related Work}
\framesubtitle{Applications and Datasets}
  \vspace{-1.4em}
  \begin{columns}[T]
    \column{\customcolumnwidth}
      \textbf{Applicability of VLMs}
      \vspace{-0.2em}
      \begin{itemize}
        \item Large-scale pre-training enables \emph{generalization} to various tasks
        \item Traditional DNNs trained for \emph{specific} tasks
      \end{itemize}
      \textbf{LLMs for Data Annotation}
      \vspace{-0.4em}
      \begin{itemize}
        \item LLMs used to generate multimodal instruction-following data~\footfullciteieee{Liu2023a}
        \item Seen to outperform crowd-workers in \emph{data annotation} tasks~\footfullciteieee{Gilardi2023}
      \end{itemize}
    \column{\customcolumnwidth}
    \textbf{Key Datasets}:
    Various datasets exist for various vision tasks.
    \vspace{-0.2em}
    \begin{itemize}
      \setlength{\itemsep}{0em}
      \item ImageNet, CIFAR-10~\footfullciteieee{Krizhevsky2009}, GTSDB~\footfullciteieee{Houben2013}: Object recognition, detection
      \item Microsoft COCO: Object detection, segmentation, captioning
      \item Derm7Pt~\footfullciteieee{Kawahara2019}: Specialized dermatology dataset
      \item MVTec: Anomaly detection
    \end{itemize}
  \end{columns}
\end{frame}
\note{
- VLMs are used in a wide range of vision tasks, leveraging their ability to generalize from large-scale pre-training \\
- Traditional DNNs are typically trained on specific datasets for specific tasks, limiting their generalization \\
- LLMs have been used to generate multimodal instruction-following data to train VLMs, enhancing dataset creation \\
- LLMs have been seen to outperform human annotators, providing more consistent and scalable data annotation \\
- Few-shot transfer capabilities of VLMs are less studied compared to LLMs in NLP \\
- We acknowledge important VLM limitations: \\
  - Difficulty with spatial relationships and perspective \\
  - Inherited issues from LLMs like hallucinations \\
  - Biases from pre-training data making model ignore visual context \\
- Something to note: this is a large field of research, and most of the research was done in the last 4-5 years \\
}


\subsection{Research Gaps}
\begin{frame}{Related Work}
  \framesubtitle{Research Gaps}
  \vspace{-1.4em}
  \textbf{Gaps Addressed in Our Work}
  \vspace{-0.2em}
  \begin{itemize}
    \item \emph{Few-shot} transfer of \emph{VLMs} (with in-context learning) \emph{less explored} than NLP counterparts~\footfullciteieee{Liu2023}
    \item \emph{Dataset annotation} applications~\footfullciteieee{Tan2024}:
    \vspace{-0.2em}
    \begin{itemize}
      \item Use of VLMs for dataset annotation \emph{less explored}
      \item Lack of analyses on \emph{downstream vision models} from VLM-generated pseudolabels
    \end{itemize}
  \end{itemize}
  \blfootnote{\vspace{0.05em}}
\end{frame}
