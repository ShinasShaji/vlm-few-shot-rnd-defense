\section{Conclusions}
% \subsection{Key Findings}
\begin{frame}{Key Findings}
\framesubtitle{What did we learn about VLM few-shot transfer?}
  \vspace{-1em}
  \begin{columns}[T]
    \column{\customcolumnwidth}
      \textbf{General Domain (CIFAR-10)}
      \begin{itemize}
        \item VLMs show fairly good transfer to this task
        \item Downstream models achieve near ground-truth performance
        \begin{itemize}
          \item Only 5\% below models trained from ground-truth
          \item Resilient to pseudolabel noise
          \item Outperform VLMs in most cases
        \end{itemize}
      \end{itemize}
    \column{\customcolumnwidth}
      \textbf{Specialized Domain (Derm7Pt)}
      \begin{itemize}
        \item Limited transfer to specialized domains (Derm7Pt)
        \begin{itemize}
          \item Performance near random-guessing
          \item Suggests strong reliance on pre-trained knowledge
        \end{itemize}
      \end{itemize}
  \end{columns}
  \vspace{1em}
  \textbf{We find:}
  \begin{itemize}
    \item Zero/low-shot approaches with simple prompting most effective
    \item Simpler prompting strategies outperformed complex ones
    \item Examples help model recognize the task (from pre-trained priors)
  \end{itemize}
\end{frame}
\note{
- Zero-shot and low-shot approaches with simple prompting proved most effective for general domain tasks \\
- Downstream models trained on VLM-generated pseudolabels achieved impressive performance, only 5\% below ground truth \\
- Models showed remarkable resilience to label noise, maintaining consistent performance \\
- However, performance degraded significantly on specialized medical tasks (Derm7Pt). The domain is likely underrepresented in the pre-training data of the VLMs \\
- This aligns with research showing models rely more on pre-trained knowledge than learning new capabilities \\
- Architecture choices significantly impacted both performance and resource efficiency \\
- Additional shots or reasoning requirements often increased costs without proportional gains \\
- Results suggest focusing on activating existing capabilities rather than teaching new ones \\
}


% \subsection{Limitations and Future Work}
\begin{frame}{Limitations and Future Work}
\framesubtitle{What constrained us, and what could be worked upon?}
  \vspace{-1em}
  \begin{columns}[T]
    \column{\customcolumnwidth}
      \textbf{Technical Limitations}
      \begin{itemize}
        \item Context length and memory constraints
        \begin{itemize}
          \item Limits number of examples
          \item Limits image resolution
          \item Restricts use of larger models
        \end{itemize}
        \item Limited / degraded performance scaling with number of examples
        \item Limited generalization to specialized domains
      \end{itemize}
      The findings of this work may not generalize to all vision tasks
      \column{\customcolumnwidth}
      \textbf{Future Research Directions}
      \begin{itemize}
        \item Generalizable domain-specific pre-training strategies
        \item More efficient VLM architectures for multi-image processing
        \item Improving prompting strategies and visual grounding~\footfullciteieee{Yang2023}
        \item Extending investigation to other vision tasks:
        \begin{itemize}
          \item Image captioning
          \item Segmentation
        \end{itemize}
      \end{itemize}
  \end{columns}
\end{frame}
\note{
- Context length and memory limitations restricted few-shot experiments to limited examples and constrained larger models \\
- Poor performance on specialized domains suggests models rely more on pre-trained priors than learning new capabilities \\
- There are some things that you can tell beforehand are bad ideas, and this was not specifically one of them. We expected before starting the project that more examples would strictly improve performance, but this was not the case. In NLP and LLMs, few-shot transfer and chain-of-thought prompting is extremely effective, but this is not the case for vision-language models and vision-language tasks, as we see here. \\
- More efficient VLM architectures needed for scalable high-resolution multi-image processing. Not only this, but for the use of VLMs and VLAs in robots in general, the ability to process multiple images at once and keep them in context is extremely important. \\
}


\begin{frame}{Key Contributions}
\framesubtitle{How did we help?}
  \vspace{-1em}
  \begin{itemize}
    \item Advancing the understanding of few-shot transfer of VLMs:
    \begin{itemize}
      \item by providing a comprehensive evaluation framework for assessing VLM transfer capabilities
      \item by providing practical guidelines for their transfer to dataset annotation tasks to train task-specific downstream models
    \end{itemize}
    \item Empirical findings suggest downstream models are resilient to label noise, which:
    \begin{itemize}
      \item is significant for deploying these systems in resource-constrained environments
      \item suggests that VLMs can support the learning of smaller, task-specific models despite imperfect predictions
    \end{itemize}
  \end{itemize}
\end{frame}
\note{}

% \section{Q\&A}
\begin{frame}{Thank You!}
  \vspace{2em}
  \centering
  \vfill
  {\LARGE \textbf{Questions?}}
  \vfill
  \vspace{2em}
  \raggedright\textbf{Contact:}
  \begin{itemize}
    \item shinas.shaji@smail.inf.h-brs.de
    \item shinas.shaji@iais.fraunhofer.de
  \end{itemize}
  \vfill
\end{frame}